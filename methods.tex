\chapter{Methods \label{ch:meth}}

In Chapters \ref{ch:epid} and \ref{ch:fmri}, we consider estimation of states and unknown fixed parameters in Bayesian state-space models for which filtered distributions cannot be calculated analytically. In this case, the most common approach to approximating these distributions is through Markov-chain Monte Carlo (MCMC) methods \citep{Gelf:Smit:samp:1990}. MCMC is an effective tool for analyzing data in complex modeling situations \citep{Robe:Case:mont:2004}. However, in sequential analysis using state-space models, where new observations are arriving as time progresses, MCMC is inefficient due to the increase in computational cost incurred by the need for the entire MCMC to be rerun as each new observation arrives.

Sequential Monte Carlo (SMC) - or particle filtering - techniques, on the other hand, enable on-line inference through updating the approximation to the posterior distribution as new data become available \citep{Douc:deFr:Gord:sequ:2001, cappe2007overview}. In addition, SMC methods can be flexible, general, easy to implement, amenable to parallel computing, and provide direct estimates of the marginal likelihood. As with MCMC methods, however, the performance of SMC methods suffers as the dimension of the parameter space increases. Furthermore, while MCMC methods directly provide smoothed estimates of states in state-space models, SMC algorithms are inefficient for smoothing and have been mostly used only for filtering \cite[Section 5][]{douc:joh:tut:2009}. Each of MCMC and SMC approaches have strengths and limitations in different scenarios.

In Chapters \ref{ch:epid} and \ref{ch:comp}, we compare the performance of several particle filtering algorithms in different model settings. In Chapter \ref{ch:fmri}, we use SMC methods as a tool for model comparison, where we compare the relative posterior probabilities of several models that represent different types/sources of autocorrelation that might be present in fMRI time series data. We discuss possible reasons for the presence of autocorrelation in fMRI time series data and possible modeling approaches in Section \ref{sec:fmri:cor}. In each of Chapters \ref{ch:epid}, \ref{ch:comp}, and \ref{ch:fmri}, we compare SMC results with MCMC, which has been the standard over the past two decades for Bayesian analysis of analytically intractable models. Thus, in this chapter, we review several strategies for both MCMC and particle filtering.

Specifically, in Section \ref{sec:mcmc}, we describe the MCMC algorithms we use for the epidemic model described in Section \ref{sec:epid} ( considered further in Chapter \ref{ch:epid}) and the dynamic regression models described in Sections \ref{sec:dlm:M101} and \ref{sec:dlm:M011} (considered further Chapter \ref{ch:fmri}). In Section \ref{sec:filtering}, we describe several particle filtering strategies and how to apply them to the models outlined in Chapter \ref{ch:models}. In Section \ref{sec:resample}, we discuss the resampling methods within several particle filtering algorithms. In Section \ref{sec:comp}, we show how SMC techniques can be used for model comparison. Lastly, in Section \ref{sec:pmcmc}, we describe a particle MCMC algorithm that we also use to analyze simulated data from the epidemic model, and which we found to perform more efficiently than standard MCMC.

\section{Markov chain Monte Carlo (MCMC) algorithms \label{sec:mcmc}}

MCMC methods provide sample-based approximations to the posterior distribution through the generation of dependent samples from distributions whose densities can be evaluated. In this section, we outline the MCMC algorithms that we use to analyze simulated data from the state-space model of a disease epidemic described in Section \ref{sec:epid}, and from the dynamic regression models described in Sections \ref{sec:dlm:M011} and \ref{sec:dlm:M101}. We also describe a particle MCMC (PMCMC) approach that we found to be more efficient when analyzing simulations from the epidemic model. For more comprehensive descriptions of MCMC and PMCMC methods, we refer the reader to \citet{Robe:Case:mont:2004} and \citet{Andr:Douc:Hol:pmcmc:2010} respectively.

\subsection{MCMC applied to epidemic model \label{sec:mcmc:epid}}

Consider the specific state-space model of an epidemic described in Section \ref{sec:epid}, where $x_t = (s_t,i_t)'$ is the latent disease state, $\theta = (\beta,\gamma,\nu)$ are the unknown fixed parameters, the state equation \eqref{eqn:epid:state} describes the evolution of $x_t$ given $x_{t-1}$ and $\theta$, and the observation equation \eqref{eqn:epid:obs} describes the likelihood of new data, $y_t$, given $x_t$ and $\theta$. We assume the prior distribution, $p(x_0,\theta)$, of the form
\begin{equation}
p(x_0,\theta) = p(x_0)p(\theta) = p(s_0,i_0)p(\beta,\gamma)p(\nu), \label{eqn:epid:prior}
\end{equation}
where $p(s_0,i_0)$ is given by equation \eqref{eqn:epid:prior:state}.

Suppose we observe syndromic surveillance data, $y_t$, for $t = 1, 2, \ldots, T$. Using the fact that, for state-space models, we can express the joint density of the data, states, and fixed parameters by
\begin{equation}
p(y_{1:T},x_{0:T},\theta) = \prod_{t = 1}^T \left\{p(y_t|x_t,\theta)p(x_t|x_{t-1},\theta)\right\}p(x_0,\theta), \label{eqn:joint}
\end{equation}
we derive the \emph{full conditional distribution}, i.e. distribution of a random vector conditional on all of the remaining variables in the model, for each of $x_0, x_1, \ldots, x_T$, $\beta$, $\gamma$, and $\nu$ as
\begin{align}
p(x_0|\hdots) &\propto p(x_1|x_0,\theta)p(x_0) \label{eqn:epid:fullcond} \\
p(x_t|\hdots) &\propto p(y_t|x_t)p(x_{t+1}|x_t,\theta)p(x_t|x_{t-1},\theta) \mbox{, for } t = 1,\ldots,T-1 \nonumber \\
p(x_T|\hdots) &\propto p(y_T|x_T)p(x_T|x_{T-1},\theta) \nonumber \\
p(\beta|\hdots) &\propto \prod_{t=1}^T \left\{p(x_t|x_{t-1},\theta)\right\}p(\beta, \gamma) \nonumber \\
p(\gamma|\hdots) &\propto \prod_{t=1}^T \left\{p(x_t|x_{t-1},\theta)\right\}p(\beta, \gamma) \nonumber \\
p(\nu|\hdots) &\propto \prod_{t=1}^T \left\{p(x_t|x_{t-1},\theta)\right\}p(\nu), \nonumber
\end{align}
where $p(w|\hdots)$ represents the full conditional distribution of $w$, for any $w$. Note that since each of the unknown fixed parameters $\beta$, $\gamma$, and $\nu$ is present only in the state equation \eqref{eqn:epid:state} of the model, their full conditional distributions do not depend on $y_t$. By the same argument $p(y_t|x_t,\theta)$ reduces to $p(y_t|x_t)$.

We use these full conditional distributions to generate samples from $p(x_{0:T},\theta|y_{1:T})$ by implementing a Gibbs sampler with adaptive rejection Metropolis-Hastings (MH) steps \citep{Metr:Rose:Rose:Tell:Tell:equa:1953, Hast:mont:1970,  Gema:Gema:stoc:1984, gilks1995adaptive}. In general, the algorithm works by iteratively sampling each state and fixed parameter, conditional on the current sample, from some proposal distribution, $g$, and accepting the proposed sample with probability $R$. $R$ is given by
\begin{equation}
R = \frac{f(w^*)g(w|w^*)}{f(w)g(w^*|w)}, \label{eqn:mhrat}
\end{equation}
where $w$ is the current sample, $w^*$ is the proposed sample from $g$, and $f(\cdot)$ is the full conditional distribution of the state or fixed parameter evaluated at `$\cdot$' \cite[Chapter 7][]{giv:hoet:2005:comp}. $R$ is termed the \emph{Metropolis ratio}. In our algorithm, we use Gaussian random-walk proposals for each state and fixed parameter, i.e. each proposed sample is drawn from a normal distribution centered at the current sampled value with standard deviation given by a tuning parameter that is adjusted according to the MH acceptance rate. Because these proposal distributions are symmetric, $g(w|w^*)$ and $g(w^*|w)$ cancel out, reducing the Metropolis ratio to
\begin{equation}
R = \frac{f(w^*)}{f(w)}. \label{eqn:mhrat:reduce}
\end{equation}

Let $x^{(j)}_{0:T} = \left(x^{(j)}_0, x^{(j)}_1, \ldots, x^{(j)}_T\right)'$ and $\theta^{(j)} = \left(\beta^{(j)},\gamma^{(j)},\nu^{(j)}\right)'$ represent the sampled values of the states and fixed parameters, respectively, at iteration $j$ of the Gibbs sampler. The full Gibbs sampler applied to the epidemic model proceeds as follows:
\begin{enumerate}
\item Start with initial draws $x^{(0)}_{0:T} = \left(x^{(0)}_0, x^{(0)}_1, \ldots, x^{(0)}_T\right)'$ and $\theta^{(0)} = \left(\beta^{(0)}, \gamma^{(0)}, \nu^{(0)}\right)'$. Set $j = 1$.
\item \label{step:gibbs} Sample the states, $x^{(j)}_t$ for $t = 0,1,\ldots,T$, from their full conditional distributions. For each $t = 1, 2, \ldots, T$,
    \begin{enumerate}
    \item Draw $x^*_t \sim \mbox{N}\left(x^{(j-1)}_t,\tau^2_{x_t} I_2\right)$.
    \item Calculate the Metropolis ratio, $R$, by
      \[R = \left\{
      \begin{array}{cl}
      \frac{p\left(x^{(j-1)}_1\left|x^*_0,\theta^{(j-1)}\right. \right)\ \ p\left(x^*_0\right)}{p\left(x^{(j-1)}_1\left|x^{(j-1)}_0,\theta^{(j-1)}\right.\right)\ \ p\left(x^{(j-1)}_0\right)} & \mbox{, if } t = 0 \\
      & \\
      \frac{p\left(y_t|x^*_t\right)\ p\left( \left. x^{(j-1)}_{t+1} \right| x^*_t,\theta^{(j-1)}  \right)\ p\left(x^*_t \left| x^{(j)}_{t-1},\theta^{(j-1)} \right. \right)}{p\left(y_t \left|x^{(j-1)}_t \right. \right) p\left(x^{(j-1)}_{t+1}\left|x^{(j-1)}_t,\theta^{(j-1)}\right.\right) p\left(x^{(j-1)}_t\left|x^{(j)}_{t-1},\theta^{(j-1)}\right.\right)}
      & \mbox{, if } 1 \le t \le T-1 \\
      & \\
      \frac{p\left(y_T \left|x^*_T \right. \right)\  p\left(x^*_T \left|x^{(j)}_{T-1},\theta^{(j-1)} \right. \right)}{p\left(y_T \left|x^{(j-1)}_T \right. \right)p\left(x^{(j-1)}_T\left|x^{(j)}_{T-1},\theta^{(j-1)} \right. \right)} & \mbox{, if } t = T
      \end{array}
      \right.\]
    \item Draw $u \sim \mbox{Unif}[0,1]$. If $u < \min\{1, R\}$, set $x^{(j)}_t = x^*_t$. Otherwise, set $x^{(j)}_t = x^{(j-1)}_t$.
    \end{enumerate}
\item \label{step:beta} Sample $\beta^{(j)}$ from its full conditional conditional distribution.
    \begin{enumerate}
    \item Draw $\beta^* \sim \mbox{N}\left(\beta^{(j-1)},\tau^2_{\beta}\right)$.
    \item Calculate Metropolis ratio, $R$, by
    \begin{align*}
    R &= \frac{p\left(\beta^*|x_{0:T}^{(j)},\gamma^{(j-1)},\nu^{(j-1)}\right)}{p\left(\beta^{(j-1)}|x_{0:T}^{(j)},\gamma^{(j-1)},\nu^{(j-1)}\right)} \\
     &= \frac{\left\{\prod_{t=1}^T p\left(x^{(j)}_t\left|x^{(j)}_{t-1},\beta^*,\gamma^{(j-1)},\nu^{(j-1)}\right.\right)\right\}p(\beta^*,\gamma^{(j-1)})}{\left\{\prod_{t=1}^T p\left(x^{(j)}_t\left|x^{(j)}_{t-1},\theta^{(j-1)}\right.\right)\right\}p\left(\beta^{(j-1)},\gamma^{(j-1)}\right)}.
    \end{align*}
    \item Draw $u \sim \mbox{Unif}[0,1]$. If $u < \min\{1, R\}$, set $\beta^{(j)} = \beta^*$. Otherwise, set $\beta^{(j)} = \beta^{(j-1)}$.
    \end{enumerate}
\item \label{step:gamma} Sample $\gamma^{(j)}$ from its full conditional conditional distribution.
    \begin{enumerate}
    \item Draw $\gamma^* \sim \mbox{N}\left(\gamma^{(j-1)},\tau^2_{\gamma}\right)$.
    \item Calculate Metropolis ratio, $R$, by
    \begin{align*}
    R &=     \frac{p\left(\gamma^*|x_{0:T}^{(j)},\beta^{(j)},\nu^{(j-1)}\right)}{p\left(\gamma^{(j-1)}|x_{0:T}^{(j)},\beta^{(j)},\nu^{(j-1)}\right)} \\
     &= \frac{\left\{\prod_{t=1}^T p\left(x^{(j)}_t\left|x^{(j)}_{t-1},\beta^{(j)},\gamma^*,\nu^{(j-1)}\right.\right)\right\}p(\beta^{(j)}, \gamma^*)}{\left\{\prod_{t=1}^T p\left(x^{(j)}_t\left|x^{(j)}_{t-1},\beta^{(j)},\gamma^{(j-1)},\nu^{(j-1)}\right.\right)\right\}p\left(\beta^{(j)}, \gamma^{(j-1)}\right)}.
    \end{align*}
    \item Draw $u \sim \mbox{Unif}[0,1]$. If $u < \min\{1, R\}$, set $\gamma^{(j)} = \gamma^*$. Otherwise, set $\gamma^{(j)} = \gamma^{(j-1)}$.
    \end{enumerate}
\item \label{step:nu} Sample $\nu^{(j)}$ from its full conditional conditional distribution.
    \begin{enumerate}
    \item Draw $\nu^* \sim \mbox{N}\left(\nu^{(j-1)},\tau^2_{\nu}\right)$.
    \item Calculate Metropolis ratio, $R$, by
    \begin{align*}
    R &=     \frac{p\left(\nu^*|x_{0:T}^{(j)},\beta^{(j)},\gamma^{(j)}\right)}{p\left(\nu^{(j-1)}|x_{0:T}^{(j)},\beta^{(j)},\gamma^{(j)}\right)} \\
     &= \frac{\left\{\prod_{t=1}^T p\left(x^{(j)}_t\left|x^{(j)}_{t-1},\beta^{(j)},\gamma^{(j)},\nu^*\right.\right)\right\}p(\nu^*)}{\left\{\prod_{t=1}^T p\left(x^{(j)}_t\left|x^{(j)}_{t-1},\beta^{(j)},\gamma^{(j)},\nu^{(j-1)}\right.\right)\right\}p\left(\nu^{(j-1)}\right)}.
    \end{align*}
    \item Draw $u \sim \mbox{Unif}[0,1]$. If $u < \min\{1, R\}$, set $\nu^{(j)} = \nu^*$. Otherwise, set $\nu^{(j)} = \nu^{(j-1)}$.
    \end{enumerate}
\item Set $j = j + 1$ and go back to step \ref{step:gibbs}
\end{enumerate}

The output of this algorithm is a dependent chain of samples which, provided $j$ is large enough, can be assumed to represent draws from the stationary distribution $p(x_{0:T},\theta|y_{1:T})$ \cite[Chapter 7][]{Robe:Case:mont:2004}. Initial values $\theta^{(0)}$ and $x_{0:T}^{(0)}$ could be chosen arbitrarily or by sampling from the prior $\prod_{t=1}^T\left\{p(x_t|x_{t-1},\theta)\right\}p(x_0,\theta)$. In either case, the effective sample size of the chain could be sensitive to the initial values. Generating multiple chains from different starting points could help determine reasonable starting values or a necessary burn-in period before the samples can be assumed to come from the stationary distribution \citep{giv:hoet:2005:comp}.

The standard deviations of the random-walk proposal distributions, i.e. $\tau_{x_t}$ for $t = 0,1,\ldots,T$, $\tau_{\beta}$, $\tau_{\gamma}$, and $\tau_{\nu}$, are tuning parameters that are adjusted during the burn-in period of the MCMC. During burn-in, if the proposed value of a state or parameter at any given iteration of the Gibbs sampler is accepted, the corresponding tuning parameter is adjusted by multiplying by 1.1. If the proposed value is rejected, the tuning parameter is adjusted by dividing by 1.1. The idea here is that a high acceptance rate indicates that proposed samples are in areas of high posterior probability while a low acceptance rate indicates that they are in areas of low posterior probability. We seek proposal distributions that strike a balance in the acceptance rate such that the entire sample space of the posterior is explored. Acceptance rates indicative of optimal mixing of MCMC chains varies by model and have been explored by \citet{Robe:Gel:gilks:1997:optmh} and \citet{bed:2008:optmh}.

\subsection{MCMC applied to dynamic regression \label{sec:mcmc:dr}}

We now derive an MCMC algorithm to sample from the joint posterior distribution of states and unknown parameters from the dynamic intercept ($M_{101}$) and dynamic slope ($M_{011}$) models discussed in Sections \ref{sec:dlm:M101} and \ref{sec:dlm:M011}. Recall that these models are DLMs of the form given in equations \eqref{eqn:dlm:reg:obs} and \eqref{eqn:dlm:reg:state} with
\begin{align*}
U_t &= (1, u_t) &\quad \beta &= (\beta_0, \beta_1)' \\
V &= \sigma^2_m &\quad F_t &= \left\{\begin{array}{ll} 1, & \mbox{for } M_{101} \\ u_t, & \mbox{for } M_{011} \end{array}\right. \\
G &= \phi &\quad W &= \sigma^2_s,
\end{align*}
where $x_t$ is the univariate state representing the change in the intercept or slope at time $t$ and $\theta = (\beta',\phi,\sigma^2_s,\sigma^2_m)'$ are the unknown fixed parameters. We place a prior of the form
\begin{equation}
p(x_0, \theta) = p(x_0)p(\beta|\sigma^2_m)p(\sigma^2_m)p(\phi|\sigma^2_s)p(\sigma^2_s) \label{eqn:dr:prior}
\end{equation}
on the initial state and fixed parameters, where $p(x_0) = \delta_0(x_0)$ and, as stated in equations \eqref{eqn:dynreg:prior1} and \eqref{eqn:dynreg:prior2},
\begin{align*}
&\beta|\sigma^2_m \sim \mbox{N}(\vartheta_0, \sigma^2_mB_0) \qquad \sigma^2_m \sim \mbox{IG}(a_{m_0}, b_{m_0}) \\
&\phi|\sigma^2_s \sim \mbox{N}(\varphi_0, \sigma^2_s\Phi_0) \qquad \sigma^2_s \sim \mbox{IG}(a_{s_0}, b_{s_0}).
\end{align*}
The conjugate form of these priors, conditional on $x_t$, allows for direct sampling from the full conditional distributions of the fixed parameters. Combining this with the forward-filtering backward sampling (FFBS) algorithm for jointly sampling the states \citep{Cart:Kohn:on:1994} allows for a relatively straightforward Gibbs sampler.

Suppose we observe $y_t$ for $t = 1,2,\ldots,T$, and let $x^{(j)}_{0:T} = \left(x^{(j)}_0, x^{(j)}_1, \ldots, x^{(j)}_T\right)'$ and $\theta^{(j)} = \left({\beta^{(j)}}', \phi^{(j)}, {\sigma^2_s}^{(j)}, {\sigma^2_m}^{(j)}\right)'$ represent the sampled values of the states and fixed parameters, respectively, at iteration $j$ of the Gibbs sampler. We generate samples from $p(x_{0:T},\theta|y_{1:T})$ using the following Gibbs sampling algorithm:
\begin{enumerate}
\item Start with initial draws $\theta^{(0)} = \left({\beta^{(0)}}', \phi^{(0)}, {\sigma^2_s}^{(0)}, {\sigma^2_m}^{(0)}\right)'$ and \\ $x^{(0)}_{0:T} = \left(x^{(0)}_0, x^{(0)}_1, \ldots, x^{(0)}_T\right)$. Set $j = 1$.
\item \label{step:gibbs:dr:beta} Jointly sample ${\sigma^2_m}^{(j)} \sim \mbox{IG}(a_{m_T}, b_{m_T})$ and $\beta^{(j)}|{\sigma^2_m}^{(j)} \sim \mbox{N}(\vartheta_T,{\sigma^2_m}^{(j)}B_T)$, where
\begin{align}
a_{m_T} &= T/2 + a_{m_0} \label{eqn:dr:am} \\
b_{m_T} &= \frac{1}{2}(\mbox{SS}_y + \vartheta_0'B_0^{-1}\vartheta_0 - \vartheta_T'B_T^{-1}\vartheta_T) + b_{m_0} \label{eqn:dr:bm} \\
\mbox{SS}_y &= \sum^T_{t=1} (y_t - F_tx_t)'(y_t - F_tx_t) \label{eqn:dr:ssy} \\
\vartheta_T &= B_T\left( \sum^T_{t=1} U_t'(y_t - F_tx_t) + B_0^{-1}\vartheta_0\right) \label{eqn:dr:b} \\
B_T &= \left(\sum^T_{t=1} U_t'U_t + B_0^{-1}\right)^{-1}. \label{eqn:dr:B}
\end{align}
\item \label{step:gibbs:dr:phi} Jointly sample ${\sigma^2_s}^{(j)} \sim \mbox{IG}(a_{s_T}, b_{s_T})$ and $\phi^{(j)}|{\sigma^2_s}^{(j)} \sim \mbox{N}(\varphi_T,{\sigma^2_s}^{(j)}\Phi_T)$, where
\begin{align}
a_{s_T} &= T/2 + a_{s_0} \label{eqn:dr:as} \\
b_{s_T} &= \frac{1}{2}(\mbox{SS}_x + \varphi_0'\Phi_0^{-1}\varphi_0 - \varphi_T'\Phi_T^{-1}\varphi_T) + b_{s_0} \label{eqn:dr:bs} \\
\mbox{SS}_x &= \sum^T_{t=1} x_t^2 \label{eqn:dr:ssx} \\
\varphi_T &= \Phi_T\left( \sum^T_{t=1} x_tx_{t-1} + \Phi_0^{-1}\varphi_0\right) \label{eqn:dr:phi} \\
\Phi_T &= \left(\sum^T_{t=1} x_{t-1}^2 + \Phi_0^{-1}\right)^{-1}. \label{eqn:dr:Phi}
\end{align}
\item \label{step:ffbs} Sample $x_{0:t}^{(j)}$ using the following FFBS algorithm \cite[Section 4.4][]{petris:camp:2009:dynamic}:
\begin{enumerate}
\item Starting with initial values $m_0 = C_0 = 0$, calculate $z_t$, $R_t$, $m_t$ and $C_t$ for $t = 1,2,\ldots,T$ using the Kalman filter (see equation \eqref{eqn:dlm:kal}).
\item Draw $x_T^{(j)} \sim \mbox{N}(m_T,C_T)$. Then, for $t = T-1,\ldots,0$, draw $x_t^{(j)} \sim \mbox{N}(h_t,H_t)$, where
    \begin{align*}
    h_t &= m_t + C_tG'R_{t+1}^{-1}(x_{t+1}^{(j)} - z_{t+1}) \\
    H_t &= C_t - C_tG'R_{t+1}^{-1}GC_t.
    \end{align*}
\item Set $j = j + 1$ and go back to Step \ref{step:gibbs:dr:beta}
\end{enumerate}
\end{enumerate}
Note that both this algorithm and the MCMC for the epidemic model described in Section \ref{sec:mcmc:epid} provide joint samples from $p(x_{0:T},\theta|y_{1:T})$. Samples from the smoothed distributions, $p(x_s,\theta|y_{1:T})$ for $s < T$, can be directly obtained from these joint samples through Monte Carlo integration \cite[Chapter 3][]{Robe:Case:mont:2004}.

\section{Particle filtering \label{sec:filtering}}

Particle filtering is an SMC inferential technique based on repeated use of importance sampling. It aims to approximate the filtered distribution at time $t$ through a weighted Monte Carlo realization from this distribution in terms of $J$ particles, i.e.
\begin{equation}
p(x_t,\theta| y_{1:t}) \approx \sum_{j=1}^J w_t^{(j)} \delta_{\left(x_t^{(j)},\theta^{(j)}\right)}(x_t,\theta), \label{eqn:approx}
\end{equation}
where $\left(x_t^{(j)},\theta^{(j)}\right)$ is the location of the $j^{\mbox{th}}$ particle at time $t$ and $w_t^{(j)}$ is the weight of that particle with $\sum_{j=1}^J w_t^{(j)}=1$. A variety of SMC techniques have been developed to provide more efficient approximations to $p(x_t,\theta|y_{1:t})$ in the sense that with the same computation time a better approximation is achieved. In this section, we describe five particle filtering techniques: the bootstrap filter (BF), the auxiliary particle filter (APF), the kernel density particle filter (KDPF), the resample-move particle filter (RM), and particle learning (PL).

Each of these five strategies has its own advantages and disadvantages. The BF and APF are the simplest and most straightforward to implement, but are unequipped to efficiently deal with state-space models that contain unknown fixed parameters. PL performs the most efficiently, but can only be applied to special cases of state-space models such as DLMs. The RM, while capable of handling state-space models of any form, requires an MCMC step in addition to the SMC, and thus is not a truly sequential algorithm. The KDPF, while being the only truly sequential particle filtering algorithm that can be applied to any state-space model, is outperformed by the RM and PL in many model settings.

In Chapter \ref{ch:epid}, we compare the efficiency of the BF, APF, and KDPF in the syndromic surveillance context. In Chapter \ref{ch:comp}, we compare the KDPF, RM, and PL in terms of their efficiency for estimating the marginal likelihood of data generated from the local level DLM described in Section \ref{sec:dlm:ll}. Finally, in Chapter \ref{ch:fmri}, we employ PL for estimating states and unknown fixed parameters in DLMs using real and simulated fMRI data.

\subsection{Bootstrap filter \label{sec:bf}}

The BF is first successful version of the particle filter \citep{Gord:Salm:Smit:nove:1993,Kita:mont:1996}. Since this method and the APF were developed for when $\theta$ is known, we will (for the moment) drop $\theta$ from the notation. Given an approximation to $p(x_t|y_{1:t})$ as in equation \eqref{eqn:approx} (with $\theta$ omitted), we obtain an approximation to $p(x_{t+1}|y_{1:t+1})$ by performing the following steps for each particle $j=1,\ldots,J$:
\begin{enumerate}
\item Resample: sample an index $k\in\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\left\{w_t^{(1)},\ldots,w_t^{(j)},\ldots,w_t^{(J)}\right\}$,
\item Propagate: sample $x_{t+1}^{(j)} \sim p\left( x_{t+1}\left|x_t^{(k)}\right.\right)$, and
\item Calculate weights and renormalize:
\[ \tilde{w}_{t+1}^{(j)} = p\left(y_{t+1}\left|x_{t+1}^{(j)}\right.\right) \qquad w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/ \sum_{l=1}^J \tilde{w}_{t+1}^{(l)} \right. .\]
\end{enumerate}
This procedure can be applied recursively beginning with an initial set of weights $w_0^{(j)}$ and locations $x_0^{(j)}$ for all $j$, usually obtained by sampling from the prior with uniform weights.

\subsection{Auxiliary particle filter \label{sec:apf}}

One problem that arises in implementing the BF is that $w_t^{(j)}$ will be small for particles for which $p\left(y_{t}\left|x_{t}^{(j)}\right.\right)$ is small, and these particles will contribute little to the approximation to $p(x_{t}|y_{1:t})$. The APF aims to mitigate this by anticipating which particles will have small weight using a look ahead strategy \citep{Pitt:Shep:filt:1999}. Given an approximation to $p(x_t|y_{1:t})$, the APF approximates $p(x_{t+1}|y_{1:t+1})$ by the following:
\begin{enumerate}
\item For each particle $j$, calculate a point estimate of $x_{t+1}^{(j)}$ called $\mu_{t+1}^{(j)}$, e.g.
\[ \mu_{t+1}^{(j)} = E\left(x_{t+1}\left|x_t^{(j)} \right.\right). \]
\item Calculate auxiliary weights and renormalize:
\[ \tilde{g}_{t+1}^{(j)} = w_t^{(j)} p\left(y_{t+1}\left|\mu_{t+1}^{(j)}\right.\right) \qquad g_{t+1}^{(j)} = \tilde{g}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{g}_{t+1}^{(l)}.\right. \]
\item For each particle $j=1,\ldots,J$,
	\begin{enumerate}
    \item Resample: sample an index $k\in\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\left\{g_{t+1}^{(1)},\ldots,g_{t+1}^{(j)},\ldots,g_{t+1}^{(J)}\right\}$,
	\item Propagate: sample $x_{t+1}^{(j)} \sim p\left(x_{t+1}\left|x_t^{(k)}\right.\right)$, and
	\item Calculate weights and renormalize:
\[ \tilde{w}_{t+1}^{(j)} = \frac{p\left(y_{t+1}\left|x_{t+1}^{(j)}\right.\right)}{p\left(y_{t+1}\left|\mu_{t+1}^{(k)}\right.\right)} \qquad w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{w}_{t+1}^{(l)}. \right. \]
	\end{enumerate}
\end{enumerate}
The point estimate used in Step 1 can be any point estimate, although the expectation is commonly used. Step 3 is exactly the same as the BF with appropriate modifications to the weight calculation to adjust for the `look ahead' in steps 1 and 2. APF weights tend to be closer to uniform than BF weights, in which case a better approximation to $p(x_{t}|y_{1:t})$ is achieved.

The BF and the APF were constructed with the idea that all fixed parameters are known. In order to simultaneously estimate the time-evolving states and fixed parameters using either the BF or APF, it is necessary to incorporate the fixed parameters into the state with degenerate evolutions. That is, one regards the fixed parameters as elements of $x_t$ and specifies the state evolution equation such that these elements do not change over time. Due to the possible duplication of some particles and elimination of others through resampling, the number of unique values of the fixed parameters in the particle set will decrease over time, resulting in \emph{degeneracy} in the fixed parameters \citep{Liu:West:comb:2001}.

\subsection{Kernel density particle filter \label{sec:kd}}

The particle filter introduced by \citet{Liu:West:comb:2001}, which we refer to as the KDPF, builds on the APF and provides a general way of fighting degeneracy in fixed parameters. This is done by approximating the set of fixed parameter values by a kernel density estimate and then regenerating values from this approximation. This filter approximates $p(x_t,\theta| y_{1:t})$ via equation \eqref{eqn:approx}. To make the notation transparent, we introduce subscripts for our fixed parameters, e.g. $\theta_t^{(j)}$ represents the value for $\theta$ at time $t$ for particle $j$. This does not imply that the true $\theta$ is dynamic, but rather that particle $j$ can have different values for $\theta$ throughout time.

Let $\bar{\theta}_t$ and $V_t$ be the weighted sample mean and weighted sample covariance matrix of $\theta_t^{(1)},\ldots,\theta_t^{(J)}$.  The KDPF uses a tuning parameter $\Delta$, the discount factor that takes values in $(0,1)$, and two derived quantities $h^2 = 1 - ((3\Delta - 1)/2\Delta)^2$ and $a^2 = 1 - h^2$ that determine how smooth the kernel density approximation is. Lower values of $\Delta$ result in a smoother approximation. However, the goal here is simply to jitter particles around to refresh values of the fixed parameters and reduce the chance of degeneracy, and so $\Delta$ is typically taken to be between 0.95 and 0.99 \citep{Liu:West:comb:2001}.

Given an approximation to the filtered distribution at time $t$ as in equation \eqref{eqn:approx}, the KDPF provides an approximation to $p(x_{t+1},\theta|y_{1:t+1})$ by the following steps:
\begin{enumerate}
\item For each particle $j$, set $m_t^{(j)} = a\theta_t^{(j)} + (1-a)\bar{\theta}_t$ and calculate a point estimate of $x_{t+1}^{(j)}$ called $\mu_{t+1}^{(j)}$, e.g. $\mu_{t+1}^{(j)} = E\left(x_{t+1}\left|x_t^{(j)},\theta_t^{(j)} \right.\right)$.
\item Calculate auxiliary weights and renormalize:
\[ \tilde{g}_{t+1}^{(j)} = w_t^{(j)} p\left(y_{t+1}\left|\mu_{t+1}^{(j)},m_t^{(j)}\right.\right) \qquad g_{t+1}^{(j)} = \tilde{g}_{t+1}^{(j)}\left/ \sum_{l=1}^J \tilde{g}_{t+1}^{(l)}. \right. \]
\item For each particle $j=1,\ldots,J$,
	\begin{enumerate}
    \item Resample: sample an index $k\in\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\left\{g_{t+1}^{(1)},\ldots,g_{t+1}^{(j)},\ldots,g_{t+1}^{(J)}\right\}$,
	\item Regenerate the fixed parameters: sample $\theta_{t+1}^{(j)} \sim \mbox{N}\left(m_t^{(k)}, h^2V_t \right)$,
	\item Propagate: sample $x_{t+1}^{(j)} \sim p\left(x_{t+1}\left|x_t^{(k)},\theta_{t+1}^{(j)}\right.\right)$, and
	\item Calculate weights and renormalize:
	\[ \tilde{w}_{t+1}^{(j)} = \frac{p\left(y_{t+1}\left|x_{t+1}^{(j)},\theta_{t+1}^{(j)}\right.\right)}{p\left(y_{t+1}\left|\mu_{t+1}^{(k)},m_t^{(k)}\right.\right)}
	\qquad
	w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{w}_{t+1}^{(l)}. \right. \]
	\end{enumerate}
\end{enumerate}
The KDPF adds the kernel density regeneration to the auxiliary particle filter. Here, we use a mixture distribution that places normal kernels around each particle, where the mean of each kernel is a weighted average between the particle value and the overall mean of all particles. This ensures that the variance of regenerated fixed parameter values within a specific iteration of the particle filter is the same as the variance of the fixed parameter value prior to regeneration \citep{Liu:West:comb:2001}.

To use the KDPF with normal kernels, it is necessary to parameterize the fixed parameters so that their support is on the real line. This is not a constraint, but rather a practical implementation detail. We typically use logarithms for parameters that have positive support and the logit function for parameters in the interval (0,1). A parameter $\psi$ bounded on the interval (a,b) can first be rebounded to (0,1) through $(\psi-a)/(b-a)$, and then the logit transformation can be applied. We investigate the sensitivity of the performance of the particle filters to the choice of transformation in Chapter \ref{ch:epid}.

\subsection{Resample-move algorithm} \label{sec:rm}

In Chapter \ref{ch:epid}, we show that the KDPF can be an effective tool for estimating unknown fixed parameters in state-space models. However, the choice of a mixture normal distribution for regenerating fixed parameter values is somewhat arbitrary, and efficiency of the algorithm can be increased by using a kernel that matches $p(\theta|y_{1:t})$ more closely. The RM, introduced by \citet{Gilk:Berz:foll:2001}, aims to do this by regenerating fixed parameter values from an MCMC transition kernel with stationary distribution equal to $p(\theta|y_{1:t})$. The algorithm works by running one or a few iterations of an MCMC algorithm within each step of the particle filter for the purpose of jittering fixed parameter values. Since the weighted sample of fixed parameter values already represents an approximation to $p(\theta|y_{1:t})$, the resulting sample after running an MCMC for each particle yields a sample that can only improve the approximation \cite[Section 4.4][]{douc:joh:tut:2009}.

Since distributions that need to evaluated in MCMC algorithms often depend on all of the observed data and unobserved states, we must track the entire history of states within each particle. Thus, we now represent particle $j$ by $\left(x_{0:t}^{(j)},\theta_t^{(j)}\right)$ with weight $w_t^{(j)}$, where $x_{0:t}^{(j)} = \left(x_0^{(j)},x_1^{(j)},\ldots,x_t^{(j)}\right)$ represents the sample path of the state from time 0 to time $t$ for particle $j$. The entire collection of $J$ particles now represents an approximation to $p(x_{0:t},\theta|y_{1:t})$.

The general RM algorithm proceeds in the following way. Given a particle approximation to $p(x_{0:t},\theta|y_{1:t})$, we move to a particle approximation to $p(x_{0:t+1},\theta|y_{1:t+1})$ by the following steps for each particle $j=1,\ldots,J$:
\begin{enumerate}
\item Propagate: draw $\tilde{x}^{(j)}_{t+1}$ from $p\left(x_{t+1}|x^{(j)}_t,\theta^{(j)}_t\right)$. Add $\tilde{x}^{(j)}_{t+1}$ to particle $j$ and denote the new augmented particle by $\left(\tilde{x}^{(j)}_{0:t+1},\theta^{(j)}_t\right)$,
\item Calculate weights and renormalize:
    \[\tilde{w}^{(j)}_t = p\left(y_{t+1}|\tilde{x}^{(j)}_{t+1},\theta^{(j)}_t\right) \qquad w^{(j)}_{t+1} = \tilde{w}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{w}_{t+1}^{(l)}, \right.\]
\item Resample: sample an index $k$ from $\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\{w^{(1)}_{t+1},\ldots,w^{(j)}_{t+1},\ldots,w^{(J)}_{t+1}\}$, and
\item \label{step:move} Move particles: draw a new particle $\left(x^{(j)}_{0:t+1},\theta^{(j)}_{t+1}\right)$ from some transition kernel $q\left(x_{0:t+1},\theta|\tilde{x}^{(k)}_{0:t+1},\theta^{(k)}_t\right)$ with invariant distribution $p(x_{0:t+1},\theta|y_{1:t+1})$.
\end{enumerate}

\subsubsection{\emph{RM for the local level DLM with common observation and state variance factor} \label{sec:rm:ll}}

In Chapter \ref{ch:comp}, we run this algorithm on data simulated from the local level DLM with unknown common variance factor, $\theta$, described in Section \ref{sec:dlm:ll}. To do this, we need to define an MCMC kernel, $q$, for the ``Move particles'' step in the above algorithm. In this case, we sample from $q$ as follows: For a given particle $j$ and sampled index $k$,
\begin{enumerate}
\item \label{step:move:ll} Sample $\theta^{(j)}_{t+1} \sim \mbox{IG}(a_t, b_t)$, where
\begin{align*}
a_t &= a_0 + 1/2 + t \\
b_t &= b_0 + \frac{1}{2}\left(\sum_{i=1}^t \left(y_i - x_i^{(k)}\right)^2 + \frac{1}{\lambda}\sum_{i=1}^t \left(x_i^{(k)} - x_{i-1}^{(k)}\right)^2 + \left(x^{(k)}_0\right)^2\right),\mbox{ and}
\end{align*}
\item Sample $x_{0:t+1}^{(j)}$ using FFBS (see Step \ref{step:ffbs} of Gibbs sampler from Section \ref{sec:mcmc:dr}) with
\[m_0 = 0 \quad C_0 = V = \theta^{(j)}_{t+1} \quad W = \theta_{t+1}^{(j)}\lambda \quad F_t = G = 1.\]
\end{enumerate}
Note that the RM is not a truly sequential particle filter because of the increase in computation required with increasing $t$.

\subsection{Particle learning \label{sec:pl}}

We consider a particle filtering algorithm called particle learning \citep{Carv:Joha:Lope:Pols:part} that can be applied to a particular class of state-space models that includes DLMs. For models within this class, particle learning prescribes a truly sequential algorithm that samples new values for $\theta$ from $p(\theta|y_{1:t})$ using conditional sufficient statistics. Let $s_t$ denote the sufficient statistics for $\theta$ conditional on the current state $x_t$. Then, we add the sampled values of the sufficient statistics, $s_t^{(j)}$, to the particles, i.e. particle $j$ and time $t$ is now represented by $\left(x_t^{(j)},s_t^{(j)},\theta_t^{(j)}\right)$. We move from an approximation to $p(x_t,\theta|y_{1:t})$ to that of $p(x_{t+1},\theta|y_{1:t+1})$ by the following procedure for each particle $j = 1,2,\ldots,J$:
\begin{enumerate}
\item Calculate weights an renormalize:
    \[\tilde{w}_{t+1}^{(j)} = p\left(y_{t+1}|x_t^{(j)},\theta_t^{(j)}\right) \qquad w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)} \left/ \sum_{l=1}^J \tilde{w}_{t+1}^{(l)}\right.,\]
\item Resample: sample an index $k \in \{1,\ldots,j,\ldots,J\}$ with associated probabilities $\{w_{t+1}^{(1)},\ldots,w_{t+1}^{(j)},\ldots,w_{t+1}^{(J)}\}$,
\item Propagate: sample $x_{t+1}^{(j)} \sim p\left(x_{t+1}|y_{t+1},x_t^{(k)},\theta_t^{(k)}\right)$,
\item Update: calculate $s_{t+1}^{(j)} = S\left(y_{t+1},x_{t+1}^{(j)},s_t^{(k)}\right)$, and
\item Regenerate: sample $\theta_{t+1}^{(j)} \sim p\left(\theta|s_{t+1}^{(j)}\right)$.
\end{enumerate}
Note that this algorithm moves the ``Resample'' step ahead of the ``Propagate'' step, and thus is sometimes referred to as a \emph{resample-propagate} particle filter. Also, this algorithm requires the ability to evaluate the conditional predictive distribution $p(y_{t+1}|x_t,\theta)$ and sample from the conditional filtered distributions $p(x_{t+1}|y_{t+1},x_t,\theta)$ and $p(\theta|s_t)$. Thus, particle learning is only applicable to models for which the form of these distributions is analytically tractable. In addition, we must define the recursive map $S$ to update the sufficient statistics based on the new observation $y_{t+1}$ and the newly sampled state $x_{t+1}^{(j)}$. In Chapter \ref{ch:comp} we run the PL on simulated data from the local level DLM described in Section \ref{sec:dlm:ll}, and in Chapter \ref{ch:fmri}, we apply the PL to the dynamic regression models described in Sections \ref{sec:dlm:M101} and \ref{sec:dlm:M011} using real and simulated fMRI data. We now show how to implement PL for these specific models.

\subsubsection{\emph{PL for the local level DLM with common observation and state variance factor} \label{sec:pl:ll}}

To implement a particle learning algorithm for the local level DLM given by equations \eqref{eqn:ll:obs} and \eqref{eqn:ll:state}, we derive the conditional predictive distribution of $y_{t+1}$ given on $x_t$ and $\theta$ and the filtered distribution of $x_{t+1}$ given $x_t$ and $\theta$. These distributions are given by
\begin{align}
&y_{t+1}|x_t,\theta \sim \mbox{N}\left(x_t,\theta(1+\lambda)\right) \label{eqn:pl:ll:pred} \\
&x_{t+1}|y_{t+1},x_t,\theta \sim \mbox{N}(\mu_t,\tau^2) \label{eqn:pl:ll:state},
\end{align}
with
\begin{equation}
\mu_t = \frac{\lambda}{1+\lambda}(y_{t+1} + x_t / \lambda) \qquad \tau^2 = \theta\frac{\lambda}{1+\lambda}. \label{eqn:pl:ll:statemv}
\end{equation}
We also derive the filtered distribution of $\theta$ conditional on the states, $p(\theta|y_{1:t},x_{0:t})$, expressed by
\begin{equation}
\theta|y_{1:t},x_{0:t} \sim \mbox{IG}(a_t,b_t),
\end{equation}
where
\begin{align*}
a_t &= t + 1/2 + a_0 \\
b_t &= b_0 + \frac{1}{2}\left(\sum_{k=1}^t (y_k - x_k)^2 + \frac{1}{\lambda}\sum_{k=1}^t (x_k - x_{k-1})^2 + x_0^2\right).
\end{align*}
Thus, $s_t = (a_t,b_t)$ are the conditional sufficient statistics for $\theta$ at time $t$, which can be updated according to the recursive map $S$ defined by
\begin{align}
a_{t+1} &= a_t + 1, t \ge 1 \label{eqn:pl:ll:a} \\
b_{t+1} &= \frac{1}{2}\left((y_{t+1}-x_{t+1})^2 + \frac{1}{\lambda}(x_{t+1}-x_t)^2\right) + b_t, t \ge 1 \label{eqn:pl:ll:b}
\end{align}
with initial conditions
\begin{align*}
a_1 &= 3/2 + a_0 \\
b_1 &= \frac{1}{2}\left((y_1-x_1)^2 + \frac{1}{\lambda}(x_1-x_0)^2 + x_0^2\right) + b_0.
\end{align*}

\subsubsection{\emph{PL for dynamic regression models} \label{sec:pl:dr}}

Consider the dynamic regression models $M_{101}$ and $M_{011}$ described in Sections \ref{sec:dlm:M101} and \ref{sec:dlm:M011} and given by equations \eqref{eqn:dlm:reg:obs} and \eqref{eqn:dlm:reg:state} with
\begin{align*}
U_t &= (1, u_t) &\quad \beta &= (\beta_0, \beta_1)' \\
V &= \sigma^2_m &\quad F_t &= \left\{\begin{array}{ll} 1, & \mbox{for} M_{101} \\ u_t, & \mbox{for} M_{011} \end{array}\right. \\
G &= \phi &\quad W &= \sigma^2_s.
\end{align*}
We specify the prior distributions $p(\beta,\sigma^2_m)$ and $p(\phi,\sigma^2_s)$ according to equations \eqref{eqn:dynreg:prior1} and \eqref{eqn:dynreg:prior2}, restated below as
\begin{align}
\beta|\sigma^2_m &\sim \mbox{N}(\vartheta_0, \sigma^2_m B_0) &\quad \sigma^2_m &\sim \mbox{IG}(a_{m_0},b_{m_0}) \label{eqn:pl:prior:beta} \\
\phi|\sigma^2_s &\sim \mbox{N}(\varphi_0, \sigma^2_s \Phi_0) &\quad \sigma^2_s &\sim \mbox{IG}(a_{s_0},b_{s_0}) \label{eqn:pl:prior:phi}
\end{align}
with $x_0 = 0$ (i.e. $p(x_0) = \delta_0(x_0)$) and the hyperparameters $\vartheta_0$, $B_0$, $\varphi_0$, $\Phi_0$, $a_{m_0}$, $b_{m_0}$, $a_{s_0}$, and $b_{s_0}$ assumed known.

To implement a particle learning algorithm for this model, we derive the conditional predictive and conditional filtered distributions
\begin{align}
&y_{t+1}|x_t,\theta \sim \mbox{N}(U_{t+1}\beta + F_t\phi x_t, F_t^2\sigma^2_s + \sigma^2_m) \label{eqn:pl:dr:pred} \\
&x_{t+1}|y_{t+1},x_t,\theta \sim \mbox{N}(\mu_t,\tau^2), \label{eqn:pl:dr:state}
\end{align}
where \[\mu_t = \tau^2\left(\frac{(y_{t+1}-U_{t+1}\beta)F_{t+1}}{\sigma^2_m} + \frac{\phi x_t}{\sigma^2_s}\right) \qquad \tau^2 = \left(\frac{F_{t+1}^2}{\sigma^2_m} + \frac{1}{\sigma^2_s}\right)^{-1}.\]
In addition, we derive $p(\theta|y_{1:t},x_{0:t})$ using the fact that
\begin{equation}
p(\theta|y_{1:t},x_{0:t}) \propto \left(\prod_{k=1}^t p(y_k|x_k,\beta,\sigma^2_m)p(x_k|x_{k-1},\phi,\sigma^2_s)\right)p(\beta,\sigma^2_m)p(\beta,\sigma^2_m)\delta_0(x_0). \label{eqn:pl:dr:theta}
\end{equation}
The filtered distribution for $\theta$ conditional on the states is then given by
\begin{align}
\beta|\sigma^2_m,y_{1:t},x_{0:t} &\sim \mbox{N}(\vartheta_t, \sigma^2_m B_t) &\quad \sigma^2_m|y_{1:t},x_{0:t} &\sim \mbox{IG}(a_{m_t},b_{m_t}) \label{eqn:pl:post:beta} \\
\phi|\sigma^2_s,y_{1:t},x_{0:t} &\sim \mbox{N}(\varphi_t, \sigma^2_s \Phi_t) &\quad \sigma^2_s|y_{1:t},x_{0:t} &\sim \mbox{IG}(a_{s_t},b_{s_t}), \label{eqn:pl:post:phi}
\end{align}
where $\vartheta_t$, $B_t$, $\varphi_t$, $\Phi_t$, $a_{m_t}$, $b_{m_t}$, $a_{s_t}$, and $b_{m_t}$ are calculated according to the equations in Steps \ref{step:gibbs:dr:beta} and \ref{step:gibbs:dr:phi} of the Gibbs sampler outlined in Section \ref{sec:mcmc:dr} (with $T = t$). Thus, $s_t = (\vartheta_t, B_t, a_{m_t}, \xi_{m_t}, \varphi_t, \Phi_t, a_{s_t}, \xi_{s_t})$ form the sufficient statistics for $\theta$ and are updated by the recursive map given by
\begin{align}
B_t^{-1}\vartheta_t &= B_{t-1}^{-1}\vartheta_{t-1} + U_t'(y_t - F_tx_t) & B_t^{-1} &= B_{t-1}^{-1} + U_t'U_t \label{eqn:pl:dr:suff} \\
a_{m_t} &= a_{m_{t-1}} + 1/2 & \xi_{m_t} &= \xi_{m_{t-1}} + (y_t - F_tx_t)^2 \nonumber \\
\Phi_t^{-1}\varphi_t &= \Phi_{t-1}^{-1}\varphi_{t-1} + x_tx_{t-1} & \Phi_t^{-1} &= \Phi_{t-1}^{-1} + x_{t-1}^2 \nonumber \\
a_{s_t} &= a_{s_{t-1}} + 1/2 & \xi_{s_t} &= \xi_{s_{t-1}} + x_t^2 \nonumber
\end{align}
Note that we update $\xi_{m_t}$ and $\xi_{s_t}$ in the recursive map and calculate the inverse-gamma rate parameters $b_{m_t}$ and $b_{s_t}$ according to
\begin{align}
b_{m_t} &= \frac{1}{2}\left(\xi_{m_t} + \vartheta_0'B_0^{-1}\vartheta_0 - \vartheta_t'B_t^{-1}\vartheta_t\right) + b_{m_0} \label{eqn:pl:dr:rate} \\
b_{s_t} &= \frac{1}{2}\left(\xi_{s_t} + \varphi_0'\Phi_0^{-1}\varphi_0 - \varphi_t'\Phi_t^{-1}\varphi_t\right) + b_{s_0}. \nonumber
\end{align}

\section{Resampling \label{sec:resample}}

Successful implementation of any particle filtering algorithm depends on which resampling scheme to use and when to resample. Resampling is sampling (with replacement) random indices between 1 and $J$, where index $j$ has probability $w^{(j)}$ of being selected. Throughout our discussion, we have explicitly used multinomial resampling, but alternative resampling schemes exist including residual, stratified, and systematic resampling \citep{Douc:Capp:Moul:comp:2005}. Residual resampling deterministically samples $\lfloor w^{(j)} J \rfloor$ copies of particle $j$, for each $j$, and distributes the remaining $J - \sum^J_{j=1} \lfloor w^{(j)} J \rfloor$  particles according to a multinomial distribution with associated probabilities $(w^{(j)} J - \lfloor w^{(j)} J \rfloor) / (J - \sum^J_{j=1} \lfloor w^{(j)} J \rfloor)$, where $\lfloor . \rfloor$ is the largest integer less than or equal ``.''. Stratified resampling samples uniformly over the interval $[(j-1) / J, j / J]$, for $j = 1, 2, \ldots, J$, and calculates the number of copies of particle $j$ according to the empirical cumulative distribution function of the particle indices (i.e. the ``inversion method''). Finally, systematic resampling is similar to stratified resampling, except that only one uniform draw is initially sampled from $[0, 1/J]$ and the remaining $J-1$ are calculated by adding $(j-1) / J$ to the sampled value prior to applying the inversion method.

Resampling is meant to rebalance the weights of the particles in order to avoid degeneracy, but this introduces additional Monte Carlo variability to the particle sample. Despite systematic resampling only requiring a single uniform draw, \citet{Douc:Capp:Moul:comp:2005} show via example that it can introduce more Monte Carlo variability than the other three resampling schemes. In Chapter \ref{ch:epid}, we discuss some advantages and disadvantages of the different resampling methods when applied to our specific model of a disease outbreak and suggest the use of stratified or residual resampling.

The frequency of resampling should be reduced to balance the loss of information due to degeneracy with the loss of information due to the additional Monte Carlo variability introduced during resampling. Typically, a measure of the nonuniformity of particle weights is used to determine if resampling should be performed at a given iteration of a particle filter. The common measures are effective sample size, coefficient of variation, and entropy. We use effective sample size \citep{Liu:Chen:Wong:reje:1998}, a value ranging between 1 and $J$ that can be interpreted as the number of independent particle samples. An effective sample size of $J$ corresponds to all particle weights being equal, and a value of 1 corresponds to one particle weight being 1 with the rest 0. Using this measure of nonuniformity, we set a threshold of $0.8J$, meaning that if the number of independent samples is less than 80\% of the total number of particles at time $t$, resampling is performed at that time.

The algorithms described in Sections \ref{sec:bf} through \ref{sec:pl} were constructed under the assumption that resampling is performed at every iteration of the filter. However, in practice, we omit the resampling step in each algorithm at each time point where the effective sample size exceeds $0.8J$. If resampling is not performed, we modify the algorithm at that timepoint by 1) omitting the `Resample' step, 2) replacing all instances of the sampled index $k$ with the particle index $j$, and 3) adjusting the calculation of $\tilde{w}_{t+1}^{(j)}$ by multiplying by $w_t^{(j)}$ (in the BF, RM, and, PL) or $\tilde{g}^{(j)}_{t+1}$ (in the APF and KDPF), i.e. the particle weights get carried over. For the KDPF, RM, and PL, regeneration is not performed when resampling is not performed since, in this case, there is no reduction in the number of unique fixed parameter values. In this case, we let $\theta_{t+1}^{(j)} = \theta_t^{(j)}$ for all $j$.

\section{Model comparison \label{sec:comp}}

Each of the particle filters described in previous sections, in addition to generating a weighted sample approximation to $p(x_t,\theta|y_{1:t})$, provide an approximation to the marginal likelihood, $p(y_{1:t})$. We first note that given $p(y_{1:t-1})$, $p(y_{1:t})$ can be updated recursively by
\begin{equation}
p(y_{1:t}) = p(y_t|y_{1:t-1})p(y_{1:t-1}). \label{eqn:marglik:recurse}
\end{equation}
Thus, $p(y_{1:t})$ can be calculated through the one-step ahead predictive densities, $p(y_k|y_{1:k-1})$ for $k=1,\ldots,t$ (letting $p(y_1|y_0) = p(y_1)$), according to
\begin{equation}
p(y_{1:t}) = \prod_{k=1}^t p(y_k|y_{1:k-1}). \label{eqn:marglik}
\end{equation}
At each step of the particle filter (i.e. at each time $t$), an approximation to $p(y_t|y_{1:t-1})$ can be obtained using
\begin{equation}
p(y_t|y_{1:t-1}) \approx \left\{\begin{array}{ll} \sum_{j=1}^J w^{(j)}_{t-1}\tilde{w}^{(j)}_t, & \mbox{for the BF, RM, and PL} \\ \left(\frac{1}{J}\sum_{j=1}^J \tilde{w}^{(j)}_{t-1}\right)\left(\sum_{j=1}^J\tilde{g}^{(j)}_t\right) & \mbox{for the APF and KDPF} \end{array} \right.\label{eqn:onestep:pf}
\end{equation}
\cite[Section 4.2][]{douc:joh:tut:2009}. Given approximations to $p(y_k|y_{1:k-1})$ for $k = 1,\ldots,t$, the marginal likelihood can be approximated via equation \eqref{eqn:marglik}.

Having prescribed a method for approximating $p(y_{1:t})$ sequentially using particle filtering, we can compare a set of $N$ possible models ${M_1,M_2,\ldots,M_N}$ according to their posterior model probabiities, given by
\begin{equation}
p(M_i|y_{1:t}) = \frac{p(y_{1:t}|M_i)p(M_i)}{\sum_{i=1}^N p(y_{1:t}|M_i)p(M_i)}. \label{eqn:modelcomp}
\end{equation}
In Chapter \ref{ch:comp}, we compare estimated marginal likelihoods using the KDPF, RM and PL with the true marginal likelihood under the local level DLM described in Section \ref{sec:dlm:ll} that can be calculated analytically. In Chapter \ref{ch:fmri} we compare relative posterior probabilities among the models $M_{101}$, $M_{011}$, and $M_{001}$ using PL.

\section{Particle MCMC \label{sec:pmcmc}}

At the beginning of this chapter, some of the advantages and disadvantages of both MCMC and SMC were mentioned. A significant amount of research has focused on combining aspects of both types of methods to create more efficient algorithms for sampling from high dimensional posterior distributions. One such algorithm is the RM described in Section \ref{sec:rm} is one such example, which incorporates an MCMC algorithm within the particle filter as a way to avoid degeneracy in fixed parameter values. Particle MCMC (PMCMC) \citep{Andr:Douc:Hol:pmcmc:2010} is another example. This method incorporates a particle filter within an iteration of an MCMC algorithm in order to increase efficiency when ideal proposal distributions are intractable.

The MCMC algorithm proposed in Section \ref{sec:mcmc:epid} for analyzing data from the the epidemic model described in \ref{sec:epid} could be made more efficient by using PMCMC instead. Instead of using Gaussian random walk proposals for sampling each of $x_1,\ldots,x_T$ from their full conditional distributions, as is done in Step \ref{step:gibbs} of the Gibbs sampler in Section \ref{sec:mcmc:epid}, we could instead propose a sample path $x_{0:T}^*$ from $p(x_{0:T}|\theta,y_{1:T})$ using a particle filter. The {\tt pmcmc} function within R package {\tt pomp} \citep{pomp} implements this kind of algorithm to generate samples for the fixed parameters that are asymptotically (as $J \rightarrow \infty$) distributed according to $p(\theta|y_{1:T})$ \citep{Andr:Rob:2009:pseudomarg}.

Let $\theta^{(j)} = \left(\beta^{(j)},\gamma^{(j)},\nu^{(j)}\right)'$ represent the sampled values of the fixed parameters at iteration $j$. The general algorithm implemented by {\tt pmcmc}, called the particle marginal Metropolis-Hastings sampler (PMMH) \cite[Section 2.4.2][]{Andr:Douc:Hol:pmcmc:2010}, proceeds as follows:
\begin{enumerate}
\item Initialization:
\begin{enumerate}
\item Set initial values of the fixed parameters $\theta^{(0)}$,
\item Run an SMC algorithm to generate an approximation to $p(x_{0:T}|y_{1:T},\theta^{(0)})$ via
    \[\hat{p}\left(x_{0:T}|y_{1:T},\theta^{(0)}\right) = \sum_{j=1}^J w_T^{(j)} \delta_{\left(x_{0:T}^{(j)}\right)}(x_{0:T}),\]
\item Sample $x_{0:T}^* \sim \hat{p}\left(x_{0:T}|y_{1:T},\theta^{(0)}\right)$ and calculate an estimate of the marginal likelihood (conditional on $\theta^{(0)}$), denoted $\hat{p}\left(y_{1:T}|\theta^{(0)}\right)$, via equations \eqref{eqn:onestep:pf} and \eqref{eqn:marglik}, and
\item Set $i = 1$.
\end{enumerate}
\item For $i \ge 1$,
\begin{enumerate}
\item Sample $\theta^*$ from some proposal distribution $q(\theta|\theta^{(i-1)})$,
\item Run an SMC algorithm to generate an approximation to $p(x_{0:T}|y_{1:T},\theta^*)$ via
    \[\hat{p}(x_{0:T}|y_{1:T},\theta^*) = \sum_{j=1}^J w_T^{(j)} \delta_{\left(x_{0:T}^{(j)}\right)}(x_{0:T}),\]
\item Sample $x_{0:T}^* \sim \hat{p}(x_{0:T}|y_{1:T},\theta^*)$ and calculate an estimate of the marginal likelihood (conditional on $\theta^*$), denoted $\hat{p}(y_{1:T}|\theta^*)$, via equations \eqref{eqn:onestep:pf} and \eqref{eqn:marglik},
\item With probability
    \[R = \min\left(1, \frac{\hat{p}(y_{1:T}|\theta^*)p(\theta^*)}{\hat{p}\left(y_{1:T}|\theta^{(i-1)}\right)p\left(\theta^{(i-1)}\right)} \frac{q\left(\theta^{(i-1)}|\theta^*\right)}{q\left(\theta^*|\theta^{(i-1)}\right)} \right),\]
    set
    \[\theta^{(i)} = \theta^* \qquad \hat{p}\left(y_{1:T}|\theta^{(i)}\right) = \hat{p}(y_{1:T}|\theta^*).\]
    Otherwise, set
    \[\theta^{(i)} = \theta^{(i-1)} \qquad \hat{p}\left(y_{1:T}|\theta^{(i)}\right) = \hat{p}\left(y_{1:T}|\theta^{(i-1)}\right).\]
\end{enumerate}
\end{enumerate}

Extensions to this algorithm to provide samples for the unobserved states approximately distributed according to $p(x_{0:T}|y_{1:T})$ or joint samples for states and fixed parameters approximately distributed according to $p(x_{0:T},\theta|y_{1:T})$ have yet to be implemented in {\tt pomp} \cite[Section 2.4.3][]{Andr:Douc:Hol:pmcmc:2010}. We implement this algorithm on data simulated from the epidemic model described in Section \ref{sec:mcmc:epid} using {\tt pmcmc}, which samples fixed parameter values using Gaussian random walk proposals with prespecified standard deviations, and the algorithm uses a plain bootstrap filter to sample states and estimate the marginal likelihood. In Chapter \ref{ch:epid}, we compare the performance of the PMCMC algorithm with that of the KDPF and standard MCMC described in Section \ref{sec:mcmc:epid}. 