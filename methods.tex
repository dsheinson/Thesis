\chapter{Methods \label{ch:meth}}

In Chapters \ref{ch:epid} and \ref{ch:fmri}, we consider estimation of states and unknown fixed parameters in Bayesian state-space models for which the filtered distribution cannot be calculated analytically. In this case, the most common approach to approximating this distribution is through Markov-chain Monte Carlo (MCMC) methods \citep{Gelf:Smit:samp:1990}. While an effective tool for analyzing data in complex modeling situations, MCMC can be cumbersome to implement and inefficient in high-dimensional settings. In sequential analysis using state-space models, MCMC is inefficient due to the increase in computational cost incurred by the need for the entire MCMC to be rerun as each new observation arrives.

Sequential Monte Carlo (SMC) - or particle filtering - techniques, on the other hand, enable on-line inference through updating the approximation to the posterior distribution as new data becomes available \citep{Douc:deFr:Gord:sequ:2001, cappe2007overview}. In addition, SMC methods can be flexible, general, easy to implement, amenable to parallel computing, and provide direct estimates of the marginal likelihood. As with MCMC methods, however, the performance of SMC methods suffers as the dimension of the parameter space increases. Furthermore, while MCMC methods directly provide smoothed estimates of states in state-space models, SMC algorithms are inefficient for smoothing and have been mostly used only for filtering \cite[Sec 5][]{douc:joh:tut:2009}.

In Chapters \ref{ch:epid} and \ref{ch:comp}, we compare the performance of several particle filtering algorithms in different model settings. In Chapter \ref{ch:fmri}, we use SMC methods as a tool for model comparison and compare relative posterior probabilities of proposed models for generating fMRI time series data. In each of these chapters, we compare SMC results with MCMC, which has been the standard over the past two decades for Bayesian analysis of intractable models. Thus, in this chapter, we outline several strategies for both MCMC and particle filtering.

\section{Markov-chain Monte Carlo (MCMC) \label{sec:mcmc}}

MCMC methods provide sample-based estimates of the posterior distribution through the generation of dependent samples from distributions whose densities can be evaluated. In this section, we outline the MCMC algorithms used to analyze simulated data from the state-space model of a disease epidemic described in Section \ref{sec:epid} and the dynamic regression models described in Sections \ref{sec:dlm:ll} and \ref{sec:dlm:arwn}. We also describe a particle MCMC (PMCMC) approach that we found to be more efficient for the epidemic model. For more comprehensive descriptions of MCMC and PMCMC methods, we refer the reader to \citet{Robe:Case:mont:2004} and \citet{Andr:Douc:Hol:pmcmc:2010}.

\subsection{MCMC applied to epidemic model \label{sec:mcmc:epid}}

Consider the state-space model of an epidemic described in Section \ref{sec:epid}, where $x_t = (s_t,i_t)'$ is the latent state, $\theta = (\beta,\gamma,\nu)$ are the unknown, fixed parameters, equation \eqref{eqn:epid:state} describes the evolution of $x_t$ given $x_{t-1}$, and \eqref{eqn:epid:obs} gives the likelihood of new data, $y_t$, given the current state, $x_t$. The prior distribution, $p(x_0,\theta)$, is assumed to be of the form
\begin{equation}
p(x_0,\theta) = p(x_0)p(\theta) = p(s_0,i_0)p(\beta,\gamma)p(\nu) \label{eqn:epid:prior}
\end{equation}
where $p(s_0,i_0)$ is given by equation \eqref{eqn:epid:prior:state}.

Suppose we observe syndromic surveillance data, $y_t$, for $t = 1, 2, \ldots, T$. Using the fact that, for state-space models, we can express the joint density of the data, states, and fixed parameters by
\begin{equation}
p(y_{1:T},x_{0:T},\theta) = \prod_{t = 1}^T \left\{p(y_t|x_t,\theta)p(x_t|x_{t-1},\theta)\right\}p(x_0,\theta), \label{eqn:joint}
\end{equation}
we derive the \emph{full conditional distribution}, or distribution conditional on all of the remaining variables, for each of $x_0, x_1, \ldots, x_T$, $\beta$, $\gamma$, and $\nu$ as
\begin{align}
p(x_0|\hdots) &\propto p(x_1|x_0,\theta)p(x_0) \label{eqn:epid:fullcond} \\
p(x_t|\hdots) &\propto p(y_t|x_t)p(x_{t+1}|x_t,\theta)p(x_t|x_{t-1},\theta) \mbox{, for } t = 1,\ldots,T-1 \nonumber \\
p(x_T|\hdots) &\propto p(y_T|x_T)p(x_T|x_{T-1},\theta) \nonumber \\
p(\beta|\hdots) &\propto \prod_{t=1}^T \{p(x_t|x_{t-1},\theta)\}p(\beta, \gamma) \nonumber \\
p(\gamma|\hdots) &\propto \prod_{t=1}^T \{p(x_t|x_{t-1},\theta)\}p(\beta, \gamma) \nonumber \\
p(\nu|\hdots) &\propto \prod_{t=1}^T \{p(x_t|x_{t-1},\theta)\}p(\nu), \nonumber
\end{align}
where $p(x|\hdots)$ represents the full conditional distribution of $x$. Note that, since all of the unknown, fixed parameters show up only in the state equation of the model, their full conditional distributions do not depend on $y_t$.

We use these full conditional distributions to generate samples from $p(x_{0:T},\theta|y_{1:T})$ by implementing a Gibbs sampler with adaptive rejection Metropolis-Hastings (MH) steps \citep{Metr:Rose:Rose:Tell:Tell:equa:1953, Hast:mont:1970,  Gema:Gema:stoc:1984, gilks1995adaptive}. In general, the algorithm works by iteratively sampling each state and fixed parameter, conditional on the current sample, from some proposal distribution, $g$, and accepting the proposed sample with probability $R$, given by
\begin{equation}
R = \frac{f(x^*)g(x|x^*)}{f(x)g(x^*|x)}, \label{eqn:mhrat}
\end{equation}
where $x$ is the current sample, $x^*$ is the proposed sample from $g$, and $f(\cdot)$ is the full conditional distribution of the state or fixed parameter evaluated at `$\cdot$' \cite[Chap 7][]{giv:hoet:2005:comp}. $R$ is termed the \emph{Metropolis-Hastings ratio}. In our algorithm, we use Gaussian random-walk proposals for each state and fixed parameter, i.e. each proposed sample is drawn from a normal distribution centered at the current sampled value and standard deviation given by a tuning parameter that is adjusted according to the MH acceptance rate.

The full Gibbs sampler applied to the epidemic model proceeds as follows:
\begin{enumerate}
\item Start with initial draws $x^{(0)}_{0:T} = (x^{(0)}_0, x^{(0)}_1, \ldots, x^{(0)}_T)'$ and $\theta^{(0)} = (\beta^{(0)}, \gamma^{(0)}, \nu^{(0)})'$. Set $j = 1$.
\item \label{step:gibbs} Sample the states, $x^{(j)}_t$ for $t = 0,1,\ldots,T$, from their full conditional distributions. For $t = 1, 2, \ldots, T$,
    \begin{enumerate}
    \item Draw $x^*_t \sim N(x^{(j-1)}_t,\tau^2_{x_t})$.
    \item Calculate Metropolis ratio, $R$ by
    \[R = \qquad \left\{
    \begin{array}{cc}
    \frac{p(x^{(j-1)}_1|x^*_0,\theta^{(j-1)})p(x^*_0)}{p(x^{(j-1)}_1|x^{(j-1)}_0,\theta^{(j-1)})p(x^{(j-1)}_0)} & \mbox{, if } t = 0 \\
    \frac{p(y_t|x^*_t)p(x^{(j-1)}_{t+1}|x^*_t,\theta^{(j-1)})p(x^*_t|x^{(j)}_{t-1},\theta^{(j-1)})}{p(y_t|x^{(j-1)}_t)p(x^{(j-1)}_{t+1}|x^{(j-1)}_t,\theta^{(j-1)})p(x^{(j-1)}_t|x^{(j)}_{t-1},\theta^{(j-1)})} & \mbox{, if } 1 \le t \le T-1 \\
        \frac{p(y_T|x^*_T)p(x^*_T|x^{(j)}_{T-1},\theta^{(j-1)})}{p(y_T|x^{(j-1)}_T)p(x^{(j-1)}_T|x^{(j)}_{T-1},\theta^{(j-1)})}  & \mbox{, if } t = T
    \end{array}
    \right\}.\]
    \item Draw $u \sim \mbox{Unif}[0,1]$. If $u < \min\{1, R\}$, set $x^{(j)}_t = x^*_t$. Otherwise, set $x^{(j)}_t = x^{(j-1)}_t$.
    \end{enumerate}
\item \label{step:beta} Sample $\beta^{(j)}$ from its full conditional conditional distribution.
    \begin{enumerate}
    \item Draw $\beta^* \sim N(\beta^{(j-1)},\tau^2_{\beta})$.
    \item Calculate Metropolis ratio, $R$ by
    \[R = \frac{\prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\beta^*,\gamma^{(j-1)},\nu^{(j-1)})\}p(\beta^*,\gamma)}{\prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\theta^{(j-1)})\}p(\beta^{(j-1)},\gamma)}.\]
    \item Draw $u \sim \mbox{Unif}[0,1]$. If $u < \min\{1, R\}$, set $\beta^{(j)} = \beta^*$. Otherwise, set $\beta^{(j)} = \beta^{(j-1)}$.
    \end{enumerate}
\item \label{step:gamma} Sample $\gamma^{(j)}$ from its full conditional conditional distribution.
    \begin{enumerate}
    \item Draw $\gamma^* \sim N(\gamma^{(j-1)},\tau^2_{\gamma})$.
    \item Calculate Metropolis ratio, $R$ by
    \[R = \frac{\prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\beta^{(j)},\gamma^*,\nu^{(j-1)})\}p(\beta, \gamma^*)}{\prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\beta^{(j)},\gamma^{(j-1)},\nu^{(j-1)})\}p(\beta, \gamma^{(j-1)})}.\]
    \item Draw $u \sim \mbox{Unif}[0,1]$. If $u < \min\{1, R\}$, set $\gamma^{(j)} = \gamma^*$. Otherwise, set $\gamma^{(j)} = \gamma^{(j-1)}$.
    \end{enumerate}
\item \label{step:nu} Sample $\nu^{(j)}$ from its full conditional conditional distribution.
    \begin{enumerate}
    \item Draw $\nu^* \sim N(\nu^{(j-1)},\tau^2_{\nu})$.
    \item Calculate Metropolis ratio, $R$ by
    \[R = \frac{\prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\beta^{(j)},\gamma^{(j)},\nu^*)\}p(\nu^*)}{\prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\beta^{(j)},\gamma^{(j)},\nu^{(j-1)})\}p(\nu^{(j-1)})}.\]
    \item Draw $u \sim \mbox{Unif}[0,1]$. If $u < \min\{1, R\}$, set $\nu^{(j)} = \nu^*$. Otherwise, set $\nu^{(j)} = \nu^{(j-1)}$.
    \end{enumerate}
\item Set $j = j + 1$ and go back to step \ref{step:gibbs}
\end{enumerate}
The standard deviation of the random-walk proposal distributions, i.e. $\tau_{x_t}$ for $t = 0,1,\ldots,T$, $\tau_{\beta}$, $\tau_{\gamma}$, and $\tau_{\nu}$, are tuning parameters that are adjusted during the burn-in period of the MCMC. During burn-in, if the proposed value of a state or parameter at any given step of the Gibbs sampler is accepted, the corresponding tuning parameter is adjusted by multiplying by 1.1. If the proposed value is rejected, the tuning parameter is adjusted by dividing by 1.1. The idea here is that a high acceptance rate is indicative of a proposal distribution that samples in areas of high probability of the posterior, and a low acceptance rate indicates that the proposal distribution samples more in areas of low posterior probability. We seek proposal distributions that strike a balance in the acceptance rate such that the entire sample space of the posterior is explored. Acceptance rates indicative of optimal efficiency of MH algorithms and mixing of MCMC chains varies by model and has been explored by \citet{Robe:Gel:gilks:1997:optmh} and \citet{bed:2008:optmh}.

\subsection{MCMC applied to dynamic regression \label{sec:mcmc:dr}}

We now derive an MCMC algorithm to sample from the joint posterior distribution of states and unknown parameters from the dynamic intercept ($M_{101}$) and dynamic slope ($M_{011}$) models discussed in Sections \ref{sec:dlm:arwn}. Recall that these models are DLMs of the form given in equations \eqref{eqn:dlm:reg:obs} and \eqref{eqn:dlm:reg:state} with
\begin{align*}
U_t &= (1, u_t) &\quad \beta &= (\beta_0, \beta_1)' \\
V &= \sigma^2_m &\quad F_t &= \left\{\begin{array}{ll} 1, & \mbox{for} M_{101} \\ u_t, & \mbox{for} M_{011} \end{array}\right. \\
G &= \phi &\quad W &= \sigma^2_s,
\end{align*}
where $x_t$ is the univariate state representing the change in the intercept (or slope) at time $t$ and $\theta = (\beta',\phi,\sigma^2_s,\sigma^2_m)'$ are the unknown, fixed parameters. We place a prior of the form
\begin{equation}
p(x_0, \theta) = p(x_0)p(\beta|\sigma^2_m)p(\sigma^2_m)p(\phi|\sigma^2_s)p(\sigma^2_s) \label{eqn:dr:prior}
\end{equation}
on the initial state and fixed parameters, where, as stated in equations \eqref{eqn:dynreg:prior1} and \eqref{eqn:dynreg:prior2},
\begin{align*}
&\beta|\sigma^2_m \sim \mbox{N}(\vartheta_0, \sigma^2_mB_0) \qquad \sigma^2_m \sim \mbox{IG}(a_{m_0}, b_{m_0}) \\
&\phi|\sigma^2_s \sim \mbox{N}(\varphi_0, \sigma^2_s\Phi_0) \qquad \sigma^2_s \sim \mbox{IG}(a_{s_0}, b_{s_0})
\end{align*}
and $x_0 = 0$ (i.e. $p(x_0) = \delta_{(0)}$). The conjugate form of these priors, conditional on $x_t$, allows for direct sampling from the full conditional distributions of the fixed parameters. Combining this with the forward-filtering backward sampling (FFBS) algorithm for jointly sampling the states \citep{Cart:Kohn:on:1994} allows for a relatively straightforward Gibbs sampler.

Suppose we observe $y_t$ for $t = 1,2,\ldots,T$. We generate samples from $p(x_{0:T},\theta|y_{1:T})$ using the following Gibbs sampling algorithm:
\begin{enumerate}
\item Start with initial draws $\theta^{(0)} = ({\beta^{(0)}}', \phi^{(0)}, {\sigma^2_s}^{(0)}, {\sigma^2_m}^{(0)})'$ and \\ $x^{(0)}_{0:T} = (x^{(0)}_0, x^{(0)}_1, \ldots, x^{(0)}_T)$. Set $j = 1$.
\item \label{step:gibbs:dr:beta} Jointly sample ${\sigma^2_m}^{(j)} \sim \mbox{IG}(a_{m_T}, b_{m_T})$ and $\beta^{(j)}|{\sigma^2_m}^{(j)} \sim \mbox{N}(\vartheta_T,{\sigma^2_m}^{(j)}B_T)$, where
\begin{align}
a_{m_T} &= T/2 + a_{m_0} \label{eqn:dr:am} \\
b_{m_T} &= \frac{1}{2}(\mbox{SS}_y + \vartheta_0'B_0^{-1}\vartheta_0 - \vartheta_T'B_T^{-1}\vartheta_T) + b_{m_0} \label{eqn:dr:bm} \\
\mbox{SS}_y &= \sum^T_{t=1} \left((y_t - F_tx_t)'(y_t - F_tx_t)\right) \label{eqn:dr:ssy} \\
\vartheta_T &= B_T\left( \sum^T_{t=1} U_t'(y_t - F_tx_t) + B_0^{-1}\vartheta_0\right) \label{eqn:dr:b} \\
B_T &= \left(\sum^T_{t=1} U_t'U_t + B_0^{-1}\right)^{-1} \label{eqn:dr:B}
\end{align}
\item \label{step:gibbs:dr:phi} Jointly sample ${\sigma^2_s}^{(j)} \sim \mbox{IG}(a_{s_T}, b_{s_T})$ and $\phi^{(j)}|{\sigma^2_s}^{(j)} \sim \mbox{N}(\varphi_T,{\sigma^2_s}^{(j)}\Phi_T)$, where
\begin{align}
a_{s_T} &= T/2 + a_{s_0} \label{eqn:dr:as} \\
b_{s_T} &= \frac{1}{2}(\mbox{SS}_x + \varphi_0'\Phi_0^{-1}\varphi_0 - \varphi_T'\Phi_T^{-1}\varphi_T) + b_{s_0} \label{eqn:dr:bs} \\
\mbox{SS}_x &= \sum^T_{t=1} x_t^2 \label{eqn:dr:ssx} \\
\phi_T &= \Phi_T\left( \sum^T_{t=1} x_tx_{t-1} + \Phi_0^{-1}\varphi_0\right) \label{eqn:dr:phi} \\
\Phi_T &= \left(\sum^T_{t=1} x_{t-1}^2 + \Phi_0^{-1}\right)^{-1} \label{eqn:dr:Phi}
\end{align}
\item \label{step:ffbs} Sample $x_{0:t}^{(j)}$ using the following FFBS algorithm \cite[Sec 4.4][]{petris:camp:2009:dynamic}:
\begin{enumerate}
\item Starting with initial values $m_0 = C_0 = 0$, calculate $z_t$, $R_t$, $m_t$ and $C_t$ for $t = 1,2,\ldots,T$ using the Kalman filter (equation \eqref{eqn:dlm:kal}).
\item Draw $x_T^{(j)} \sim \mbox{N}(m_T,C_T)$. Then, for $t = T-1,\ldots,0$, draw $x_t^{(i)} \sim \mbox{N}(h_t,H_t)$, where
    \begin{align*}
    h_t &= m_t + C_tG'R_{t+1}^{-1}(x_{t+1}^{(j)} - z_{t+1}) \\
    H_t &= C_t - C_tG'R_{t+1}^{-1}GC_t
    \end{align*}
\item Set $j = j + 1$ and go back to Step \ref{step:gibbs:dr:beta}
\end{enumerate}
\end{enumerate}
Note that this algorithm as well as the MCMC for the epidemic model described in Section \ref{sec:mcmc:epid} provide joint samples from $p(x_{0:T},\theta|y_{1:T})$ from which samples from the smoothed distributions, $p(x_s,\theta|y_{1:T})$ for $s < T$, can be directly obtained by Monte Carlo integration.

\section{Particle filtering \label{sec:filtering}}

Particle filtering is an SMC inferential technique based on repeated use of importance sampling. It aims to approximate the filtered distribution at time $t$ through a weighted Monte Carlo realization from this distribution in terms of $J$ particles, i.e.
\begin{equation}
p(x_t,\theta| y_{1:t}) \approx \sum_{j=1}^J w_t^{(j)} \delta_{\left(x_t^{(j)},\theta^{(j)}\right)} \label{eqn:approx}
\end{equation}
where $\left(x_t^{(j)},\theta^{(j)}\right)$ is the location of the $j^{\mbox{th}}$ particle at time $t$, $w_t^{(j)}$ is the weight of that particle with $\sum_{j=1}^J w_t^{(j)}=1$, and $\delta_x$ is the Dirac delta function with point mass at $x$. A variety of SMC techniques have been developed to provide more efficient approximations to $p(x_t,\theta|y_{1:t})$ in the sense that with the same computation time a better approximation is achieved. In this section, we describe five particle filtering techniques: the bootstrap filter (BF), auxiliary particle filter (APF), kernel density particle filter (KDPF), resample-move particle filter (RM), and particle learning algorithm (PL).

Each of the five algorithms comes with its own advantages and disadvantages. The BF and APF are the simplest and most straightforward to implement, but are unequipped to efficiently deal with state-space models that contain unknown fixed parameters. The PL performs the most efficiently, but can only be applied to special cases of state-space models such as DLMs. The RM, while equipped to handle state-space models of any form, requires an MCMC step in addition to the SMC, and thus is not a truly sequential algorithm. The KDPF, while being the only truly sequential particle filtering algorithm that can be applied to any state-space model, is outperformed by RM, PL, and MCMC in many model settings.

In Chapter \ref{ch:epid}, we compare the efficiency of the BF, APF, and KDPF in the syndromic surveillance context. In Chapter \ref{ch:comp}, we compare the KDPF, RM, and PL in terms of their efficiency in estimating the marginal likelihood. Finally, in Chapter \ref{ch:fmri}, we employ the PL for estimating states and unknown fixed parameters in DLMs using real and simulated fMRI data.

\subsection{Bootstrap filter \label{sec:bf}}

The BF is first successful version of the particle filter \citep{Gord:Salm:Smit:nove:1993,Kita:mont:1996}. Since this method and the APF were developed for the situation when $\theta$ is known, we will (for the moment) drop $\theta$ from the notation. Given an approximation to the filtered distribution at time $t$ as in equation \eqref{eqn:approx}, to obtain an approximation to the filtered distribution at time $t+1$, perform the following steps for each particle $j=1,\ldots,J$:
\begin{enumerate}
\item Resample: sample an index $k\in\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\left\{w_t^{(1)},\ldots,w_t^{(j)},\ldots,w_t^{(J)}\right\}$,
\item Propagate: sample $x_{t+1}^{(j)} \sim p\left( x_{t+1}\left|x_t^{(k)}\right.\right)$, and
\item Calculate weights and renormalize:
\[ \tilde{w}_{t+1}^{(j)} = p\left(y_{t+1}\left|x_{t+1}^{(j)}\right.\right) \qquad w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/ \sum_{l=1}^J \tilde{w}_{t+1}^{(l)} \right. .\]
\end{enumerate}
This procedure can be applied recursively beginning with an initial set of weights $w_0^{(j)}$ and locations $x_0^{(j)}$ for all $j$, usually obtained by sampling from the prior with uniform weights.

\subsection{Auxiliary particle filter \label{sec:apf}}

One problem that arises in implementing the BF is that $w_t^{(j)}$ will be small for particles where $p\left(y_{t}\left|x_{t}^{(j)}\right.\right)$ is small, and these particles will contribute little to the approximation to $p(x_{t}|y_{1:t})$. The APF aims to mitigate this by anticipating which particles will have small weight using a look ahead strategy \citep{Pitt:Shep:filt:1999}. Given an approximation to the filtered distribution at time $t$ as in equation \eqref{eqn:approx}, the APF approximates $p(x_{t+1}|y_{1:t+1})$ by the following:
\begin{enumerate}
\item For each particle $j$, calculate a point estimate of $x_{t+1}^{(j)}$ called $\mu_{t+1}^{(j)}$, e.g.
\[ \mu_{t+1}^{(j)} = E\left(x_{t+1}\left|x_t^{(j)} \right.\right). \]
\item Calculate auxiliary weights and renormalize:
\[ \tilde{g}_{t+1}^{(j)} = w_t^{(j)} p\left(y_{t+1}\left|\mu_{t+1}^{(j)}\right.\right) \qquad g_{t+1}^{(j)} = \tilde{g}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{g}_{t+1}^{(l)}.\right. \]
\item For each particle $j=1,\ldots,J$,
	\begin{enumerate}
    \item Resample: sample an index $k\in\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\left\{g_{t+1}^{(1)},\ldots,g_{t+1}^{(j)},\ldots,g_{t+1}^{(J)}\right\}$,
	\item Propagate: sample $x_{t+1}^{(j)} \sim p\left(x_{t+1}\left|x_t^{(k)}\right.\right)$, and
	\item Calculate weights and renormalize:
\[ \tilde{w}_{t+1}^{(j)} = \frac{p\left(y_{t+1}\left|x_{t+1}^{(j)}\right.\right)}{p\left(y_{t+1}\left|\mu_{t+1}^{(k)}\right.\right)} \qquad w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{w}_{t+1}^{(l)}. \right. \]
	\end{enumerate}
\end{enumerate}
The point estimate used in Step 1 can be any point estimate, although the expectation is commonly used. Step 3 is exactly the same as the BF with appropriate modifications to the weight calculation to adjust for the `look ahead' in steps 1 and 2. APF weights tend to be closer to uniform than BF weights, in which case a better approximation to $p(x_{t}|y_{1:t})$ is achieved.

The BF and the APF were constructed with the idea that all fixed parameters are known. In order to simultaneously estimate the time-evolving states and fixed parameters using either the BF or APF, it is necessary to incorporate the fixed parameters into the state with degenerate evolutions. That is, one regards the fixed parameters as elements of the state vector $x_t$ and specifies the state evolution equation such that these elements do not change over time. Due to the possible duplication of some particles and elimination of others through resampling, the number of unique values of the fixed parameters in the particle set will decrease over time, resulting in degeneracy in the fixed parameters \citep{Liu:West:comb:2001}.

\subsection{Kernel density particle filter \label{sec:kd}}

The particle filter introduced by \citet{Liu:West:comb:2001}, which we refer to as the KDPF, builds on the APF and provides a general way of fighting degeneracy in fixed parameters. This is done by approximating the set of fixed parameter values by a kernel density estimate and then regenerating values from this approximation. This filter approximates $p(x_t,\theta| y_{1:t})$ via equation \eqref{eqn:approx}. To make the notation transparent, we introduce subscripts for our fixed parameters, e.g. $\theta_t^{(j)}$ represents the value for $\theta$ at time $t$ for particle $j$. This does not imply that the true $\theta$ is dynamic, but rather that particle $j$ can have different values for $\theta$ throughout time.

Let $\bar{\theta}_t$ and $V_t$ be the weighted sample mean and weighted sample covariance matrix of $\theta_t^{(1)},\ldots,\theta_t^{(J)}$.  The KDPF uses a tuning parameter $\Delta$, the discount factor that takes values in $(0,1)$, and two derived quantities $h^2 = 1 - ((3\Delta - 1)/2\Delta)^2$ and $a^2 = 1 - h^2$ that determine how smooth the kernel density approximation is. Lower values of $\Delta$ result in a smoother approximation. However, the goal here is simply to jitter particles around to refresh values of the fixed parameters and reduce the chance of degeneracy, and so $\Delta$ is typically taken to be between 0.95 and 0.99 \citep{Liu:West:comb:2001}.

Given an approximation to the filtered distribution at time $t$ as in equation \eqref{eqn:approx}, the KDPF provides an approximation to $p(x_{t+1},\theta|y_{1:t+1})$ by the following steps:
\begin{enumerate}
\item For each particle $j$, set $m_t^{(j)} = a\theta_t^{(j)} + (1-a)\bar{\theta}_t$ and calculate a point estimate of $x_{t+1}^{(j)}$ called $\mu_{t+1}^{(j)}$, e.g. $\mu_{t+1}^{(j)} = E\left(x_{t+1}\left|x_t^{(j)},\theta_t^{(j)} \right.\right)$.
\item Calculate auxiliary weights and renormalize:
\[ \tilde{g}_{t+1}^{(j)} = w_t^{(j)} p\left(y_{t+1}\left|\mu_{t+1}^{(j)},m_t^{(j)}\right.\right) \qquad g_{t+1}^{(j)} = \tilde{g}_{t+1}^{(j)}\left/ \sum_{l=1}^J \tilde{g}_{t+1}^{(l)}. \right. \]
\item For each particle $j=1,\ldots,J$,
	\begin{enumerate}
    \item Resample: sample an index $k\in\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\left\{g_{t+1}^{(1)},\ldots,g_{t+1}^{(j)},\ldots,g_{t+1}^{(J)}\right\}$,
	\item Regenerate the fixed parameters: sample $\theta_{t+1}^{(j)} \sim \mbox{N}\left(m_t^{(k)}, h^2V_t \right)$,
	\item Propagate: sample $x_{t+1}^{(j)} \sim p\left(x_{t+1}\left|x_t^{(k)},\theta_{t+1}^{(j)}\right.\right)$, and
	\item Calculate weights and renormalize:
	\[ \tilde{w}_{t+1}^{(j)} = \frac{p\left(y_{t+1}\left|x_{t+1}^{(j)},\theta_{t+1}^{(j)}\right.\right)}{p\left(y_{t+1}\left|\mu_{t+1}^{(k)},m_t^{(k)}\right.\right)}
	\qquad
	w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{w}_{t+1}^{(l)}. \right. \]
	\end{enumerate}
\end{enumerate}
The KDPF adds the kernel density regeneration to the auxiliary particle filter. Here, we use a mixture distribution that places normal kernels around each particle, where the mean of each kernel is a weighted average between the particle value and the overall mean of all particles. This ensures that the variance of regenerated fixed parameter values at a specific time point does not change \citep{Liu:West:comb:2001}.

To use the KDPF with normal kernels, it is necessary to parameterize the fixed parameters so that their support is on the real line. This is not a constraint, but rather a practical implementation detail. We typically use logarithms for parameters that have positive support and the logit function for parameters in the interval (0,1). A parameter $\psi$ bounded on the interval (a,b) can first be rebounded to (0,1) through $(\psi-a)/(b-a)$, and then the logit transformation can be applied. We investigate the sensitivity of the performance of the particle filters to the choice of transformation in Chapter \ref{ch:epid}.

\subsection{Resample-move algorithm} \label{sec:rm}

In Chapter \ref{ch:epid}, we show that the KDPF can be an effective tool for handling unknown, fixed parameters in state-space models. However, the choice of a mixture normal distribution for regenerating fixed parameter values is somewhat arbitrary, and efficiency of the algorithm can be increased by using a kernel that matches $p(\theta|y_{1:t})$ more closely. The resample-move algorithm (RM), introduced by \citet{Gilk:Berz:foll:2001}, does precisely this by regenerating fixed parameter values from an MCMC transition kernel with stationary distribution equal to $p(\theta|y_{1:t})$. The algorithm works by running one or a few iterations of an MCMC algorithm within each step of the particle filter for the purpose of jittering fixed parameter values. Since the weighted sample of fixed parameter values already represents an approximation to $p(\theta|y_{1:t})$, the resulting sample after running an MCMC for each particle yields a sample that can only improve the approximation.

Because an MCMC kernel will often depend on all the observed data and unobserved states, we must track the entire history of states in each particle. That is, particle $j$ is represented by $\left(x_{0:t}^{(j)},\theta_t^{(j)}\right)$ with weight $w_t^{(j)}$, where $x_{0:t}^{(j)} = (x_0^{(j)},x_1^{(j)},\ldots,x_t^{(j)})$. The entire collection of $J$ particles now represents an approximation to $p(x_{0:t},\theta|y_{1:t})$. The general RM algorithm proceeds as follows. Given a particle approximation to the posterior, $p(x_{0:t},\theta|y_{1:t})$, we move to a particle approximation to $p(x_{0:t+1},\theta|y_{1:t+1})$ by the following steps for each particle $j=1,\ldots,J$:
\begin{enumerate}
\item Propagate: draw $\tilde{x}^{(j)}_{t+1}$ from $p\left(x_{t+1}|x^{(j)}_t,\theta^{(j)}_t\right)$. Add $\tilde{x}^{(j)}_{t+1}$ to particle $j$ and denote the new augmented particle by $\left(\tilde{x}^{(j)}_{0:t+1},\theta^{(j)}_t\right)$.
\item Calculate weights and renormalize:
    \[\tilde{w}^{(j)}_t = p\left(y_{t+1}|\tilde{x}^{(j)}_{t+1},\theta^{(j)}_t\right) \qquad w^{(j)}_{t+1} = \tilde{w}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{w}_{t+1}^{(l)}. \right.\]
\item Resample: sample an index $k$ from $\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\{w^{(1)}_{t+1},\ldots,w^{(j)}_{t+1},\ldots,w^{(J)}_{t+1}\}$.
\item \label{step:move} Move particles: draw a new particle $\left(x^{(j)}_{0:t+1},\theta^{(j)}_{t+1}\right)$ from some transition kernel $q\left(x_{0:t+1},\theta|\tilde{x}^{(k)}_{0:t+1},\theta^{(k)}_t\right)$ with invariant distribution $p(x_{0:t+1},\theta|y_{1:t+1})$.
\end{enumerate}

In Chapter \ref{ch:comp}, we run this algorithm on data simulated from the local level DLM with unknown common variance factor, $\theta$ (described in Section \ref{sec:dlm:ll}). To do this, we define an MCMC algorithm for the ``Move particles'' step in the above algorithm. In this case, this step proceeds as follows, for each $j = 1,2,\ldots,J$:
\begin{enumerate}
\item \label{step:move:ll} Sample $\theta^{(j)}_{t+1} \sim \mbox{IG}(a_t, b_t)$, where
\begin{align*}
a_t &= a_0 + 1/2 + t \\
b_t &= b_0 + \frac{1}{2}\left(\sum_{k=1}^t (y_k - x_k^{(j)})^2 + \frac{1}{\lambda}\sum_{k=1}^t (x_k^{(j)} - x_{k-1}^{(j)})^2 + {x^{(j)}_0}^2\right)
\end{align*}
\item Sample $x_{0:t+1}^{(j)}$ using FFBS (see Step \ref{step:ffbs} of Gibbs sampler from Section \ref{sec:mcmc:dr}) with
\[m_0 = 0 \quad C_0 = V = \theta^{(j)}_{t+1} \quad W = \theta_{t+1}^{(j)}\lambda \quad F_t = G = 1\]
\end{enumerate}
Notice in this algorithm that, because of the increase in computation required with increasing $t$, the RM is not a truly sequential particle filter.

\subsection{Particle Learning \label{sec:pl}}

We consider a particle filtering algorithm called particle learning \citep{Carv:Joha:Lope:Pols:part} that can be used for a particular class of state-space models which includes DLMs. For models within this class, particle learning prescribes a truly sequential algorithm that samples new values for $\theta$ from $p(\theta|y_{1:t})$ using conditional sufficient statistics. Let $s_t$ denote the sufficient statistics for $\theta$ conditional on the current state $x_t$. Then, we add the sampled values of the sufficient statistics, $s_t^{(j)}$, to the particles, i.e. particle $j$ and time $t$ is now represented by $\left(x_t^{(j)},s_t^{(j)},\theta_t^{(j)}\right)$. We move from an approximation to $p(x_t,\theta|y_{1:t})$ to that of $p(x_{t+1},\theta|y_{1:t+1})$ by the following procedure for each particle $j = 1,2,\ldots,J$:
\begin{enumerate}
\item Calculate weights an renormalize:
    \[\tilde{w}_{t+1}^{(j)} = p\left(y_{t+1}|x_t^{(j)},\theta_t^{(j)}\right) \qquad w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)} \left/ \sum_{l=1}^J \tilde{w}_{t+1}^{(l)}\right.,\]
\item Resample: sample an index $k \in \{1,\ldots,j,\ldots,J\}$ with associated probabilities $\{w_{t+1}^{(1)},\ldots,w_{t+1}^{(j)},\ldots,w_{t+1}^{(J)}\}$
\item Propagate: sample $x_{t+1}^{(j)} \sim p\left(x_{t+1}|y_{t+1},x_t^{(k)},\theta_t^{(k)}\right)$,
\item Update: calculate $s_{t+1}^{(j)} = S\left(y_{t+1},x_{t+1}^{(j)},s_t^{(k)}\right)$,
\item Regenerate: sample $\theta_{t+1}^{(j)} \sim p\left(\theta|s_{t+1}^{(j)}\right)$.
\end{enumerate}
Note that the algorithm requires the ability to evaluate the \emph{conditional predictive distribution} $p(y_{t+1}|x_t,\theta)$, sample from the filtered distribution of $x_{t+1}$ conditional on the previous state and fixed parameters $p(x_{t+1}|y_{t+1},x_t,\theta)$, and sample from $p(\theta|s_t)$. Thus, particle learning is only applicable to models for which the form of these distributions is analytically tractable. In addition, we must define the recursive map $S$ to update the sufficient statistics based on the new observation $y_{t+1}$ and the newly sampled state $x_{t+1}^{(j)}$. We now show how to implement the PL for the local level DLM described in Section \ref{sec:dlm:ll} and the dynamic regression models described Section \ref{sec:dlm:arwn}.

\subsubsection{PL for local level DLM}

To implement a particle learning algorithm for the local level DLM given by equations \eqref{eqn:ll:obs} and \eqref{eqn:ll:state}, we derive the conditional predictive distribution and filtered distribution of the states conditional on $\theta$, given by
\begin{align}
&y_{t+1}|x_t,\theta \sim \mbox{N}\left(x_t,\theta(1+\lambda)\right) \label{eqn:pl:ll:pred} \\
&x_{t+1}|y_{t+1},x_t,\theta \sim \mbox{N}(\mu_t,\tau^2) \label{eqn:pl:ll:state}
\end{align}
where \[\mu_t = \frac{\lambda}{1+\lambda}(y_{t+1} + x_t / \lambda) \qquad \tau^2 = \theta\frac{\lambda}{1+\lambda}.\]
The filtered distribution of $\theta$ conditional on the states, $p(\theta|y_{1:t},x_{0:t})$, is given by
\begin{equation}
\theta|y_{1:t},x_{0:t} \sim \mbox{IG}(a_t,b_t)
\end{equation}
where
\begin{align*}
a_t &= t + 1/2 + a_0 \\
b_t &= b_0 + \frac{1}{2}\left(\sum_{k=1}^t (y_k - x_k)^2 + \frac{1}{\lambda}\sum_{k=1}^t (x_k - x_{k-1})^2 + x_0^2\right)
\end{align*}
Thus, $s_t = (a_t,b_t)$ are the conditional sufficient statistics for $\theta$ at time $t$, updated by the recursive map $S$ defined by
\begin{align}
a_{t+1} &= a_t + 1, t \ge 1 \label{eqn:pl:ll:delta} \\
b_{t+1} &= \frac{1}{2}\left((y_{t+1}-x_{t+1})^2 + \frac{1}{\lambda}(x_{t+1}-x_t)^2\right) + b_t, t \ge 1 \label{eqn:pl:ll:nu}
\end{align}
with initial conditions
\begin{align*}
a_1 &= 3/2 + a_0 \\
b_1 &= \frac{1}{2}\left((y_1-x_1)^2 + \frac{1}{\lambda}(x_1-x_0)^2 + x_0^2\right) + b_0
\end{align*}

\subsubsection{PL for dynamic regression}

Consider the dynamic regression models $M_{101}$ and $M_{011}$ described in Section \ref{sec:dlm:arwn} and given by equations \eqref{eqn:dlm:reg:obs} and \eqref{eqn:dlm:reg:state} with
\begin{align*}
U_t &= (1, u_t) &\quad \beta &= (\beta_0, \beta_1)' \\
V &= \sigma^2_m &\quad F_t &= \left\{\begin{array}{ll} 1, & \mbox{for} M_{101} \\ u_t, & \mbox{for} M_{011} \end{array}\right. \\
G &= \phi &\quad W &= \sigma^2_s,
\end{align*}

To implement a particle learning algorithm with this model, we derive the conditional predictive and conditional filtered distributions
\begin{align}
&y_{t+1}|x_t,\theta \sim \mbox{N}(U_{t+1}\beta + F_t\phi x_t, F_t^2\sigma^2_s + \sigma^2_m) \label{eqn:pl:dr:pred} \\
&x_{t+1}|y_{t+1},x_t,\theta \sim \mbox{N}(\mu_t,\tau^2) \label{eqn:pl:dr:state}
\end{align}
where \[\mu_t = \tau^2\left(\frac{(y_{t+1}-U_{t+1}\beta)F_{t+1}}{\sigma^2_m} + \frac{\phi x_t}{\sigma^2_s}\right) \qquad \tau^2 = \left(\frac{F_{t+1}^2}{\sigma^2_m} + \frac{1}{\sigma^2_s}\right)^{-1},\]
In addition, we derive $p(\theta|y_{1:t},x_{0:t})$ using the fact that
\begin{equation}
p(\theta|y_{1:t},x_{0:t}) \propto \left(\prod_{k=1}^t p(y_k|x_k,\beta,\sigma^2_m)p(x_k|x_{k-1},\phi,\sigma^2_s)\right)p(\beta,\sigma^2_m)p(\beta,\sigma^2_m) \label{eqn:pl:dr:theta}
\end{equation}
where the prior distributions $p(\beta,\sigma^2_m)$ and $p(\phi,\sigma^2_s)$ are specified by
\begin{align}
\beta|\sigma^2_m &\sim \mbox{N}(\vartheta_0, \sigma^2_m B_0) &\quad \sigma^2_m &\sim \mbox{IG}(a_{m_0},b_{m_0}) \label{eqn:pl:prior:beta} \\
\phi|\sigma^2_s &\sim \mbox{N}(\varphi_0, \sigma^2_s \Phi_0) &\quad \sigma^2_s &\sim \mbox{IG}(a_{s_0},b_{s_0}) \label{eqn:pl:prior:phi}
\end{align}
with known hyperparameters $\vartheta$, $B_0$, $\varphi_0$, $\Phi_0$, $a_{m_0}$, $b_{m_0}$, $a_{s_0}$, and $b_{s_0}$. The filtered distribution for $\theta$ conditional on the states is then given by
\begin{align}
\beta|\sigma^2_m,y_{1:t},x_{0:t} &\sim \mbox{N}(\vartheta_t, \sigma^2_m B_t) &\quad \sigma^2_m|y_{1:t},x_{0:t} &\sim \mbox{IG}(a_{m_t},b_{m_t}) \label{eqn:pl:post:beta} \\
\phi|\sigma^2_s,y_{1:t},x_{0:t} &\sim \mbox{N}(\varphi_t, \sigma^2_s \Phi_t) &\quad \sigma^2_s|y_{1:t},x_{0:t} &\sim \mbox{IG}(a_{s_t},b_{s_t}) \label{eqn:pl:post:phi}
\end{align}
where $\vartheta_t$, $B_t$, $\varphi_t$, $\Phi_t$, $a_{m_t}$, $b_{m_t}$, $a_{s_t}$, and $b_{m_t}$ are calculated according to the equations in Steps \ref{step:gibbs:dr:beta} and \ref{step:gibbs:dr:phi} of the Gibbs sampler outlined in Section \ref{sec:mcmc:dr} (with $T = t$). Thus, $s_t = (\vartheta_t, B_t, a_{m_t}, \xi_{m_t}, \varphi_t, \Phi_t, a_{s_t}, \xi_{s_t})$ form the sufficient statistics for $\theta$ and are updated by the recursive map given by
\begin{align}
B_t^{-1}\vartheta_t &= B_{t-1}^{-1}\vartheta_{t-1} + U_t'(y_t - F_tx_t) & B_t^{-1} &= B_{t-1}^{-1} + U_t'U_t \label{eqn:pl:dr:suff} \\
a_{m_t} &= a_{m_{t-1}} + 1/2 & \xi_{m_t} &= \xi_{m_{t-1}} + (y_t - F_tx_t)^2 \nonumber \\
\Phi_t^{-1}\varphi_t &= \Phi_{t-1}^{-1}\varphi_{t-1} + x_tx_{t-1} & \Phi_t^{-1} &= \Phi_{t-1}^{-1} + x_{t-1}^2 \nonumber \\
a_{s_t} &= a_{s_{t-1}} + 1/2 & \xi_{s_t} &= \xi_{s_{t-1}} + x_t^2 \nonumber
\end{align}
Note that we update $\xi_{m_t}$ and $\xi_{s_t}$ in the recursive map and calculate the inverse-gamma rate parameters $b_{m_t}$ and $b_{s_t}$ according to
\begin{align}
b_{m_t} &= \frac{1}{2}\left(\xi_{m_t} + \vartheta_0'B_0^{-1}\vartheta_0 - \vartheta_t'B_t^{-1}\vartheta_t\right) + b_{m_0} \label{eqn:pl:dr:rate} \\
b_{s_t} &= \frac{1}{2}\left(\xi_{s_t} + \varphi_0'\Phi_0^{-1}\varphi_0 - \varphi_t'\Phi_t^{-1}\varphi_t\right) + b_{s_0}. \nonumber
\end{align}

\section{Resampling \label{sec:resample}}

In addition to choosing which particle filter algorithm to use, successful implementation also depends on which resampling scheme to use and when to resample. Resampling is sampling (with replacement) random indices between 1 and $J$, where index $j$ has probability $w^{(j)}$ of being selected. Throughout our discussion, we have explicitly used multinomial resampling, but alternative resampling schemes exist including residual, stratified, and systematic resampling \citep{Douc:Capp:Moul:comp:2005}. Residual resampling deterministically samples $\lfloor w^{(j)} J \rfloor$ copies of particle $j$, for each $j$, and distributes the remaining $J - \sum^J_{j=1} \lfloor w^{(j)} J \rfloor$  particles according to a multinomial distribution with associated probabilities $(w^{(j)} J - \lfloor w^{(j)} J \rfloor) / (J - \sum^J_{j=1} \lfloor w^{(j)} J \rfloor)$, where $\lfloor . \rfloor$ is the largest integer less than or equal ``.''. Stratified resampling samples uniformly over the interval $[(j-1) / J, j / J]$, for $j = 1, 2, \ldots, J$, and calculates the number of copies of particle $j$ according to the empirical cumulative distribution function of the particle indices (i.e. the ``inversion method''). Finally, systematic resampling is similar to stratified resampling, except that only one uniform draw is initially sampled from $[0, 1/J]$ and the remaining $J-1$ are calculated by adding $(j-1) / J$ to the sampled value prior to applying the inversion method.

Resampling is meant to rebalance the weights of the particles in order to avoid degeneracy, but this introduces additional Monte Carlo variability to the particle sample. Despite systematic resampling only requiring a single uniform draw, \citet{Douc:Capp:Moul:comp:2005} show via example that it can introduce more Monte Carlo variability than the other three resampling schemes. In Chapter \ref{ch:epid}, we discuss some advantages and disadvantages of the different resampling methods when applied to our specific model of a disease outbreak and suggest the use of stratified or residual resampling.

The frequency of resampling should be reduced to balance the loss of information due to degeneracy with the loss of information due to the additional Monte Carlo variability introduced during resampling. Typically, a measure of the nonuniformity of particle weights is used to determine if resampling should be performed at a given iteration of a particle filter. The common measures are effective sample size, coefficient of variation, and entropy. We use effective sample size \citep{Liu:Chen:Wong:reje:1998}, a value ranging between 1 and $J$ that can be interpreted as the number of independent particle samples. An effective sample size of $J$ corresponds to all particle weights being equal, and a value of 1 corresponds to one particle weight being 1 with the rest 0. Using this measure of nonuniformity in our runs, we set a threshold of $0.8J$, meaning that if the number of independent samples is less than 80\% of the total number of particles at time $t$, resampling is performed at that time.

The algorithms described in Sections \ref{sec:bf} through \ref{sec:pl} were constructed under the assumption that resampling is performed at every iteration of the filter. However, in practice, we omit the resampling step in each algorithm at each time point where the effective sample size exceeds $0.8J$. If resampling is not performed, we modify the algorithm at that timepoint by 1) omitting the `Resample' step, 2) replacing all instances of the sampled index $k$ with the particle index $j$, and 3) adjusting the calculation of $\tilde{w}_{t+1}^{(j)}$ by multiplying by $w_t^{(j)}$ (in the BF, RM, and, PL) or $\tilde{g}^{(j)}_{t+1}$ (in the APF and KDPF), i.e. the particle weights get carried over. For the KDPF, RM, and PL, regeneration is not performed when resampling is not performed since, in this case, there is no reduction in the number of unique fixed parameter values. In this case, we let $\theta_{t+1}^{(j)} = \theta_t^{(j)}$ for all $j$.

\section{Model Comparison \label{sec:comp}}

Each of the particle filters described in previous sections, in addition to generating a weighted sample approximation to $p(x_t,\theta|y_{1:t})$, provide an approximation to the marginal likelihood, $p(y_{1:t})$. We first note that given $p(y_{1:t-1})$, $p(y_{1:t})$ can be updated recursively by
\begin{equation}
p(y_{1:t}) = p(y_t|y_{1:t-1})p(y_{1:t-1}). \label{eqn:marglik:recurse}
\end{equation}
Thus, $p(y_{1:t})$ can be calculated through the one-step ahead predictive densities, $p(y_k|y_{1:k-1})$, for $k=1,\ldots,t$ (letting $p(y_1|y_0) = p(y_1)$) according to
\begin{equation}
p(y_{1:t}) = \prod_{k=1}^t p(y_k|y_{1:k-1}). \label{eqn:marglik}
\end{equation}
At each step of the particle filter (i.e. at each time $t$), an approximation to $p(y_t|y_{1:t-1})$ can be obtained using
\begin{equation}
p(y_t|y_{1:t-1}) \approx \left\{\begin{array}{ll} \sum_{j=1}^J w^{(j)}_{t-1}\tilde{w}^{(j)}_t, & \mbox{for the BF, RM, and PL} \\ \left(\frac{1}{J}\sum_{j=1}^J \tilde{w}^{(j)}_{t-1}\right)\left(\sum_{j=1}^J\tilde{g}^{(j)}_t\right), & \mbox{for the APF and KDPF} \end{array} \right.\label{eqn:onestep:pf}
\end{equation}
Given approximations to $p(y_k|y_{1:k-1})$ for $k = 1,\ldots,t$, the marginal likelihood can be approximated via equation \eqref{eqn:marglik}.

Having prescribed a method for approximating $p(y_{1:t})$ sequentially using particle filtering, we can compare a set of possible models ${M_1,M_2,\ldots,M_N}$ according to their posterior model probabiities, given by
\begin{equation}
p(M_i|y_{1:t}) = \frac{p(y_{1:t}|M_i)p(M_i)}{\sum_{i=1}^N p(y_{1:t}|M_i)p(M_i)} \label{eqn:modelcomp}
\end{equation}
In Chapter \ref{ch:comp}, we compare estimated marginal likelihoods using the KDPF, RM and PL with the true marginal likelihood under the local level DLM. In Chapter \ref{ch:fmri} we compare posterior model probabilities for models $M_{101}$, $M_{011}$, and $M_{001}$ using the PL.

\section{Particle MCMC \label{sec:pmcmc}}

At the beginning of this chapter, some of the advantages and disadvantages of both MCMC and SMC were mentioned. Much research has focused on combining aspects of both types of methods to create more efficient algorithms for sampling from high dimensional posterior distributions. The resample-move algorithm described in Section \ref{sec:rm} is one such example, where an MCMC algorithm is incorporated within the particle filter as a way to avoid degeneracy in fixed parameter values. Particle MCMC (PMCMC) \citep{Andr:Douc:Hol:pmcmc:2010} is another such example. This method incorporates a particle filter within an iteration of an MCMC algorithm in order to increase efficiency when ideal proposal distributions are intractable.

The MCMC algorithm proposed in Section \ref{sec:mcmc:epid} for analyzing data from the the epidemic model described in \ref{sec:epid} is one such case where efficiency can be gained by using PMCMC instead. Instead of using Gaussian random walk proposals for sampling each of $x_1,\ldots,x_T$ from their full conditional distributions, as is done in Step \ref{step:gibbs} of the Gibbs sampler in Section \ref{sec:mcmc:epid}, we could instead propose a sample path $x_{0:T}^*$ from $p(x_{0:T}|\theta,y_{1:T})$ using a particle filter. The {\tt pmcmc} function within R package {\tt pomp} \citep{pomp} implements this kind of algorithm to generate samples for the fixed parameters that are asymptotically (as $J \rightarrow \infty$) distributed according to $p(\theta|y_{1:T})$ \citep{Andr:Rob:2009:pseudomarg}. The general algorithm, called the particle marginal Metropolis-Hastings sampler (PMMH) \cite[Section 2.4.2][]{Andr:Douc:Hol:pmcmc:2010}, proceeds as follows:
\begin{enumerate}
\item Initialization:
\begin{enumerate}
\item set initial value $\theta^{(0)}$,
\item run an SMC algorithm to generate an approximation to $p(x_{0:T}|y_{1:T},\theta^{(0)})$ via
    \[\hat{p}(x_{0:T}|y_{1:T},\theta^{(0)}) = \sum_{j=1}^J w_T^{(j)} \delta_{\left(x_{0:T}^{(j)}\right)},\]
\item sample $x_{0:T}^* \sim \hat{p}(x_{0:T}|y_{1:T},\theta^{(0)})$ and calculate an estimate of the marginal likelihood (conditional on $\theta^{(0)}$), denoted $\hat{p}(y_{1:T}|\theta^{(0)})$, via equations \eqref{eqn:onestep:pf} and \eqref{eqn:marglik}, and
\item set $i = 1$.
\end{enumerate}
\item For $i \ge 1$,
\begin{enumerate}
\item sample $\theta^*$ from some proposal distribution $q(\theta|\theta^{(i-1)})$,
\item run an SMC algorithm to generate an approximation to $p(x_{0:T}|y_{1:T},\theta^*)$ via
    \[\hat{p}(x_{0:T}|y_{1:T},\theta^*) = \sum_{j=1}^J w_T^{(j)} \delta_{\left(x_{0:T}^{(j)}\right)},\]
\item sample $x_{0:T}^* \sim \hat{p}(x_{0:T}|y_{1:T},\theta^*)$ and calculate an estimate of the marginal likelihood (conditional on $\theta^*$), denoted $\hat{p}(y_{1:T}|\theta^*)$, via equations \eqref{eqn:onestep:pf} and \eqref{eqn:marglik},
\item with probability
    \[R = \min\left(1, \frac{\hat{p}(y_{1:T}|\theta^*)p(\theta^*)}{\hat{p}(y_{1:T}|\theta^{(i-1)})p(\theta^{(i-1)})} \frac{q(\theta^{(i-1)}|\theta^*)}{q(\theta^*|\theta^{(i-1)})} \right),\]
    set
    \[\theta^{(i)} = \theta^* \qquad \hat{p}(y_{1:T}|\theta^{(i)}) = \hat{p}(y_{1:T}|\theta^*).\]
    Otherwise, set
    \[\theta^{(i)} = \theta^{(i-1)} \qquad \hat{p}(y_{1:T}|\theta^{(i)}) = \hat{p}(y_{1:T}|\theta^{(i-1)}).\]
\end{enumerate}
\end{enumerate}

Extensions to this algorithm to provide samples for the unobserved states approximately distributed according to $p(x_{0:T}|y_{1:T})$ or joint samples for states and fixed parameters approximately distributed according to $p(x_{0:T},\theta|y_{1:T})$ have yet to be implemented in {\tt pomp} \cite[see Section 2.4.3][]{Andr:Douc:Hol:pmcmc:2010}. We implement this algorithm on data simulated from the epidemic model described in Section \ref{sec:mcmc:epid} using {\tt pmcmc}, which samples fixed parameter values using Gaussian random walk proposals with prespecified standard deviations and uses a plain bootstrap filter to sample states and estimate the marginal likelihood. In Chapter \ref{ch:epid}, we compare the performance of the PMCMC algorithm with that of the KDPF and standard MCMC described in Section \ref{sec:mcmc:epid}. 