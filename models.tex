\chapter{Models \label{ch:models}}

\section{State-space models \label{sec:ss}}

State-space models are a general class of statistical models used for analysis of dynamic data.  They are constructed using an observation equation, $y_t \sim p_{y,t}(y_t|x_t,\theta)$, and a state evolution equation, $x_t \sim p_{x,t}(x_t|x_{t-1},\theta)$, where $y_t$ is the observed response, $x_t$ is a latent, dynamic state, the subscript $t$ is a time index, and $\theta$ is an unknown fixed parameter, all of which could be vectors. The $y_t$'s are assumed independent given $x_t$ and $\theta$, and the $x_t$'s are assumed independent given $x_{t-1}$ and $\theta$. The distributions $p_{y,t}$ and $p_{x,t}$ are assumed known conditional on the values of $\theta$ and $x_t$ in the observation equation and $\theta$ and $x_{t-1}$ in the evolution equation. Depending on whether the observations and the states are continuous or discrete, the distributions themselves may be continuous or discrete. The distributions are typically assumed to only vary with $x_t$ and $\theta$, and therefore the $t$ subscript is dropped. For simplicity, we also drop the $x$ and $y$ subscript and instead let the arguments make clear which distribution we are referring to. Thus, the general state-space model is
\begin{align}
y_t &\sim p(y_t|x_t,\theta) \label{eqn:obs} \\
x_t &\sim p(x_t|x_{t-1},\theta). \label{eqn:state}
\end{align}
A fully specified Bayesian model is obtained by also specifying the prior $p(x_0,\theta)$.

Equations \eqref{eqn:obs} and \eqref{eqn:state} describe a very general class of models, including non-Markovian structures and models where the dimension of $x_t$ does not remain constant with respect to $t$. For instance, we could describe a process where $x_t$ depends on the entire history of states up to $t$ by letting $x_{t-1} = (x^*_1, x^*_2, \ldots, x^*_{t-1})'$ and defining $x_t = (x_{t-1}, x^*_t)'$, where $x^*_t$ is the new state generated at time $t$. In addition, the form of equations \eqref{eqn:obs} and \eqref{eqn:state} could be linear or nonlinear with respect to $x_t$ or $\theta$. For example, in Chapter \ref{ch:epid}, we describe a state-space model of a disease epidemic that is linear in the observation equation with respect to $x_t$ and $\theta$ but nonlinear in the state equation with respect to $\theta$.

Special cases of state-space models include hidden Markov models \citep{cappe:2005:inference}, where the state $x_t$ has discrete support, and dynamic linear models (DLMs) \citep{West:Harr:baye:1997, petris:camp:2009:dynamic}, where each distribution is Gaussian whose mean is a linear function of the states and whose variance does not depend on the mean. A simple form of a DLM, known as a first-order DLM or local-level model, is used in Chapter \ref{ch:comp} to compare several particle filtering algorithms in terms of their ability to estimate the marginal likelihood (a.k.a. the prior predictive density, $p(y_{1:t})$). In Chapter \ref{ch:fmri}, DLM representations of regression models with autocorrelated errors are used to analyze fMRI data. DLMs are described in more detail in Section \ref{sec:dlm}.

\section{Dynamic linear models (DLMs) \label{sec:dlm}}

The general form of a DLM is represented as a state space model with observation and state equations given by
\begin{align}
y_t &= F_tx_t + v_t \label{eqn:dlm:obs} \\
x_t &= G_tx_{t-1} + w_t. \label{eqn:dlm:state}
\end{align}
Here, $y_t$ is a $q \times 1$ observation vector, $x_t$ is a $p \times 1$ state vector, and $v_t$ and $w_t$ are independent and identically distributed (iid) Gaussian random variables with mean 0 and covariance matrices $V_t$ ($q \times q$) and $W_t$ ($p \times p$), respectively. We also assume $v_t$ and $w_{t'}$ independent for all $t$ and $t'$. $F_t$ is a $q \times p$ matrix that defines the linear dependence between $y_t$ and $x_t$ in the observation equation; $G_t$ is a $p \times p$ matrix that does so for $x_t$ and $x_{t-1}$ in the state equation. Lastly, we specify the distribution of the prior state by $x_0 \sim \mbox{N}(m_0, C_0)$, where $m_0$ is a $p \times 1$ vector and $C_0$ is a $p \times p$ matrix. The matrices $V_t$, $W_t$, $F_t$, and $G_t$ are allowed to vary with time, and any or all of $V_t$, $W_t$, $F_t$, $G_t$, and $C_0$ could possibly contain unknown parameters.

All DLMs discussed in chapters \ref{ch:comp} and \ref{ch:fmri} assume univariate observations, i.e. $y_t$ is a scalar and $q = 1$. In addition, we assume $G_t$, $V_t$, and $W_t$ are time invariant, and so the subscript $t$ is omitted. Some DLMs we consider have time-invariant $F_t$, e.g. the local level DLM featured in Chapter \ref{ch:comp} and the dynamic intercept model discussed in Chapter \ref{ch:fmri}, while others such as the dynamic slope model featured in Chapter \ref{ch:fmri} incorporate time-varying $F_t$. Lastly, all DLMs we consider assume $F_t$ is known, while $G$, $V$, and $W$ may contain unknown parameters. We provide an overview of these DLMs in sections \ref{sec:dlm:ll} through \ref{sec:dlm:arwn}.

\subsection{First-order DLM with common variance factor \label{sec:dlm:ll}}

The first-order DLM or local level model for univariate $y_t$ and $x_t$ is specified by setting $F_t = G_t = 1$ for all $t$. Note that in this case $q = p = 1$, making both $V$ and $W$ $1 \times 1$ matrices. We also assume that the observation and state variance share an unknown factor, $\theta$, and that the \emph{signal-to-noise ratio}, defined as $\lambda = W / V$, is known. Thus, we have the following model
\begin{align}
y_t &\sim \mbox{N}(x_t, \theta) \label{eqn:ll:obs} \\
x_t &\sim \mbox{N}(x_{t-1}, \theta\lambda) \label{eqn:ll:state}
\end{align}
with prior distribution $p(x_0, \theta)$ specified by
\begin{equation}
x_0|\theta \sim \mbox{N}(0, \theta) \quad \theta \sim \mbox{IG}(\delta_0,\nu_0). \label{eqn:ll:prior}
\end{equation}
The hyperparameters $\delta_0$ and $\nu_0$ are assumed known.

\subsection{Regression with ARMA errors \label{sec:dlm:arma}}

A DLM is convenient for representing a linear regression model with autocorrelated errors. To do so, we introduce known covariates and unknown regression coefficients into the model, i.e.
\begin{align}
y_t &= U_t\beta + F_tx_t + v_t \label{eqn:dlm:reg:obs} \\
x_t &= Gx_{t-1} + w_t \label{eqn:dlm:reg:state}
\end{align}
where $U_t$ is a known $q$ by $d$ matrix and $\beta$ is an unknown $d$ by $1$ vector. A regression model could be specified without introducing $U_t$ and $\beta$ and instead incorporating $U_t$ inside of $F_t$ and $\beta$ as part of $x_t$. However, we write the model as in equations \eqref{eqn:dlm:reg:obs} and \eqref{eqn:dlm:reg:state} to make the separation of fixed regression parameters $\beta$ and the dynamic state $x_t$ explicit.

In Chapter \ref{ch:fmri}, we consider regression models for univariate $y_t$ with autoregressive-moving average (ARMA) error structure, i.e. models of the form given in equations \eqref{eqn:dlm:reg:obs} and \eqref{eqn:dlm:reg:state} with $q = 1$ and $x_t$ following a zero-mean $\mbox{ARMA}(P, Q)$ stochastic process \citep{shum:stof:2006:timeseries}, where $P$ and $Q$ are the orders of the autoregressive (AR) and moving average (MA) components, respectively. In these models, $x_t = (x_{t,1} ,x_{t,2}, \ldots, x_{t,m})'$ is an $m$-dimensional vector with $m = \mbox{max}(P,Q+1)$, $F_t$ is a time-invariant $m \times 1$ vector with first element equal to 1 and the rest 0, $V = 0$ (a $1 \times 1$ matrix), $G$ is an $m \times m$ matrix that takes the form
\begin{equation}
G = \left(
 \begin{array}{ccccc}
 \phi_1 & \vdots \\
 \phi_2 & \vdots \\
 \phi_3 & \vdots && I_{m-1} \\
 \vdots & \vdots \\
 \cdots & \cdots & \cdots & \cdots & \cdots \\
 \phi_m &\vdots & 0 & \cdots & 0
 \end{array}
\right), \label{eqn:dlm:ar:G}
\end{equation}
and $W = \sigma^2ee'$ with $e = (1, \gamma_1, \ldots, \gamma_m)'$. We let $\theta = (\beta', \phi', \gamma', \sigma^2)'$ represent the unknown parameters of the model, where $\phi = (\phi_1,\phi_2,\ldots,\phi_P)'$ and $\gamma = (\gamma_1,\gamma_2,\ldots,\gamma_Q)'$ are the coefficients of the AR and MA components, respectively, and $\sigma^2$ is the unknown variance of the ARMA innovations. We adopt the convention that $\phi_s = 0$ for $s > P$ and $\gamma_r = 0$ for $r > Q$. Multiplying out the state equation and successively back-substituting the components of $x_t$ \cite[see Sec 3.2.5,][]{petris:camp:2009:dynamic} yields the more familiar form of a regression model with ARMA errors, given by
\begin{equation}
y_t = U_t\beta + \phi_1x_{t-1,1} + \phi_2x_{t-2,1} + \cdots \phi_Px_{t-P,1} + \gamma_1\epsilon_{t-1} + \gamma_2\epsilon_{t-2} + \cdots \gamma_Q\epsilon_{t-Q}, \label{eqn:arma}
\end{equation}
where $\epsilon_t \stackrel{iid}{\sim} \mbox{N}(0,\sigma^2)$.

It is often desired that constraints be imposed on $\phi$ and $\gamma$ such that the ARMA process is stationary and invertible. Stationarity ensures that the long term behavior of $x_t$ is predictable and invertibility ensures that current and future states do not depend on the distant past \citep{shum:stof:2006:timeseries}. We require that roots of the AR polynomial
\[\phi(z) = 1 - \phi_1z - \phi_2z^2 - \cdots - \phi_Pz^P\]
lie outside of the unit circle in the complex plane to ensure stationarity. To ensure invertibility, we impose the same constraint on the MA polynomial, given by
\[\gamma(z) = 1 + \gamma_1z + \gamma_2z^2 + \cdots + \gamma_Pz^Q.\]
In Chapter \ref{ch:fmri}, we impose these constraints when fitting regression models with ARMA errors to fMRI data using maximum likelihood estimation.

In a Bayesian context, these models are completely specified by defining the prior distribution $p(x_0, \theta)$, where the initial state, $x_0$, consists of the presample errors. It is often desired that $x_0$ come from the stationary distribution of the ARMA process, given by
\begin{equation}
x_0|\theta \sim \mbox{N}(0, \sigma^2\Omega), \label{eqn:arma:prior}
\end{equation}
where $\mbox{vec}(\Omega) = (I_m - G\otimes G)^{-1} \mbox{vec}(ee')$ and $\phi$ is restricted to the region of stationarity. For a Bayesian treatment of unknown parameters in regression models with stationary and invertible ARMA errors, we refer the reader to \citet{chib:greenberg:1994:arma}.

\subsection{Dynamic regression \label{sec:dlm:arwn}}

The regression model with ARMA errors described in Section \ref{sec:dlm:arma} is represented as a DLM by setting the observation variance $V$ in equation \eqref{eqn:dlm:reg:obs} equal to 0 and completely specifying the error structure through the state equation. By instead letting $V > 0$, we can add an additional layer of variance to the model that represents observation or measurement noise. Furthermore, we can introduce additional structure into the errors through $F_t$. For example, consider a simple linear regression model with an intercept and a slope given by $\beta_0$ and $\beta_1$, respectively, and errors that follow an AR(1) plus white noise (AR(1)+WN) process, i.e.
\begin{align}
y_t &= \beta_0 + \beta_1u_t + x_t + v_t \label{eqn:arwn:obs} \\
x_t &= \phi x_{t-1} + w_t, \label{eqn:arwn:state}
\end{align}
where $u_t$ is a known explanatory variable, $v_t \stackrel{iid}{\sim} \mbox{N}(0, \sigma^2_m) \perp w_t \stackrel{iid}{\sim} \mbox{N}(0, \sigma^2_s)$, and $\theta = (\beta_0,\beta_1,\phi,\sigma^2_s,\sigma^2_m)'$ represents the unknown parameters in the model. Here, $\phi$ is the lag-1 coefficient of the AR process (note the subscript `1' is removed since there is only 1 AR coefficient), $\sigma^2_m$ is the observation variance, and $\sigma^2_s$ is the state variance a.k.a. the variance of the innovations of the AR(1) process.

Reexpressing equation \eqref{eqn:arwn:obs} as
\begin{equation}
y_t = (\beta_0 + x_t) + \beta_1u_t + v_t \label{eqn:arwn:dynint}
\end{equation}
shows that we can interpret this model as a \emph{dynamic intercept model}, i.e. a simple linear regression model with an intercept that changes over time according to an AR(1) process. We can represent a dynamic intercept model as a DLM defined by equations \eqref{eqn:dlm:reg:obs} and \eqref{eqn:dlm:reg:state} where $U_t = (1, u_t)$, $\beta = (\beta_0, \beta_1)'$, $F_t = 1$ for all $t$, $V = \sigma^2_m$, $G = \phi$, and $W = \sigma^2_s$. Letting instead $F_t = u_t$ yields a \emph{dynamic slope model}, or a simple linear regression model with a slope that changes over time. This can be seen by multiplying out equation \eqref{eqn:dlm:reg:obs}:
\begin{equation}
y_t = \beta_0 + \beta_1u_t + x_tu_t + v_t = \beta_0 + (\beta_1 + x_t)u_t + v_t. \label{eqn:arwn:dynslo}
\end{equation}

Finally, we consider a model with both a dynamic slope and a dynamic intercept by letting $x_t = (x_{t,1},x_{t,2})'$ be two-dimensional and adjusting $F_t$, $G$, and $W$ such that
\begin{equation}
F_t = (1, u_t) \quad G = \left(\begin{array}{cc} \phi & 0 \\ 0 & \rho \end{array}\right) \quad W = \left(\begin{array}{cc} \sigma^2_s & 0 \\ 0 & \sigma^2_b \end{array}\right). \label{eqn:arwn:dynboth:def}
\end{equation}
Multiplying out both equations \eqref{eqn:dlm:reg:obs} and \eqref{eqn:dlm:reg:state}, we have
\begin{align}
y_t &= (\beta_0 + x_{t,1}) + (\beta_1 + x_{t,2})u_t + v_t. \label{eqn:arwn:dynboth:obs} \\
x_{t,1} &= \phi x_{t-1,1} + w_{t,1} \label{eqn:arwn:dynboth:state1} \\
x_{t,2} &= \rho x_{t-1,2} + w_{t,2} \label{eqn:arwn:dynboth:state2}
\end{align}
where $w_{t,1} \stackrel{iid}{\sim} \mbox{N}(0,\sigma^2_s)$, $w_{t,2} \stackrel{iid}{\sim} \mbox{N}(0,\sigma^2_b)$, and $\theta = (\beta_0,\beta_1,\phi,\rho,\sigma^2_s,\sigma^2_b,\sigma^2_m)'$ are the unknown parameters. In this model, $\phi$ now represents the lag-1 autocorrelation for the change in the intercept, $\rho$ is the lag-1 autocorrelation for the change in the slope, $\sigma^2_s$ is the variance of the dynamic intercept, and $\sigma^2_b$ is the variance of the dynamic slope.

In Chapter \ref{ch:fmri}, we compare the dynamic slope, dynamic intercept, and simple linear regression models for fitting fMRI data. For ease of reference, we adopt notation to refer to models within a general class of dynamic regression models. Let $M_{ijk}$ represent a (possibly) dynamic regression model where $i$ is the order of the AR process for the dynamic intercept, $j$ is the order of the AR process for the dynamic slope, and $k$ is either 1 or 0 indicating whether or not the observation variance $\sigma^2_m$ is constrained to be 0. Letting $i$ or $j$ be equal to 0 removes the stochasticity in that component. Thus, $M_{101}$ corresponds to the dynamic intercept model described by equations \ref{eqn:arwn:obs} and \ref{eqn:arwn:state}, $M_{011}$ is the dynamic slope model described by equations \ref{eqn:arwn:dynslo} and \ref{eqn:arwn:state}, $M_{111}$ is the model with both a dynamic intercept and dynamic slope described by equations \ref{eqn:arwn:dynboth:obs}, \ref{eqn:arwn:dynboth:state1}, and \ref{eqn:arwn:dynboth:state2}, and $M_{001}$ describes a simple linear regression model with fixed coefficients and independent errors.

In all of these dynamic regression models, we allow for nonstationarity of the state process. This is intended to allow for modeling a wider range of behavior in fMRI data as well as to allow for estimation using the particle learning algorithm, which we describe in Chapter \ref{ch:meth}. For this reason, we constrain $x_0 = 0$ (or $x_0 = (0,0)'$ for $M_{111}$) so that the models are identifiable. This is equivalent to setting $m_0 = 0$ and $C_0 = 0$ (or $C_0$ a $2 \times 2$ zero matrix for $M_{111}$) in the prior distribution of $x_0$. In Chapter \ref{ch:fmri}, Bayesian models for $M_{101}$ and $M_{011}$ are analyzed using priors of the form $p(x_0, \theta) = p(\beta|\sigma^2_m)p(\sigma^2_m)p(\phi|\sigma^2_s)p(\sigma^2_s)$ with
\begin{align}
&\beta|\sigma^2_m \sim \mbox{N}(b_0, \sigma^2_mB_0) \qquad \sigma^2_m \sim \mbox{IG}(a_{m_0}, b_{m_0}) \label{eqn:dynreg:prior1} \\
&\phi|\sigma^2_s \sim \mbox{N}(\phi_0, \sigma^2_s\Phi_0) \qquad \sigma^2_s \sim \mbox{IG}(a_{s_0}, b_{s_0}) \label{eqn:dynreg:prior2}
\end{align}
and the hyperparameters $b_0$, $B_0$, $\phi_0$, $\Phi_0$, $a_{m_0}$, $b_{m_0}$, $a_{s_0}$, and $b_{s_0}$ assumed known.

\section{Sequential estimation \label{sec:sequential}}

When data are collected sequentially, it is often of interest to determine the \emph{filtered distribution}, i.e. the distribution of the current state and parameters conditional on the data observed up to that time. This distribution describes all of the available information up to time $t$ about the current state of the system and any fixed parameters. It can be updated recursively using Bayes' rule:
\begin{equation}
p(x_t,\theta| y_{1:t}) \propto p(y_t|x_t,\theta)p(x_t,\theta|y_{1:t-1}) \label{eqn:filtered}
\end{equation}
where $y_{1:t} = (y_1,\ldots,y_t)$.

Only in special cases can $p(x_t,\theta| y_{1:t})$ be evaluated analytically. The DLM described by equations \eqref{eqn:dlm:obs} and \eqref{eqn:dlm:state} is one such case provided all fixed parameters are known. In this case, $x_t|y_{1:t} \sim \mbox{N}(m_t,C_t)$, where $m_t$ and $C_t$ are calculated recursively using the Kalman filter \citep{kal:1960:ekf}. Starting with known $m_0$ and $C_0$, the filtering recursions are given by the following equations \cite[Sec 2.7.2][]{petris:camp:2009:dynamic}:
\begin{align}
a_t &= Gm_{t-1} &\qquad R_t &= GC_{t-1}G' + W \label{eqn:dlm:kal} \\
f_t &= F_ta_t &\qquad Q_t &= F_tR_tF_t' + V \nonumber \\
m_t &= a_t + R_tF_t'Q_t^{-1}(y_t-f_t) &\qquad C_t &= R_t - R_tF_t'Q_t^{-1}F_tR_t \nonumber
\end{align}

When unknown fixed parameters are present in DLMs, analytical tractibility exists in only a few cases such as the local level DLM described in Section \ref{sec:dlm:ll} with common observation and state variance factor \cite[Sec 4.3][]{petris:camp:2009:dynamic}. In this case, $p(x_t,\theta|y_{1:t})$ is given by
\begin{equation}
x_t|\theta,y_{1:t} \sim \mbox{N}(m_t,\theta c_t) \quad \theta|y_{1:t} \sim \mbox{IG}(\delta_t, \nu_t) \label{eqn:ll:post}
\end{equation}
where $m_t$, $c_t$, $\delta_t$, and $\nu_t$ are calculated recursively according to
\begin{align}
f_t &= m_{t-1} &\qquad q_t &= c_{t-1} + \lambda + 1 \label{eqn:ll:kal} \\
m_t &= (1 - c_t)f_t + c_ty_t &\qquad C_t &= 1 - \frac{1}{q_t} \nonumber \\
\delta_t &= \delta_{t-1} + \frac{1}{2} &\qquad \nu_t &= \nu_{t-1} + \frac{(y_t-f_t)^2}{2q_t}. \nonumber
\end{align}
starting with $m_0 = 0$, $c_0 = 1$, and known $\delta_0$ and $b_0$. These equations also provide the marginal posterior distribution of the state, $p(x_t|y_{1:t})$, and one-step ahead predictive density, $p(y_t|y_{1:t-1})$, given by
\begin{align}
x_t|y_{1:t} &\sim \mbox{T}(m_t,c_t \frac{\nu_t}{\delta_t},2\delta_t) \label{eqn:ll:marg} \\
y_t|y_{1:t-1} &\sim \mbox{T}(f_t,q_t \frac{\nu_{t-1}}{\delta_{t-1}},2\delta_{t-1}) \label{eqn:ll:onestep}
\end{align}
In Chapter \ref{ch:comp} we evaluate the abilities of several particle filters to approximate the marginal likelihood of data generated from this model by comparing with the true marginal likelihood that can be calculated analytically according to
\begin{equation}
p(y_{1:t}) = \prod_t p(y_t|y_{1:t-1}) \label{eqn:ll:marglik}
\end{equation}

The remaining DLMs described in sections \ref{sec:dlm:arma} and \ref{sec:dlm:arwn} do not emit analytically tractable forms of the posterior. When analytical tractability is not present, we turn to numerical methods including deterministic versions, e.g. the extended Kalman filter \cite[Sec 1.6][]{hay:2001:kal} and the Gaussian sum filter \citep{Alsp:Sore:nonl:1972}, or Monte Carlo versions such as particle filters. In Chapter \ref{ch:meth}, we describe a variety of particle filtering algorithms that can be used to estimate states and unknown fixed parameters in state-space models for which the filtered distribution is intractable. 