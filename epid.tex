\chapter{Simulation study: tracking a disease epidemic} \label{ch:epid}

In this chapter, we compare the performance of the BF, APF and KDPF using simulated data from the epidemic model described in Section \ref{sec:epid}. This data is analogous to that analyzed by \citet{skvortsov2012monitoring} using the BF. In addition, using the KDPF, we compare the performance of bounded versus unbounded priors on the fixed parameters as well as different resampling schemes. Lastly, we discuss the role of the discount factor $\Delta$ when implementing the KDPF and compare results from the KDPF with the MCMC and PMCMC algorithms described in Sections \ref{sec:mcmc:epid} and \ref{sec:pmcmc}.

This chapter proceeds as follows. In Section \ref{sec:epid:sim}, we describe how data from the epidemic model described in Section \ref{sec:epid} were simulated. In Section \ref{sec:epid:pf}, we describe how the BF, APF, and KDPF were implemented and both uniform and log-normal priors on the fixed parameters. In Section \ref{sec:epid:unif}, we compare the performance of the BF, APF, and KDPF using uniform priors and systematic resampling. In Section \ref{sec:epid:prior}, we compare the sensitivity of uniform versus log-normal priors on the fixed parameters to analysis using the KDPF. In Section \ref{sec:epid:resample}, we compare the multinomial, residual, stratified, and systematic resampling schemes using the KDPF with log-normal priors on the fixed parameters.

\section{Simulated epidemic data} \label{sec:epid:sim}

Forty epidemics were simulated according to equation \eqref{eqn:epid:state} for a population of size $P = 5000$ and $T = 125$ days. True values of $\beta$, $\gamma$, and $\nu$ were different for each simulated outbreak, determined by sampling from the log-normal prior distribution, $p(\theta)$, that we define in equation \eqref{eqn:epid:prior:ln} in Section \ref{sec:pf}. For all simulations, infection was introduced in 10 people in the population at day 0 (i.e. true $i_0 = 10/5000$ and $s_0 = 4990/5000$). Among the 40 simulations, the average time at which the epidemics peak is 57 days and the average proportion of the population that has been infected by $t = 125$ is 74\%. The left panel of Figure \ref{fig:epid:data} shows the evolution of $s_t$, $i_t$, and $r_t$ for a single simulation. The evolution of $i_t$ for the remaining 39 simulations are superimposed (light gray).

Data from randomly selected streams at each day were generated from equation \eqref{eqn:epid:obs}. The right panel of Figure \ref{fig:epid:data} displays the observed data from the simulated epidemic shown on the left. Values of known constants for $L = 4$ streams were kept the same for each simulation and are given in Table \ref{tab:epid:constants} ($\eta_l$ was set to 0 for all $l$). Values for $b_l$, $\varsigma_l$, and $\sigma_l$ were chosen to be the same as those used in the numerical study carried out by \citet{skvortsov2012monitoring}. The values chosen for $\varsigma_l$ were motivated by evidence based on real syndromic data that suggests values close to 1 \citep{chew2010twitter}.

\begin{figure}
\ssp
\centering
\caption{Simulated epidemic data} \label{fig:epid:data}
\begin{minipage}{0.48\linewidth}
\includegraphics[width=1.0\textwidth]{sim-orig-epid}
\end{minipage}
\begin{minipage}{0.48\linewidth}
\includegraphics[width=1.0\textwidth]{sim-orig-z-1}
\end{minipage}
\caption*{Simulated epidemic curves (left) and syndromic observations (right) for a single simulated epidemic (colored lines) with $\beta = 0.254$, $\gamma = 0.111$, and $\nu = 1.246$ along with infectious curves, $i_t$, for the remaining simulations (light gray).}
\end{figure}

\begin{table}
\ssp
\centering
\caption{Values of known constants in epidemic model} \label{tab:epid:constants}
\begin{tabular}{|cccc|}
\hline
$l$ & $b_l$ & $\varsigma_l$ & $\sigma_l$ \\
\hline
1 & 0.25 & 1.07 & 0.0012 \\
2 & 0.27 & 1.05 & 0.0008 \\
3 & 0.23 & 1.01 & 0.0010 \\
4 & 0.29 & 0.98 & 0.0011 \\
\hline
\end{tabular}
\end{table}

\section{Particle filter runs} \label{sec:epid:pf}

For each simulated data set, the BF, APF, and KDPF were run using $J = 100, 1000, 10000, \mbox{ and } 20000$ particles to obtain weighted sample approximations to $p(x_t,\theta|y_{1:t})$ for $t = 1,\ldots,T$. For each $J$, separate runs using multinomial, residual, stratified, and systematic resampling were implemented \citep{smcUtils} and an effective sample size threshold was set at 80\% of the total number of particles to determine when to resample \citep{Liu:Chen:Wong:reje:1998}. For the KDPF, sensitivity to changes in the discount factor, $\Delta$, was explored by running with $\Delta = 0.9, 0.95, 0.96, 0.97, 0.98, 0.99$.

To start each particle filter run, values for the initial state and fixed parameters for $J$ particles were sampled from the prior density $p(x_0,\theta) = p(\theta)p(i_0,s_0)$, where $p(i_0,s_0)$ is the joint pdf of the random variables $s_0$ and $i_0$ and $p(\theta)$ is the joint prior density of $\beta$, $\gamma$, and $\nu$. We let $i_0 \sim \mbox{N}_{[0,1]}(0.002,0.0005^2)$ and set $s_0 = 1 - i_0$, as in equation \eqref{eqn:epid:prior}. This is motivated by the fact that a very small percentage of the population is infected during the initial stage of an epidemic and no infected individuals have yet recovered from illness.

To investigate the impact of different prior distributions for $\theta$ on the performance of the particle filters, the runs described above were performed once using uniform priors on $\theta$ and then again using log-normal priors. We first ran the particle filters using uniform priors on $\theta$ that were chosen to be the same as those used in \citet{skvortsov2012monitoring}, i.e. $p(\theta) = p(\beta)p(\gamma)p(\nu)$ with
\begin{align}
\beta &\sim \mbox{Unif}(0.14, 0.50) \label{eqn:epid:prior:unif} \\
\gamma &\sim \mbox{Unif}(0.09, 0.143) \nonumber \\
\nu &\sim \mbox{Unif}(0.95, 1.3). \nonumber
\end{align}
These priors allow for values of $R_0$ in a range of approximately 1 to 5.5 and an average infectious period in a range of roughly 7 to 11 days. $R_0$ values for strains of influenza have been estimated to be around 2-3 \citep{mills2004influenza, heff2005repratio, zhang2011flu}. Thus, while these priors impose restrictive bounds on the parameters, they are not particularly informative for tracking a flu epidemic.

We then ran the particle filters using our own log-normal priors on $\theta$ that we define by $p(\theta) = p(\beta, \gamma)p(\nu)$ (i.e. $\beta$ and $\gamma$ are not independent). When implementing the particle filtering algorithms, the prior draws for $\beta$ were determined by multiplying sampled values of $\gamma$ by the basic reproductive rate $R_0 = \beta / \gamma$. All parameters were sampled independently with priors
\begin{align}
R_0 &\sim \mbox{LN}(0.7520, 0.1768^2) \label{eqn:epid:prior:ln} \\
\gamma &\sim \mbox{LN}(-2.1764, 0.1183^2) \nonumber \\
\nu &\sim \mbox{LN}(0.1055, 0.0800^2). \nonumber
\end{align}
Here, we incorporate prior information on $R_0$ instead of $\beta$ directly, since prior knowledge of the basic reproductive number may be easier to obtain than for the contact rate itself. These log-normal priors constrain $\beta$, $\gamma$ and $\nu$ to be positive. The mean and variance of the prior distributions on $\log \gamma$ and $\log \nu$ were chosen such that random draws of $\gamma$ and $\nu$ would fall within the bounds of their respective uniform priors (in equation \eqref{eqn:epid:prior:unif}) with 95\% probability. The mean and variance of $\log R_0$ were chosen such that $R_0$ would fall between 1.5 and 3 with 95\% probability.

It is important to note that particle filters do not perform well when diffuse priors are placed on unknown states or fixed parameters. This is because priors that are too vague yield a small number of prior draws sampled in areas of high likelihood, resulting in degeneracy of the particle filter after only a few time points. We discuss this challenge in more detail in Section \ref{sec:discussion}.

Logit and log transformations were applied to the components of $\theta$ in the manner described at the end of Section \ref{sec:kd} so that the normal mixture kernel could be used in the KDPF while constraining $\beta$, $\gamma$, and $\nu$ to be within their respective prior domains (i.e. logit was used with uniform priors and log with log-normal priors).

\section{Comparison of particle filter algorithms under uniform priors} \label{sec:epid:unif}

First, we compare the performance of the particle filters using uniform priors on the elements of $\theta$ and systematic resampling, since these priors and resampling scheme were used in \citet{skvortsov2012monitoring}. For ease of comparison, the same prior draws were used in each particle filter for fixed $J$. Figure \ref{fig:epid:pfs} shows 95\% credible bounds of $p(\beta|y_{1:t})$, $p(\gamma|y_{1:t})$, and $p(\nu|y_{1:t})$ for $t = 1,2,\ldots,T$ and $J = 100, 1000, 10000, 20000$ using the simulated data displayed in Figure \ref{fig:epid:data}. Initially, the credible bounds for the BF and APF match those of the KDPF, but quickly degenerate toward a single value due to elimination of unique particles during resampling. Although the time of degeneracy increases as $J$ gets larger, the BF and APF bounds become misleading during the second half of the epidemic, even for $J = 20000$. The bounds for the KDPF, on the other hand, have dramatically reduced degeneracy since new values of $\theta$ are regenerated from the kernel density approximation.

The KDPF also has an advantage over the BF and APF in terms of computational efficiency. Notice that the bounds for the KDPF become wider as $J$ increases, but they do not change much between $J = 10000$ and $J = 20000$. This suggests that by 10000 particles, the weighted sample approximation of $p(x_t,\theta|y_{1:t})$ has converged to the true posterior over the entire epidemic period, unlike with the BF and APF. Even though the bounds for $\beta$ for the BF and APF seem to roughly match those of the KDPF for $J = 20000$ over the first half of the epidemic, the KDPF provides the same measure of uncertainty for $J = 10000$ and does not degenerate during the second half of the epidemic.

Estimation of $\nu$ is more challenging than for $\beta$ or $\gamma$ because of the nonlinear nature of the state equation with respect to this parameter. We notice from the plots for $\nu$ with $J \ge 10000$ that very little information is gained over the course of the epidemic about this parameter relative to its uniform prior. Furthermore, the 95\% credible intervals for $\nu$ expand between $t = 70$ and $t = 80$, while we typically expect to see the width of credible intervals for an unknown fixed parameter decrease monotonically as data is accumulated. A plausible explanation here is that $p(\nu|y_{1:t})$ has been squeezed against the lower bound of the prior for $\nu$. Rerunning the analysis (results not shown) using our log-normal priors relaxes the prior bounds on $\nu$ and shows a shift in the distribution toward higher values around $t = 70$ as opposed to the widening of the interval that we see in Figure \ref{fig:epid:pfs}.

Table \ref{tab:epid:pfs} shows that the behavior of the BF, APF, and KDPF illustrated in Figure \ref{fig:epid:pfs} is consistent across the 40 simulations. For instance, for $J = 20000$, the 95\% credible intervals at $t = 125$ for each of $\beta$, $\gamma$ and $\nu$ cover the truth for 39 out of 40 (97.5\%) simulations using the KDPF. The BF and APF runs using the same number of particles, on the other hand, yield 95\% credible intervals at $t = 125$ that cover the truth for no more than 13 out of 40 (32.5\%) simulations when considering $\beta$, $\gamma$, and $\nu$ marginally.

\begin{figure}
\ssp
\centering
\caption{Comparing credible intervals for the BF, APF, and KDPF} \label{fig:epid:pfs}
\includegraphics[width=0.95\textwidth]{PF-1-systematic-unif-logit-99-61-params}
\caption*{Sequential 95\% credible intervals for $\beta$ (left column), $\gamma$ (middle column), and $\nu$ (right column) for increasing number of particles (rows) for the BF (red), APF (blue), and KDPF (green), compared with the truth (black lines), when using systematic resampling and uniform priors. Data were generated from the simulated epidemic shown in Figure \ref{fig:epid:data}. For the KDPF, $\Delta$ was set to 0.99.}
\end{figure}

\begin{table}
\ssp
\centering
\caption{Comparing credible intervals for the BF, APF, and KDPF} \label{tab:epid:pfs}
\begin{tabular}{|c|ccc|ccc|ccc|}
\hline
  & \multicolumn{3}{|c|}{$\beta$} & \multicolumn{3}{|c|}{$\gamma$} & \multicolumn{3}{|c|}{$\nu$} \\
  \hline
  $J$ & BF & APF & KDPF & BF & APF & KDPF & BF & APF & KDPF \\
  \hline
  100 & 0.000 & 0.000 & 0.175 & 0.000 & 0.000 & 0.175 & 0.000 & 0.000 & 0.100 \\
  \hline
  1000 & 0.000 & 0.000 & 0.800 & 0.000 & 0.000 & 0.900 & 0.000 & 0.000 & 0.800 \\
  \hline
  10000 & 0.150 & 0.150 & 0.975 & 0.150 & 0.200 & 0.950 & 0.175 & 0.250 & 0.925 \\
  \hline
  20000 & 0.325 & 0.275 & 0.975 & 0.325 & 0.175 & 0.975 & 0.300 & 0.175 & 0.975 \\
\hline
\end{tabular}
\caption*{Proportion of simulated data sets (out of 40 total) for which 95\% credible intervals obtained from the marginal filtered distributions of the fixed parameters (columns) at the end of the epidemic (i.e. $t = 125$) cover the true value used for simulation for increasing number of particles (rows) using the BF, APF, and KDPF.}
\end{table}

\section{Illustration of the negative impact of priors with truncated support} \label{sec:epid:prior}

As mentioned in Section \ref{sec:kd}, implementing the KDPF using a normal kernel density approximation to $p(\theta|y_{1:t})$ to regenerate the fixed parameters requires applying some transformation to the components of $\theta$ so that their support is on the real line. The logit function is a convenient choice for mapping fixed parameters with bounded support to the real line; the log function is convenient for fixed parameters with positive support. Thus, we investigate the sensitivity of the results with $J = 10000$ to these two types of transformations using the KDPF.

Figure \ref{fig:epid:priors} compares scatterplots of $\beta$ versus $\gamma$ sampled jointly from the filtered distribution $p(\beta,\gamma|y_{1:t})$ at $t = 0, 20, 40, 60$. In the top row, the same truncated support prior as in \citet{skvortsov2012monitoring} is used, i.e. $\beta \sim \mbox{Unif}( 0.14, 0.50)$ and $\gamma \sim \mbox{Unif}(0.09, 0.143)$ independently. In order to ensure regeneration in the KDPF does not extend past these bounds, a logit transformation was applied in the manner described at the end of Section \ref{sec:kd} with $a = 0.14$ and $b = 0.50$ for $\beta$ and $a = 0.09$ and $b = 0.143$ for $\gamma$. The kernel density is then created on this transformed space. In the top row of Figure \ref{fig:epid:priors}, the samples concentrate on the boundaries of the uniform prior on $\gamma$, particularly for $t = 20$ and $t = 40$. This suggests that the truncated support prior bounds on $\gamma$ are too restrictive to account for the uncertainty in this recovery time.

To test this hypothesis, we reran the KDPF using exactly the same prior draws as those used in the first row of Figure \ref{fig:epid:priors}, but we apply a log transformation to $\theta$ (to ensure $\beta$, $\gamma$ and $\nu$ are positive) instead of the logit transformation. The results are shown in the second row of Figure \ref{fig:epid:priors}. Despite the particles starting within the uniform bounds at $t=0$, the samples stray outside the uniform bounds for $\gamma$, suggesting that the data are informing us that reasonable parameter values can be found outside the bounds that would have been imposed by the truncated support prior.

The bottom row of Figure \ref{fig:epid:priors} displays results from running the KDPF using prior samples taken from the log-normal priors on the elements of $\theta$ described be equation \eqref{eqn:epid:prior:ln}. As in the second row, a log transformations were applied to the fixed parameters so that the normal kernel density could be used for jittering particles. The log-normal priors on the fixed parameters are more informative than the uniform priors in the sense that a greater number of particles are concentrated near the true values of $\beta$ and $\gamma$ at $t = 0$. Yet, the distribution of particles at $t = 20$ and $t = 40$ appear more spread out in the bottom row than in the top row because the log-normal priors are less restrictive on the sample space of $\beta$ and $\gamma$ than are the truncated support uniform priors.

In the bottom row of Figure \ref{fig:epid:priors}, sampled particle values at $t = 60$ have moved inside the bounds that would have been imposed by the uniform prior and form an ellipse-shaped distribution similar to what is shown in the second row of Figure \ref{fig:epid:priors} at $t = 60$. This suggests that the tail of points concentrated along the upper uniform bound at $t = 60$ in the top row of Figure \ref{fig:epid:priors} is an artifact of the over-restrictive uniform prior and not influenced by the data. We suggest using log-normal priors on positive elements of $\theta$ as opposed to uniform priors which bound the range of possible parameter values. This allows us to use prior knowledge of the epidemic to encourage points to lie in a reasonable range while retaining flexibility in the event of model mis-specification either in the likelihood or the prior.

\begin{figure}
\ssp
\centering
\caption{Comparing priors in the KDPF} \label{fig:epid:priors}
\begin{minipage}{1.0\linewidth}
\includegraphics[width=1.0\textwidth]{PF-betaGammaScat-1-10000-KD-systematic-99-61}
\caption*{Scatterplots of $\beta$ (horizontal) versus $\gamma$ (vertical) with true values (red crosses) at $t = 0, 20, 40, 60$ days using the KDPF with $J = 10000$ particles, systematic resampling, and $\Delta = 0.99$. The logit transformation (top row) on $\theta$ shifted and scaled to $(0,1)$ and log transformation (second row and bottom row) were used before regenerating the fixed parameters. To aid comparison, the same uniform draws of $\theta$ were sampled at $t = 0$ in each of the first two rows. Log-normal prior draws were sampled in the bottom row. For demonstration, each panel shows 500 particles sampled from the weighted sample approximation to $p(x_t,\theta|y_{1:t})$. Axes are the same in each panel. Dashed horizontal and vertical lines indicate the bounds of the uniform priors on $\gamma$ and $\beta$, respectively. The upper bound on the uniform prior on $\beta$ is not shown because it lies outside the range of the horizontal axis.}
\end{minipage}
\end{figure}

\section{Comparison of resampling schemes} \label{sec:epid:resample}

As mentioned in Section \ref{sec:resample}, resampling is meant to rebalance the weights of the particles in order to avoid degeneracy, but this comes at the cost of increasing the Monte Carlo variability of the particle sample. Up to this point, we have used only systematic resampling, as in \citet{skvortsov2012monitoring}. Alternatively, we could have chosen multinomial, residual, or stratified resampling. \citet{Douc:Capp:Moul:comp:2005} explains each of these methods in detail and shows that 1) multinomial resampling introduces more Monte Carlo variability than do residual or stratified resampling, 2) residual and stratified resampling introduce the same amount of Monte Carlo variability, on average, and 3) systematic resampling can introduce more Monte Carlo variability than does multinomial resampling.

With this in mind, we turn to a comparison of different techniques for the resampling step using the KDPF with log-normal priors on the fixed parameters and $\Delta = 0.99$. To aid in comparison of the different resampling techniques, the same prior draws were used in all particle filter runs for fixed $J$. We would like to choose the resampling scheme for which the filtered distribution, $p(x_t,\theta|y_{1:t})$, approaches the true posterior the fastest as a function of the number of particles. If the filtered distributions have converged to the true posterior, then 95\% credible intervals should cover the true parameter value about 95\% of the time.

Using $J = 100$ particles (not shown), sequential 95\% credible intervals over the second half of the epidemic for each of $\beta$, $\gamma$, and $\nu$ cover the true parameter value for less than half of the 40 simulated data sets, indicating that more particles are needed to approximate the true posterior. Figure \ref{fig:epid:resamp} shows that coverage probabilities approach the nominal level for all four resampling techniques as $J$ increases. Multinomial resampling, however, appears to be outperformed by the other three resampling techniques, as coverage for all three model parameters using $J = 1000$ particles dips lower during the second half of the epidemic for multinomial resampling than it does for any of the other three methods. This is also true for $\beta$ with $J = 10000$ particles. By increasing the number of particles to $J = 20000$ (not shown), all four resampling methods yield coverage probabilities for each parameter that remain within the 95\% confidence bounds around the nominal coverage level throughout the epidemic.

Although residual, stratified, and systematic resampling perform about the same with this specific model, we prefer to use either residual or stratified resampling because of an example shown in \cite{Douc:Capp:Moul:comp:2005} where systematic resampling adds more Monte Carlo variability than any of the other three resampling schemes.

\begin{figure}
\ssp
\centering
\caption{Comparing resampling schemes in the KDPF} \label{fig:epid:resamp}
\includegraphics[width=1.0\textwidth]{PF-coverage-95-40-KD-orig-resamp-params}
\caption*{Proportion of the 40 simulated data sets for which 95\% credible intervals for $\beta$ (left), $\gamma$ (middle), and $\nu$ (right) cover the true value used for simulation for different $t$ (x-axis) and $J$ (rows) using the KDPF with log-normal priors on $\theta$ and $\Delta = 0.99$. Solid gray horizontal line denotes nominal coverage (95\%) and dashed lines give 95\% confidence bounds around the true coverage.}
\end{figure}

\section{Discount factor}

We recommend, based on the results from Sections \ref{sec:epid:unif}, \ref{sec:epid:prior}, and \ref{sec:epid:resample}, that the KDPF with residual or stratified resampling and prior distributions bounded only by the support of the parameters be used in preference to other choices mentioned. With this implementation of the particle filter, the practitioner is still left to choose a value for the discount factor, $\Delta$. As mentioned in Section \ref{sec:kd}, $\Delta$ is a tuning parameter that determines the smoothness of the kernel density approximation to $p(\theta|y_{1:t})$ when implementing the KDPF. Choosing $\Delta$ close to 0 results in a smoother approximation and more substantial jittering of particles while $\Delta$ close to 1 leads to a choppier approximation and more subtle jittering of particles.

To test the sensitivity of the KDPF to different values of $\Delta$, we ran the KDPF with log-normal priors and stratified resampling on each of the 40 simulated data sets using different values of $\Delta$ (0.9, 0.95, 0.96, 0.97, 0.98, and 0.99) and $J$ (100, 1000, 10000, and 20000). We then calculated 95\% credible intervals for each of the unknown parameters. The results (not shown) indicate that lower values of $\Delta$ lead to a higher proportion of 95\% credible intervals covering the truth, but that coverage probabilities for all values of $\Delta$ are close to the nominal level for all $t$ if $J \ge 10000$. We use $\Delta = 0.99$ because we seek an implementation of the KDPF that works well when enough particles are used to provide a good approximation to the true posterior. \citet{Liu:West:comb:2001} recommends choosing a value between 0.95 and 0.99.

\section{Comparison with MCMC} \label{sec:epid:pmcmc}

For a comparison with our KDPF results, an MCMC analysis was run using the particle MCMC (PMCMC) approach of \citet{Andr:Douc:Hol:pmcmc:2010}. Using the {\tt pmcmc} function within the R package {\tt pomp} \citep{pomp}, samples from the posterior distribution of the fixed parameters were generated conditional on the first $T$ observations from the simulated data set pictured in Figure \ref{fig:epid:data} for $T = 5, 10, 15, \ldots, 125$. For each $T$, three chains consisting of 30000 PMCMC iterations were generated and 95\% credible intervals were calculated for each chain. PMCMC chains were initialized at the true values of the fixed parameters used for simulating the data. Tuning parameters representing the standard deviations of the random-walk proposal distributions of $\beta$, $\gamma$, and $\nu$ were set to 0.005, 0.001, and 0.01, respectively. These values were chosen because they allowed the chains to mix well within reasonable computing time, but it is possible that different values could provide better mixing and hence improved estimates of the fixed parameters within the same computing time. For more information on PMCMC and using functions within the {\tt pomp} package, we refer the reader to \citet{Andr:Douc:Hol:pmcmc:2010} and \citet{pomp}.

Figure \ref{fig:epid:pmcmc} compares the performance of the KDPF with PMCMC in terms of marginal 95\% credible intervals for the fixed parameters. The intervals obtained from PMCMC samples for each chain at each $T$ are compared with those produced by 20 separate runs of the KDPF on the same data set. The KDPF was run using 20000 particles, log-normal priors on the components of $\theta$, stratified resampling, and $\Delta = 0.99$. Multiple runs of the KDPF and PMCMC on the same data allows us to assess the uncertainty in the 95\% credible intervals for the filtered distributions at each $T$. For instance, the high variance of the interval estimates for $\nu$ in the rightmost panel of the figure demonstrates the challenge in estimating this parameter. The performance of the KDPF compares well with PMCMC in this study, as the bounds of the 95\% credible intervals obtained from the PMCMC chains over the course of the epidemic appear to be more variable than the bounds obtained from the separate KDPF runs. Furthermore, a single PMCMC chain run on the full data set (i.e. for $T = 125$) took 8-9 hours to complete while the KDPF with $J = 20000$ particles provided results for all time points in about 15 minutes in our study. Section \ref{sec:discussion} provides further discussion of different scenarios where either PMCMC or the KDPF might be preferred.

\begin{figure}
\ssp
\centering
\caption{Comparing the KDPF versus PMCMC} \label{fig:epid:pmcmc}
\includegraphics[width=1.0\textwidth]{sir-pmcmc-kdpf-3-30000-20-20000}
\caption{Sequential 95\% credible intervals for $\beta$ (left), $\gamma$ (middle), and $\nu$ (right) obtained from 20 different runs of the KDPF (green lines) using $J = 20000$ particles, log-normal priors, stratified resampling, and $\Delta = 0.99$ compared with 95\% credible intervals obtained from 3 different PMCMC chains (black circles) run for 30000 iterations on data collected up until day $T$ for $T = 5,10,\ldots,125$. All KDPF and PMCMC runs used observations taken from the same simulated data set pictured in Figure \ref{fig:epid:data}.}
\end{figure}

\section{Additional Unknown Parameters} \label{sec:epid:extend}

Within SMC approaches, an advantage of using a more computationally efficient algorithm is to allow reduced model assumptions. We therefore turn our focus to extending the analysis using the KDPF to include $\{b_l,\varsigma_l,\sigma_l,\eta_l:l\in1,\ldots,L\}$ as unknown parameters, thereby increasing the number of unknown parameters beyond those considered by \citet{skvortsov2012monitoring} using the BF. For this section, we consider data coming from only one stream ($L = 1$) and let $\theta = (\beta, \gamma, \nu, b, \varsigma, \sigma, \eta)'$, dropping the subscript $l$. Keeping the same simulated evolution of the true epidemic as shown in the left panel of Figure \ref{fig:epid:data}, a single stream of syndromic observations, $y_t$ for $t = 1,\ldots,T$, was simulated from equation \eqref{eqn:epid:obs} with true values of $b$, $\varsigma$, $\sigma$, and $\eta$ set to $0.258$, $1.028$, $0.000737$ and $2.346$, respectively. Days at which data were observed from the single stream were randomly selected.

The KDPF with tuning parameter $\Delta$ set to 0.99 was run with $J = 60000$ particles and stratified resampling was used with an effective sample size threshold of $0.8J$. As before, fixed parameter values were regenerated only when resampling was performed. Initial particles for states and parameters were sampled from their prior with $p(x_0)p(\theta) = p(i_0, s_0)p(\beta, \gamma)p(\nu)p(b)p(\varsigma)p(\sigma)p(\eta)$. The prior for the state and log-normal priors for $R_0$, $\gamma$, and $\nu$ are the same as those defined in Section \ref{sec:epid:pf}. The priors for $b$, $\varsigma$, $\sigma$, and $\eta$ are
\begin{align}
b &\sim \mbox{LN}(-1.6090, 0.3536^2) \label{eqn:epid:ext:prior} \\
\varsigma &\sim \mbox{LN}(-0.0114, 0.0771^2) \nonumber \\
\sigma &\sim \mbox{LN}(-7.0516, 0.2803^2) \nonumber \\
\eta &\sim N(2.5, 1) \nonumber 
\end{align}
independently. The choice of prior mean and standard deviation on the log scale were made such that random draws of $b$, $\varsigma$, and $\sigma$ on the original scale would be within $(0.1, 0.4)$, $(0.85, 1.15)$, and $(0.0005, 0.0015)$, respectively, with 95\% probability. To assess the loss in precision of our estimates due to incorporating additional unknown parameters into our analysis, we compared with results from running the KDPF using 60000 particles with $b$, $\varsigma$, $\sigma$, and $\eta$ assumed known at their true values used for simulating the data (we refer to the run with $b$, $\varsigma$, $\sigma$, and $\eta$ assumed known as the initial analysis).

Figure \ref{fig:epid:ext} shows sequential 95\% credible intervals for both the extended (blue lines) and the initial (red lines) analyses. Most noticeable from Figure \ref{fig:epid:ext} is that the intervals for $\beta$, $\gamma$, $\nu$, $s_t$, and $i_t$ are wider for the extended analysis than they are for the initial. This is due to the added uncertainty in $b$, $\varsigma$, $\sigma$, and $\eta$ in the extended analysis. Nonetheless, we are still able to obtain credible intervals for the unknown parameters that cover the true values for this simulated data set, as well as intervals for the states that cover the true epidemic curves, using a higher number of particles ($J = 60000$) than was used in the initial KDPF analysis in prior sections.

In Figure \ref{fig:epid:ext}, the lines appear choppy or block-like. This results from data coming from only one stream, leading to more time points where no data are available and making the analysis more sensitive to abnormal data. Gaps in the data lead to a lack of resampling of particles and cause more drastic shifts in the filtered distribution once data arrive. For instance, we notice a spike in the $s_t$ curve right after $t = 40$ because of a gap in the data and a shift in the trajectory of data points near the epidemic peak.

Lastly, we comment on a widening of the credible intervals for $\nu$. This phenomenon suggests that the log-normal priors used on $\nu$ are too restrictive, and that our model provides even less insight about this parameter than our prior belief. Scarce knowledge about $\nu$ is gained over the course of the epidemic in the initial analysis due to the nonlinear nature of the evolution equation with respect to $\nu$, and we in fact lose information about $\nu$ in the extended analysis relative to our specified prior. While the extended analysis could be rerun with a different prior, we present this specific analysis to illustrate the sensitivity of the filtered distribution of $\nu$ to assumptions about other parameters. The improved efficiency of the KDPF provides insight into this sensitivity (within reasonable computing time) in the absence of assumptions that were made about fixed parameters in the observation equation in both our initial analysis and in the prior BF analysis by \citet{skvortsov2012monitoring}.

\begin{figure}
\ssp
\centering
\caption{Analyzing epidemic model with additional unknown parameters} \label{fig:epid:ext}
\includegraphics[width=1.0\textwidth]{PF-ext-1-60000-KD-stratified-orig-log-99-61}
\caption{Sequential 95\% credible intervals for the states and fixed parameters from the original (red) and extended (blue) analyses where the KDPF with $J = 60000$ particles was run with stratified resampling and $\Delta = 0.99$. Tick marks are shown along the bottom of the plot for $s_t$ at time points when data were observed (dark gray) and when particles were resampled (blue and red for the extended and original analyses, respectively).}
\end{figure}

\section{Discussion \label{sec:discussion}}

Presented in this chapter is a strategy for simultaneous estimation of the current outbreak state and fixed parameters related to disease transmission using syndromic data. We introduce a stochastic epidemiological compartment model of a disease outbreak for data from syndromic surveillance that could possibly be multivariate and have any pattern of missingness. We suggest the use of the kernel density particle filter \citep{Liu:West:comb:2001} using priors on fixed parameters that are bounded only by their support. We suggest the use of stratified or residual resampling when effective sample size has dropped markedly and that regeneration of fixed parameter values should only occur when resampling is performed. We showed how this approach is capable of estimating a model with additional unknown fixed parameters.

Advanced techniques exist that are better than the KDPF at fighting particle degeneracy, but require more practitioner input. For example, particle degeneracy could be combated within an SMC algorithm by incorporating a MCMC step to refresh fixed parameter values \citep{Gilk:Berz:foll:2001,Stor:part:2002}, e.g. the resample-move algorithm described in Section \ref{sec:rm}. However, this would require the practitioner to define an MCMC algorithm in addition to the SMC algorithm. In addition to this requirement, the algorithm would no longer be truly sequential as the computational effort increases with time. Alternatively, if the practitioner is willing to modify their model, they can take advantage of a sufficient statistic structure \citep{Fear:mark:2002}, Rao-Blackwellization \citep{Douc:Gods:Andr:on:2000}, or both \citep{Carv:Joha:Lope:Pols:part}, as in in the particle learning algorithm described in Section \ref{sec:pl}. Possible modifications to the model in Section \ref{sec:model} to allow alternative strategies include setting $\nu=1$, removing fixed parameters from $Q$, and eliminating the truncation in equation \eqref{eqn:state}.

The KDPF provides a sequential inferential strategy that is easy to implement, applies to a very broad class of models, and reduces particle degeneracy when applied to models with unknown fixed parameters. However, along with its methodological strengths, the algorithm has weaknesses that are reflective of all SMC methods in general. For instance, SMC methods do not perform well in high-dimensional parameter space. In addition, while particle filters perform well when run over fixed-length time intervals, they eventually degenerate if run over long periods of time due to the accumulation of approximation errors. Lastly, as mentioned in Section \ref{sec:epid:pf}, all SMC methods suffer from degeneracy if vague priors are used. A common solution to this problem is to first run an MCMC based on the first few data points to find a reasonable particle cloud from which to draw prior samples \cite[Chap 5,][]{petris:camp:2009:dynamic}.

In high-dimensional settings, PMCMC methods provide better estimates of unknown states and fixed parameters by using SMC methods to construct efficient proposal distributions for a joint sample of all dynamic states \citep{Andr:Douc:Hol:pmcmc:2010}. Since it is likely that more complicated models than what we presented in this paper may be required for monitoring disease outbreaks in real-life situations \citep{Sham:Kars:pnas:2012, Bhad:Ioni:mala:2011}, PMCMC may offer a better solution in certain situations such as when $x_t$ and $\theta$ are high-dimensional. However, PMCMC is a non-sequential method and only valuable for on-line analysis provided the computation time required is not too burdensome. A sequential analysis could be more valuable for processing data collected at shorter time intervals when an immediate decision regarding an intervention policy is needed \citep{Merl:John:Gram:Mang:stat:2008, ludkovski2010optimal, dukic2012tracking}. In addition, efficient comparison of competing models for an epidemic outbreak \citep{Bhad:Ioni:mala:2011} could be made more feasible by running an SMC algorithm that could assess the fit of the data to multiple models more quickly. SMC methods can also provide an approximation to the marginal likelihood of the data if formal model comparison or model averaging is desired \citep{douc:joh:tut:2009, zhou:joh:smcmodcomp:2013}. We believe both the KDPF and PMCMC are valuable tools available to the practitioner.

In this chapter, we outline a strategy for real time tracking of a disease epidemic using data from syndromic surveillance, but this strategy can be applied to many other fields requiring on-line data analysis. We present improved particle filtering methods in general within the framework of sequential estimation of states and unknown fixed parameters in state-space models to inspire future work in epidemiological modeling and other scientific areas as well.