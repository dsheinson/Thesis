\chapter{Statistical analysis of fMRI data \label{ch:fmri}}

In this chapter, we use models and tools described in Chapters \ref{ch:models} and \ref{ch:meth} to analyze time series data collected from a functional magnetic resonance imaging (fMRI) experiment. We describe the most common method of statistical analysis used in the field, i.e. the correlation-based general linear model (GLM) approach \citep{friston:frith:JCBFM:1991,friston:holmes:hbm:1995}, and discuss challenges associated with analyzing fMRI data using this method. We mainly focus on the problem of how to deal with temporally autocorrelated data. Autocorrelated time series invalidate results obtained using the standard GLM, which assumes independence of the error terms in the model. We explore possible variations of the GLM to account for this autocorrelation and show via simulation the negative consequences of using the standard GLM to analyze autocorrelated data.

We then fit fMRI data collected from a word recognition experiment to the dynamic regression models described in Section \ref{sec:dlm:arwn} using maximum likelihood estimation, and we propose a model comparison strategy using PL. Using simulated data, we explore possible identifiability issues associated with these models and use PL to examine conditions under which the models can be distinguished from one another. Finally, we analyze real fMRI data using PL and discuss the appropriateness of the dynamic slope model for this data and as a tool for future fMRI studies.

In Section \ref{sec:fmri:intro}, we provide an overview of fMRI and describe the experimental data set. In Section \ref{sec:fmri:cor}, we compare different models with autocorrelated error structures and explore their impact on fitted model residuals as well as false positive rates of concluding significant brain activation. In Section \ref{sec:fmri:id}, we investigate identifiability issues associated with the dynamic regression models described in Section \ref{sec:dlm:arwn}, and we fit real and simulated fMRI data to these models using maximum likelihood estimation. Finally, in Section \ref{sec:fmri:pl}, we use simulated data to examine the possibility of comparing these dynamic regression models using PL, and we discuss results from applying PL to real fMRI data.

\section{Overview of fMRI \label{sec:fmri:intro}}

Functional MRI provides an indirect measure of neural activation in the brain in near real time. Most fMRI experiments measure the \emph{blood oxygen level-dependent (BOLD) signal}, or ratio of oxygenated to deoxygenated hemoglobin in the blood. Evidence suggests that a type of neural activity called the local field potential is closely related to the BOLD signal recorded in an fMRI experiment \citep{logo:bold:2003,logo:bold:2001}. By providing a noninvasive way to study functional changes in the brain over time, fMRI has allowed researchers to study topics that had previously seemed impossible to give a detailed scientific investigation, such as the nature of consciousness \citep{lloyd:conscious:2002}, meditation \citep{cahn:med:2006}, and moral judgement \citep{greene:moral:2001}.

\subsection{The haemodynamic response \label{sec:fmri:hrf}}

The BOLD response to a neural impulse is characterized by an increase in the BOLD signal from a baseline level to its peak at around 6 seconds post-stimulus, followed by a gradual decay back to baseline over the next 20-25 seconds. This typical BOLD response to an impulse as a function of time is referred to as the \emph{haemodynamic response function (hrf)}, and knowledge about this function is crucial for effectively analyzing data from fMRI experiments. Although studies have shown that the hrf varies from person to person based on factors such as age \citep{rich:hrf:2003}, most analyses assume a known form of the hrf for all subjects. A commonly used hrf that is thought to represent an average BOLD response for a typical subject is defined by the SPM software package for analysis of fMRI data (http://www.fil.ion.ucl.ac.uk/spm/doc/). This hrf is known as the canonical hrf (see bottom panel of Figure \ref{fig:fmri:data}). Another commonly used hrf is the gamma function proposed by \citet{boyn:linear:1996}, given by
\begin{equation}
h(t) = \frac{(t/\tau)^{n-1}e^{-t/\tau}}{\tau(n-1)!}, \label{eqn:hrf}
\end{equation}
where $t$ is time in seconds and $\tau$ and $n$ are free parameters that determine the shape of the hrf.

\subsection{The scanning session \label{sec:fmri:scan}}

An fMRI scanning session consists of one or more runs in which a human subject is presented with a simple task designed to stimulate the brain while scans are taken every few seconds. Runs can typically last anywhere between 10 and 30 minutes, and the \emph{repetition time (TR)}, or length between individual scans, can be anywhere between 1 and 3 seconds. Each scan involves taking cross-sectional slices across the whole brain. Although TRs less than one second are possible on some machines, increasing the TR often comes at the cost sacrificing spatial resolution of the images resulting from each scan.

Each image consists of a three-dimensional array of volumetric pixels, or \emph{voxels}, and each voxel contains a value of the BOLD response for a small area of the brain. Voxel size and TR are important choices that must be made prior to running an experiment based on desired spatial and temporal resolution of the data. An average experiment might involve three 10-minute scanning runs with a TR of 2 seconds, and an average scan might consist of 36 slices, where each slice consists of a 64 by 64 array of 3 $\mbox{mm}^2$ voxels. In this average scenario, each image would be made up of 147,456 voxels, and data from the entire scanning session would contain 132,710,400 BOLD values. Combining this with the fact that many studies involve multi-subject experiments, the sheer size of fMRI data sets impose significant challenges for data analysis.

In addition to choosing scanning parameters such as voxel size and TR, designing an fMRI experiment also involves deciding on how the experimental stimulus is presented to the subject in the scanner. Three experimental designs used in fMRI are block designs, slow event-related designs, and rapid event-related designs. Block designs divide functional runs into blocks of continuous activity and continuous rest, usually lasting anywhere from 30 seconds to a couple of minutes. During the activation blocks, subjects are instructed to perform the same task continuously over the entire block. In event-related designs, the pattern of the stimulus \emph{onsets}, or TRs at which an experimental stimulus is presented to the subject, is chosen randomly, with the time between onsets usually somewhere between 2 and 16 seconds. Slow event-related designs include rest periods that last around 30 seconds, while rapid event-related designs use shorter rest periods.

The long rest periods included in block and slow event-related designs are meant to allow the BOLD response to decay back to baseline before the next stimulus presentation. This helps increase the power of statistical tests designed to identify neural activation or distinguish between event types. However, these designs result in longer experiments which are more expensive and incur a greater risk of having the subject's mind wander during rest periods and generate non-task related BOLD signal. Rapid event-related designs have become more popular with the development of statistical methods such as the GLM approach that make analysis of data collected from experiments with rapid event-related designs possible.

This section is intended to provide a quick overview of fMRI for the purpose of giving context to the analyses discussed in the rest of this chapter. For more information on fMRI and designing fMRI experiments, we refer the reader to \citet{ashby:fmri:2011,pold:fmri:2011}.

\subsection{The correlation-based GLM approach \label{sec:fmri:glm}}

The standard correlation-based GLM analysis of fMRI data models the observed BOLD response in a single voxel of the brain as a linear function of the expected BOLD response from a voxel responding to the experimental stimulus, i.e.
\begin{equation}
y_t = \beta_0 + \beta_1\mbox{conv}_t + v_t, \label{eqn:fmri:glm}
\end{equation}
where $y_t$ is the observed BOLD response at TR $t$, $\mbox{conv}_t$ is the expected BOLD response at TR $t$ in an active voxel, $\beta = (\beta_0,\beta_1)'$ are unknown fixed regression coefficients, and $v_t \stackrel{iid}{\sim} \mbox{N}(0,\sigma^2)$ are independent random errors. The expected BOLD response, $\mbox{conv}_t$, is calculated by convolving the hrf with an ``on-off'' boxcar function that is equal to 1 at TRs when the experimental stimulus is on and 0 when it is off. Specifically,
\begin{equation}
\mbox{conv}_t = \int_0^t N(\omega)h(t-\omega)d\omega, \label{eqn:fmri:conv}
\end{equation}
where $N(\omega)$ represents the value of the neural activation boxcar at time $\omega$. Note that $N(t)$, $h(t)$, and hence $\mbox{conv}_t$ can be defined in continuous time. However, since we observe the BOLD response at discrete time points determined by the TR, we define the time index $t$ in units of TRs. Expected responses to different event types can be included in this model as additional covariates by convolving the hrf with the boxcar function associated with each event.

Under the model given in equation \eqref{eqn:fmri:glm}, the hypothesis test
\begin{equation}
H_0: \beta_1 = 0 \quad H_A: \beta_1 > 0 \label{eqn:fmri:hyp}
\end{equation}
is usually of interest, where rejection of $H_0$ in favor of $H_A$ is interpreted as evidence of neural activation in that particular voxel. To test this hypothesis, ordinary least squares estimates of the unknown fixed parameters $\beta$ and $\sigma^2$ are computed, i.e.
\begin{equation}
\hat{\beta}  = (\hat{\beta}_0,\hat{\beta}_1)' = (X'X)^{-1}X'y \qquad \hat{\sigma}^2 = \frac{1}{T-2}\lVert y-X\hat{\beta} \rVert, \label{eqn:fmri:ols}
\end{equation}
where $T$ is the total number of TRs in the functional run, $X$ is the $T$ by $2$ design matrix with first column all 1's and second column equal to $\mbox{conv}_t$, $y = (y_1,\ldots,y_T)'$, and $\lVert\cdot\rVert$ is the Euclidean norm. The test statistic, $t^*$, and p-value, $p^*$, are then calculated by
\begin{equation}
t^* = \frac{\hat{\beta}_1}{\hat{\sigma}^2(X'X)_{(2,2)}^{-1}} \qquad p^* = \mbox{P}(t^* > 0|H_0), \label{eqn:fmri:ttest}
\end{equation}
where $(X'X)_{(2,2)}^{-1}$ is the element in the second row and second column of $(X'X)^{-1}$ and $P(A|H_0)$ is the probability of event $A$ assuming $H_0$ is true. Thus, $p^*$ is calculated under the assumption that $t^* \sim \mbox{T}(0,1,T-2)$ and $H_0$ is rejected if $p^*$ is less than some significance threshold $\alpha$.

The majority of fMRI studies perform this hypothesis test independently for every voxel, resulting in a statistical parametric map of brain activation. With this approach, an adjustment to the significance threshold $\alpha$ must be made to account for multiple hypothesis tests being performed simultaneously. For example, if a false positive rate of $\alpha = 0.05$ is desired, a corrected threshold $\alpha^*$ must be used for each independent test so that the probability of a false positive among all tests is 0.05. Because of the spatial relationship among voxels, these hypothesis tests are not actually independent of each other, and this complicates the problem of finding the necessary correction. Typically, an approach relying on the theory of Gaussian random fields is used \citep{wors:grf:1995,wors:mar:cerebral:1996,wors:evans:cerebral:1992,friston:frith:JCBFM:1991}.

The standard GLM approach to analyzing fMRI data, which models univariate time series separately for each voxel, is convenient because regression theory allows for simple forms of estimators and fast computation. However, aside from using a multiple comparisons correction, the spatial nature of fMRI data and the connection between voxel-wise time series is ignored using this approach. Hence, a second-stage connectivity analysis is required to gain any insight into neural networks. The development of multivariate methods that analyze activation and connectivity simultaneously has gained popularity over the last decade. These include independent components analysis \citep{beck:smith:ica:2004}, multi-voxel pattern recognition \citet{norman:mvpa:2006}, representation similarity analysis \citep{nili:rs:2014}, and Bayesian spatio-temporal modeling approaches such as those developed by \citet{wool:jenk:bayesSp:2004}, \citet{bowman:2008:SpMCMC}, \citet{quiros:diez:2010:BayesSpTemp}, and \citet{zhang:guin:2014:npBayesWave}, to name a few.

While the development of efficient numerical approximation algorithms have decreased the computational burden of analyzing fMRI data using Bayesian spatio-temporal models, they are still slower and more difficult to implement than the standard GLM approach. Thus, voxel-wise hypothesis tests are still the norm in fMRI data analysis, and we operate within the framework of the univariate GLM for the remainder of this chapter. For more information on fMRI and standard statistical techniques used in the field including the GLM, the multiple comparisons problem, and multivariate strategies, see \citet{ashby:fmri:2011, penny:spm:2011}.

\subsection{Word recognition task \label{sec:fmri:data}}

The data set that we analyze in Sections \ref{sec:fmri:dr} and \ref{sec:fmri:pl} come from an episodic word recognition experiment for one human subject. The task the subject worked on consisted of an encoding session that takes place outside of the scanner and a recognition session that takes place inside the scanner. During encoding, the subject was presented with a list of words one at a time and told to memorize the words so that if they saw the word again, they would recognize it. During the recognition session, the subject was presented with a another list of words, some of which they saw during encoding and some of which were new. The subject was asked to respond as to whether they thought each word was old or new based on their memory.

While in the scanner, the words were presented according to a rapid-event related design with random delays between onsets somewhere between 2 and 10 seconds. The expected BOLD response ($\mbox{conv}_t$) is then constructed by convolving the canonical hrf with a boxcar function that is equal to 1 during TRs when a word is presented to the subject and 0 otherwise. The middle panel of Figure \ref{fig:fmri:data} shows $\mbox{conv}_t$ for this experiment. Scans of the subject's brain were taken every 1.5 seconds for about 6 minutes ($T = 245$ TRs) with a voxel size of 3 $\mbox{mm}^3$. Although whole brain data were recorded, we look specifically at time series from 5 by 5 by 5 voxel cubes (125 voxels per cube) extracted from six different brain regions, namely the left frontal pole (FP), left intraparietal sulcus (IPS-left), right intraparietal sulcus (IPS-right), primary visual cortex (PV), left secondary visual cortex (SV-left), and right secondary visual cortex (SV-right).

An important step that is performed prior to analyzing fMRI data is preprocessing of the raw data that comes directly out of the scanner. For example, images need to be spatially realigned to reduce the effect of the subject's head movements while inside the scanner. In addition, a high-resolution structural image taken prior to the functional run can be used to discern the exact location of voxels that are difficult to locate in lower resolution functional images. This process is called \emph{coregistration} of the functional and structural data. The coregistered data then needs to be normalized to a standard brain atlas so that active voxels can be assigned to a neuroanatomic brain structure. For this data set, the functional time series were spatially realigned to the first image using a least squares approach with a 6-parameter rigid body affine transformation \citep{friston:ash:spreg:1995}. Realigned images were then ``unwarped'' to reduce the influence of residual movement-related variance on BOLD signal intensity \citep{andersson:deform:2001}. The functional data were coregistered to a high-resolution T1 anatomical image using mutual information maximization with a 6-parameter rigid body affine transform \citep{ashburner:neelin:reg:1997}. Then, the images were normalized to standard 3D brain atlas defined by the International Consortium for Brain Mapping using a combination of a 12-parameter linear affine transformation and 3 by 2 by 3 nonlinear three-dimensional discrete cosine transform, and a 7th degree B-spline was used as the interpolation method for creating normalized images \citep{ashburner:friston:spnorm:1999,mazz:toga:atlas:1995}.

Other common preprocessing steps include spatial smoothing and slice-timing correction. Spatial smoothing is intended to get rid of some of the noise in the data and allow for the use of Gaussian random field theory when applying a correction for multiple hypothesis tests. Slice-timing correction adjusts for the fact that brain slices taken during a particular TR don't occur at the same time. For the word recognition experiment, the data were spatial smoothed with an 8 mm full-width at half maximum isotropic Gaussian kernel. Slice-timing correction was not applied to the data and is not typically used when the TR is less than 2 seconds \citep{penny:spm:2011}.

The top panel of Figure \ref{fig:fmri:data} shows the preprocessed fMRI time series for a voxel in IPS-left. Notice that the pattern of the observed time series somewhat mirrors the active response displayed in the middle panel for TRs greater than 75, but not for TRs less than 75. This type of behavior motivates our thinking that a regression model with a changing slope, such as $M_{011}$, might be appropriate for modeling fMRI data.

\begin{figure}
\ssp
\centering
\caption{Single voxel time series from fMRI experiment} \label{fig:fmri:data}
\includegraphics[width=1.0\textwidth]{fmri-craig-data}
\caption*{Time series data (top), expected BOLD response (middle), and haemodynamic response function (bottom) versus TR for voxel 27 in the left intraparietal sulcus.}
\end{figure}

\section{Temporal autocorrelation \label{sec:fmri:cor}}

The standard GLM approach to identifying task-related activity in a single voxel of the brain relies on an assumption of independence of the error terms, $v_t$, in equation \eqref{eqn:fmri:glm}. This assumption is not reasonable for fMRI data since random departures between the observed and predicted BOLD responses are likely to be similar among voxels near to each other in time and space. One reason for this is that the BOLD response to neural activation is not uniform across time and space. Thus, if the BOLD response in a voxel at one particular TR is greater than average, it is likely to also be greater than average in nearby voxels and at subsequent TRs \cite[Chapter 1][]{ashby:fmri:2011}. Other factors that contribute to spatially and temporally autocorrelated errors are unaccounted for signals in the data such as non-task related cognitive activity on the part of the subject and small movements caused by heartbeat and respiration \citep{loc:jos:arma:1997}.

An early approach to handling temporally autocorrelated fMRI time series was to ``color'' the data using a low-pass temporal filter to reduce high-frequency noise and amplify the signal in the data \citep{friston:holmes:color:1995,wors:frist:color:1995}. An alternate approach used by \citet{bullmore:prewhiten:1996} involves a two-stage procedure where the data are \emph{prewhitened} by first estimating the autocorrelation in the errors using the residuals from a GLM fit and then transforming the data to remove the autocorrelation. The standard GLM analysis is then applied to the uncorrelated data. The prewhitening approach is an improvement over coloring the data because it yields minimum variance unbiased estimates of the regression coefficients, provided the autocorrelation is accurately estimated from the residuals. \citet{wool:rip:auto:2001} show using resting state data that prewhitening performs more efficiently than coloring and that the bias in the autocorrelation estimates is acceptably low if adequate spatial and temporal smoothing is carried out during preprocessing.

The prewhitening approach to handling autocorrelated errors is the standard technique used in the FSL software package (http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/). Other approaches attempt to account for temporal autocorrelations explicitly through modeling. For example, \citet{lund:mad:nvr:2006} measured effects thought to contribute to autocorrelated noise such as heartbeat, respiration, and magnetic field strength and included them as additional covariates in the GLM. SPM uses an approach developed by \citet{kiebel:holmes:spm:2007} that models the correlation in the errors by
\begin{equation}
v \sim \mbox{N}(0,\sigma^2 I_T + \lambda Q) \quad Q_{i,j} = \left\{\begin{array}{ll} 0, & \mbox{if } i=j \\ e^{-|i-j|}, & \mbox{if } i\ne j, \end{array} \right.
\end{equation}
where $T$ is the total number of TRs in the experiment, $v = (v_1,\ldots,v_T)'$, and $\lambda$ is another unknown fixed parameter. Restricted maximum likelihood estimation (REML) is carried out to estimate the unknown fixed parameters, and the hypothesis test in equation \eqref{eqn:fmri:hyp} is performed using the test statistic
\begin{equation}
t^* = \frac{\hat{\beta_1}}{\left((X'X)^{-1}X'(\hat{\sigma}^2 I_T + \hat{\lambda} Q)X(X'X)^{-1}\right)_{(2,2)}}, \label{eqn:fmri:hyp-reml}
\end{equation}
where $\hat{\beta_1}$, $\hat{\sigma}^2$, and $\hat{\lambda}$ are the REML estimates. Under the null hypothesis, $t^* \sim \mbox{T}(0,1,df)$, where the degrees of freedom $df$ is computed by the Satterthwaite approximation \citep{wors:frist:color:1995}. In Section \ref{sec:fmri:fpr}, we compare ordinary least squares (OLS) estimation, prewhitening, and REML in terms of false positive and true positive rates of significant brain activation using simulated data.

\subsection{Exploration of ARMA models \label{sec:fmri:arma}}

Numerous studies have used first and second order autoregressive models for the error term of the GLM \citep{bullmore:prewhiten:1996, loc:jos:arma:1997}. We now explore the class of regression models with ARMA errors discussed in Section \ref{sec:dlm:arma} to see if other ARMA models might be appropriate for the word recognition data set.

Recall from Section \ref{sec:dlm:arma} that the DLM formulation of a regression model with ARMA errors is given by equations \eqref{eqn:dlm:reg:obs} and \eqref{eqn:dlm:reg:state} with $x_t = (x_{t,1} ,x_{t,2}, \ldots, x_{t,m})'$ an $m$-dimensional vector, $m = \mbox{max}(P,Q+1)$, $F_t$ a time-invariant $m \times 1$ vector with first element equal to 1 and the rest 0, $V = 0$ (a $1 \times 1$ matrix), $G$ a $m \times m$ matrix that takes the form
\[
G = \left(
 \begin{array}{ccccc}
 \phi_1 & \vdots \\
 \phi_2 & \vdots \\
 \phi_3 & \vdots && I_{m-1} \\
 \vdots & \vdots \\
 \cdots & \cdots & \cdots & \cdots & \cdots \\
 \phi_m &\vdots & 0 & \cdots & 0
 \end{array}
\right),
\]
and $W = \sigma^2ee'$ with $e = (1, \gamma_1, \ldots, \gamma_m)'$. The unknown fixed parameters are given by $\theta = (\beta', \phi', \gamma', \sigma^2)'$, where $\beta = (\beta_0,\beta_1)'$, $\phi = (\phi_1,\phi_2,\ldots,\phi_P)'$ and $\gamma = (\gamma_1,\gamma_2,\ldots,\gamma_Q)'$. We let $U_t = (1,\mbox{conv}_t)$ and adopt the convention that $\phi_s = 0$ for $s > P$ and $\gamma_r = 0$ for $r > Q$.

Maximization of the likelihood is performed using the R function {\tt arima}, which calls on {\tt optim} to minimize the negative log likelihood, given by
\begin{equation}
-\log p(y_{1:T}|\epsilon_{1:T},\theta) = \frac{1}{2}\sum_{t=1}^T \log|R_t| + \frac{1}{2}\sum_{t=1}^T \epsilon_t' R_t^{-1} \epsilon_t. \label{eqn:fmri:loglik}
\end{equation}
Here, $\epsilon_t = y_t - U_t\beta - F_tz_t$ are the \emph{innovations} of the DLM representation of the regression model with ARMA errors, and $z_t$ is the one-step ahead prediction for the state at time $t$ with variance $R_t$. For fixed $\theta$, $z_t$ and $R_t$ are computed via the Kalman filter given by equation \eqref{eqn:dlm:kal} given the initial values $m_0$ and $C_0$. These initial values are chosen such that the stationarity constraint given by equation \eqref{eqn:arma:prior} is satisfied \citep{gardner:harvey:mlearma:1980}. Given an initial set of fixed parameter values, optimization is performed using an iterative algorithm that alternates between running the Kalman filter and minimizing equation \eqref{eqn:fmri:loglik} conditional on $\epsilon_{1:T}$ and $R_{1:T}$ \citep{durbin:koopman:timeseries:2012,shum:stof:2006:timeseries}. Fixed parameter values are constrained to their regions of stationarity given by equations \eqref{eqn:arpoly} and \eqref{eqn:mapoly} using the method of \citet{jones:arma:1980}.

Five randomly chosen voxels from each of the six brain regions were fit to regression models with ARMA errors for all combinations of $P$ and $Q$ up to order 10. We then evaluated each model fit using three criteria: Akaike's information criterion (AIC) \citep{sak:ish:kit:aic:1986}, AIC corrected for bias (AICC) \citep{sug:aicc:1978,hurv:tsai:aicc:1989}, and Bayes' information criterion (BIC) \citep{schwarz:bic:1978}. For our model with a single regression covariate, the formulas for these criteria are given by
\begin{align}
AIC &= -2\log p(y_{1:T}|\epsilon_{1:T},\hat{\theta}) + 2(P+Q+3) \label{eqn:aic} \\
AICC &= -2\log p(y_{1:T}|\epsilon_{1:T},\hat{\theta}) + 2T\frac{P+Q+2}{T-P-Q-3} \label{eqn:aicc} \\
BIC &= -2\log p(y_{1:T}|\epsilon_{1:T},\hat{\theta}) + (\log T)(P+Q+3), \label{eqn:bic}
\end{align}
where $\hat{\theta}$ is the MLE of the unknown fixed parameters and $\log p(y_{1:T}|\epsilon_{1:T},\hat{\theta})$ is the value of the log-likelihood at convergence of the optimization procedure.

We prefer to use models that minimize these criteria. The first term is the same for all criteria and should be smaller for better model fits. The second term is a penalty for the number of parameters in the model. BIC imposes the strongest penalty for having more parameters and is most likely out of the three to prefer simpler models. The values of $P$ and $Q$ that minimized each of these criteria for each randomly selected voxel were recorded, and the average $P$ and $Q$ for each brain region are shown in Table \ref{tab:fmri:arma}. From these averages, it appears that AIC and AICC prefer $P$ and $Q$ near 3 while BIC prefers $P = 1$ and an $Q = 0$ or 1. We prefer to use the simpler models chosen by BIC and will mainly focus on models that incorporate first-order autoregressive errors in the remainder of this chapter.

\begin{table}
\ssp
\centering
\caption{Mean AR and MA orders for experimental fMRI data} \label{tab:fmri:arma}
\begin{tabular}{|l|cc|cc|cc|}
\hline
Region & \multicolumn{6}{|c|}{Criterion} \\
\hline
 & \multicolumn{2}{|c|}{AIC} & \multicolumn{2}{|c|}{AICC} & \multicolumn{2}{|c|}{BIC} \\
\hline
 & $P$ & $Q$ & $P$ & $Q$ & $P$ & $Q$ \\
\hline
Left frontal pole          & 2.80 & 3.20 & 2.80 & 2.90 & 1.70 & 0.90 \\
Left intraparietal sulcus  & 3.75 & 3.50 & 3.50 & 3.25 & 1.81 & 0.06 \\
Right intraparietal sulcus & 3.20 & 2.80 & 3.20 & 2.80 & 0.80 & 0.80 \\
Primary visual             & 3.10 & 3.00 & 3.10 & 2.70 & 0.90 & 1.70 \\
Secondary visual left      & 3.20 & 2.90 & 2.10 & 2.40 & 1.40 & 0.00 \\
Secondary visual right     & 3.20 & 3.00 & 3.10 & 2.50 & 0.70 & 0.70 \\
\hline
Mean across regions     & 3.21 & 3.07 & 2.97 & 2.76 & 1.22 & 0.69 \\
\hline
\end{tabular}
\caption*{Mean AR and MA orders ($P$ and $Q$, respectively) chosen according to AIC, AICC, and BIC for fits of regression models with ARMA errors to voxel-wise time series from 5 by 5 by 5 voxel cubes taken from 6 different brain regions.}
\end{table}

\subsection{False positive and true positive rates \label{sec:fmri:fpr}}

In this section, we consider the impact of using different approaches to testing for brain activation in voxel-wise time series when the data are temporally autocorrelated. Specifically, we examine false positive and true positive rates. The false positive rate is the rate of concluding significant neural activation in a voxel when there is no activity present. The true positive rate (or power) is the rate of concluding significant neural activation when there is in fact activity present. We would like to use a method that can increase the true positive rate while keeping the false positive rate low. We examine the effect on the false positive rate of using standard OLS estimation of the regression slope and compare different methods for accounting for temporal autocorrelation in terms of their effect on false postive and true positive rates.

In order to be able to analyze false positive and true positive rates, we simulated fMRI data using the regression model described in Section \ref{sec:fmri:arma} with AR(1) error structure (i.e. $P = 1$ and $Q = 0$). An experiment with a rapid event-related design with a single trial type was created by simulating random times between onsets according to a truncated geometric distribution with a maximum time-to-event of 10 TRs. We used a TR of 2 seconds and let the experiment run for 250 total TRs. Onset of the stimuli were assumed to last the length of the TR, and an on-off boxcar function of 1's and 0's was constructed to match the stimulus pattern. The independent variable in the regression, ($\mbox{conv}_t$), was constructed by convolving the boxcar function with the gamma hrf from equation \eqref{eqn:hrf} with $\tau = 2$ and $n = 4$. Figure \ref{fig:fmri:design} displays the simulated experimental design and expected BOLD response for active voxels.

Time series of length $T = 250$ were then simulated according to a regression model with AR(1) errors. For these simulations, we let $\beta_0 = 750$ and $\sigma^2 = 15$, and one thousand time series were generated for each of $\beta_1 \in \{0,1,2,3\}$ and $\phi \in \{0.25,0.50,0.75,0.95\}$. Different values of $\beta_1$ were used so that we could analyze false positive rates (for $\beta_1 = 0$) and power (for $\beta_1 > 0$). Similarly, different values of $\phi$ were used for simulation so that we could analyze false positive rates and true positive rates for increasing amounts of autocorrelation in the data.

\begin{figure}
\ssp
\centering
\caption{Simulated rapid-event related design of fMRI experiment} \label{fig:fmri:design}
\includegraphics[width=1.0\textwidth]{fmri-design-3-500-1}
\caption*{Simulated boxcar function (top), hrf (middle), and convolution of the boxcar with the hrf (bottom) for a rapid-event related design of an fMRI experiment.}
\end{figure}

For each simulated time series, we tested the hypothesis in equation \eqref{eqn:fmri:hyp} using the OLS method, i.e. where the test statistic and p-value are calculated according to equations \eqref{eqn:fmri:ols} and \eqref{eqn:fmri:ttest}. We also performed hypothesis tests using prewhitening (PW) and two variations of a REML approach. Our PW and REML approaches both assume an AR(1) correlation structure for the error terms. For PW, each time series was fit using OLS, and the residuals from this fit were refit to an AR(1) process using {\tt arima}. The maximum likelihood estimate of $\phi$, given by $\hat{\phi}$, was then used to obtain an estimate of the correlation matrix $\Lambda$, computed according to
\begin{equation}
\hat{\Lambda}_{(i,j)} = \hat{\phi}^{|i-j|}. \label{eqn:pw:cor}
\end{equation}
The data were then transformed according to
\begin{equation}
y^* = \hat{S}^{-1}y \qquad X^* = \hat{S}^{-1}X, \label{eqn:pw:trans}
\end{equation}
where $y = (y_1,y_2,\ldots,y_T)'$ is the vector of simulated fMRI data and $\hat{S}$ is the Cholesky decomposition of $\hat{\Lambda}$ (i.e. $\hat{S}\hat{S}' = \hat{\Lambda}$). Lastly, the hypothesis test is performed using the OLS method with $y^*$ and $X^*$ used in place of $y$ and $X$ in equations \eqref{eqn:fmri:ols} and \eqref{eqn:fmri:ttest}.

For hypothesis tests using REML estimation, we first obtain an estimate of $\phi$ by maximizing the profiled log-restricted likelihood, derived by \citet{harville:reml:1977} and given by
\begin{align}
\log p(y|\phi) \propto & -(T-2)\log\left|\left| y^* - X^*\left[(X^*)'X^*\right]^{-1}(X^*)'y^* \right|\right| \label{eqn:fmri:plrl} \\
 & - \frac{1}{2}\log\left|(X^*)'X^*\right| - \frac{1}{2} \log \Lambda. \nonumber
\end{align}
Note that $y^*$ and $X^*$ are both functions of $\phi$. Estimates of $\beta$ and $\sigma^2$ are then computed by
\begin{equation}
\hat{\beta} = \left[\left(X^*(\hat{\phi})\right)'X^*(\hat{\phi})\right]^{-1}\left(X^*(\hat{\phi})\right)'y^*(\hat{\phi}) \qquad \hat{\sigma}^2 = \frac{1}{T-2}\left|\left| y^*(\hat{\phi})-X^*(\hat{\phi})\hat{\beta} \right|\right|, \label{eqn:fmri:reml}
\end{equation}
where $\hat{\phi}$ maximizes $\log p(y|\phi)$. $X^*(\hat{\phi})$ and $y^*(\hat{\phi})$ are calculated using equations \eqref{eqn:pw:cor} and \eqref{eqn:pw:trans}. The t-test in equation \eqref{eqn:fmri:ttest} is then carried out using these estimates and $X^*(\hat{\phi})$ in place of $X$.

We used the {\tt gls} function in R package {\tt nlme} to find $\hat{\phi}$ that maximizes the profiled log-restricted likelihood \citep{pin:bates:mixed:2000}. It is standard to then perform the t-test conditional on REML estimate of $\phi$. However, because the correlation structure is estimated and not known, the null distribution of the t-statistic does not exactly follow $\mbox{T}(0,1,T-2)$. Alternatively, we can adjust the degrees of freedom of the t-distribution to account for the estimated autocorrelation in the data. We consider a strategy that corrects the degrees of freedom using an effective sample size adjustment for time series with first-order autocorrelation \citep{dawdy:matalas:ess:1964}. Specifically, we adjust the degrees of freedom in the t-test to $T' - 2$, where
\begin{equation}
T' = T\frac{1-\hat{\phi}}{1+\hat{\phi}}. \label{eqn:fmri:ess}
\end{equation}
We will refer to the method that incorporates REML estimates of the unknown fixed parameters with an effective sample size adjustment in the degrees of freedom of the t-test as REMLc (for corrected REML).

Table \ref{tab:fmri:fpr} displays false positive rates of rejecting $H_0$ using significance thresholds $\alpha = 0.001, 0.01, \mbox{ and } 0.05$ for the 1000 simulations using $\beta_1 = 0$ and increasing values of $\phi$. Methods of estimation that accurately assess the uncertainty in $\hat{\beta_1}$ should yield false positive rates equal to $\alpha$. We can see from the table that using OLS will inflate the false positive rate, and furthermore that the factor by which the false positive rate is inflated relative to $\alpha$ increases as $\alpha$ decreases. This is an undesirable feature, since lower $\alpha$ levels should result in more conservative tests.

From the plots in Figure \ref{fig:fmri:fpr}, we see that 95\% confidence intervals around the false positive rate for PW and REML contain the nominal threshold $\alpha$ for all values of $\phi$. REMLc appears to give slightly lower false positive rates than REML and PW for $\phi \le 0.75$ and decidedly lower false positive rates for $\phi = 0.95$. While this may seem to be an advantage of REMLc, a decrease in the false positive rate can come at the cost of a decrease in the true positive rate as well. Figure \ref{fig:fmri:roc} illustrates this point using ROC curves. The ROC curves in this figure display the true positive rate versus the false positive rate for the hypothesis tests performed according to each method. The methods with the largest area under the ROC curve performed the best in terms of distinguishing between the null and alternative hypothesis. We see from the curve corresponding to $\phi = 0.95$ that REMLc is outperformed by PW and REML, suggesting that although REMLc offers a decrease in the false positive rate for highly autocorrelated data relative to the other methods, it does not identify as many active voxels.

\begin{table}
\ssp
\centering
\caption{False positive rates for simulated fMRI data} \label{tab:fmri:fpr}
\begin{tabular}{|c|cccc|}
\hline
$\alpha$ & OLS & PW & REML & REMLc \\
\hline
 & \multicolumn{4}{|c|}{$\phi = 0.25$} \\
\hline
0.001 & 0.006 & 0.001 & 0.001 & 0.001 \\
0.010 & 0.028 & 0.011 & 0.010 & 0.010 \\
0.050 & 0.106 & 0.050 & 0.049 & 0.048 \\
\hline
 & \multicolumn{4}{|c|}{$\phi = 0.50$} \\
\hline
0.001 & 0.022 & 0.002 & 0.002 & 0.002 \\
0.010 & 0.070 & 0.010 & 0.009 & 0.007 \\
0.050 & 0.161 & 0.054 & 0.052 & 0.052 \\
\hline
 & \multicolumn{4}{|c|}{$\phi = 0.75$} \\
\hline
0.001 & 0.061 & 0.001 & 0.001 & 0.000 \\
0.010 & 0.130 & 0.017 & 0.016 & 0.015 \\
0.050 & 0.213 & 0.064 & 0.062 & 0.055 \\
\hline
 & \multicolumn{4}{|c|}{$\phi = 0.95$} \\
\hline
0.001 & 0.072 & 0.000 & 0.000 & 0.000 \\
0.010 & 0.144 & 0.011 & 0.011 & 0.000 \\
0.050 & 0.230 & 0.046 & 0.049 & 0.023 \\
\hline
\end{tabular}
\caption*{False positive rates at significance levels $\alpha = 0.001, 0.01, \mbox{ and } 0.05$ (rows) for testing $H_0: \beta_1 = 0$ vs $H_A: \beta_1 > 0$ using OLS, PW, REML, and REMLc (columns) on simulated data from $M_{100}$ with $\beta = (750, 3)$, $\sigma^2 = 15$, and increasing $\phi$ (embedded tables).}
\end{table}

\begin{figure}
\ssp
\centering
\caption{False positive rates for simulated fMRI data} \label{fig:fmri:fpr}
\includegraphics[width=1.0\textwidth]{simstudy-FPR}
\caption*{False positive rates (solid lines) and 95\% confidence intervals (dashed lines) for testing $H_0: \beta_1 = 0$ vs $H_A: \beta_1 > 0$ versus the nominal threshold level $\alpha$ (gray line) using OLS (black lines), PW (red lines), REML (green lines), and REMLc (blue lines) on simulated data from $M_{100}$ with $\beta = (750, 0)$, $\sigma^2 = 15$, and increasing $\phi$ (plot panels).}
\end{figure}

\begin{figure}
\ssp
\centering
\caption{ROC curves for simulated fMRI data} \label{fig:fmri:roc}
\includegraphics[width=1.0\textwidth]{simstudy-ROC-3}
\caption*{ROC curves for testing $H_0: \beta_1 = 0$ vs $H_A: \beta_1 > 0$ using OLS (black lines), PW (red lines), REML (green lines), and REMLc (blue lines) on simulated data from $M_{100}$ with $\beta = (750, 3)$, $\sigma^2 = 15$, and increasing $\phi$ (plot panels).}
\end{figure}

\subsection{Testing independence of residuals \label{sec:fmri:res}}

In the previous section, we used false positive rates to compare several methods that attempt to sufficiently capture the autocorrelation in fMRI time series. To do this, we simulated non-active voxels by letting the true $\beta_1 = 0$. In practice, however, we don't know beforehand which voxels are inactive, making false positive rates difficult to obtain. Many studies have attempted to analyze false positive rates using \emph{resting state} data, or fMRI data generated from a subject at rest \citep{purdon:weiss:fpr:1998,burock:dale:fpr:2000,wool:rip:auto:2001}. These studies have effectively provided insight into the impact of certain autocorrelation estimation algorithms, sampling rates, and experimental designs on false positive rates. However, it is difficult to assess whether voxels from a subject who is resting are really inactive. For example, the subject's mind could wander during the experiment and generate a BOLD signal. Furthermore, research suggests that some brain areas exhibit some intrinsic activation during resting state, such as those associated with the default mode network \citep{grei:kras:dfmnet:2003,grei:sup:dfmnet:2009}.

An alternate strategy that has been used for evaluating different models of autocorrelation in fMRI data is to examine the residuals from a model fit and test whether or not they are uncorrelated \citep{luo:nich:diag:2003,leonski:bax:res:2008}. In particular \citet{leonski:bax:res:2008} compares a number of autocorrelation estimation algorithms used in different software packages such as SPM and FSL. They suggest that an AR(2) process be used for modeling the errors in fMRI time series. This is based in part on the fact that residuals from AR(2) fits were determined to be uncorrelated more often than for other models according statistical tests such as the Durbin-Watson and cumulative periodogram tests.

While an AR(2) error structure may be appropriate for modeling some data sets, we contend that an analysis of residuals should not be the basis for this decision. This is because of the potential for overfitting. To illustrate, we fit the simulated data described in Section \ref{sec:fmri:fpr} with $\beta = (750,3)'$, $\sigma^2 = 15$, and $\phi \in \{0.25,0.50,0.75,0.95\}$ to regression models with independent, AR(1), and AR(2) errors. The regression models with independent errors were fit using OLS, and those with AR(1) and AR(2) errors were fit using the {\tt arima} function in R. For each model fit, we tested the independence of the residuals using the Ljung-Box test for lag-1 autocorrelations \citep{box:test:1978}. The results in Table \ref{tab:fmri:res} show that even though the true data-generating model has AR(1) errors, residuals from the AR(2) fit were determined to be uncorrelated as much if not more than those from the AR(1) fit. For this reason, we do not recommend evaluating models with autocorrelated errors based on an assessment of independence of residuals. Instead, in Section \ref{sec:fmri:pl}, we explore a model comparison strategy based on PL.

\begin{table}
\ssp
\centering
\caption{Proportion of whitened residuals for simulated fMRI data} \label{tab:fmri:res}
\begin{tabular}{|c|ccc|}
\hline
$\alpha$ & OLS & AR(1) & AR(2) \\
\hline
 & \multicolumn{3}{|c|}{$\phi = 0.25$} \\
\hline
0.001 & 0.332 & 1.000 & 1.000 \\
0.010 & 0.134 & 1.000 & 1.000 \\
0.050 & 0.028 & 1.000 & 1.000 \\
\hline
 & \multicolumn{3}{|c|}{$\phi = 0.50$} \\
\hline
0.001 & 0.000 & 1.000 & 1.000 \\
0.010 & 0.000 & 1.000 & 1.000 \\
0.050 & 0.000 & 0.999 & 1.000 \\
\hline
 & \multicolumn{3}{|c|}{$\phi = 0.75$} \\
\hline
0.001 & 0.000 & 1.000 & 1.000 \\
0.010 & 0.000 & 1.000 & 1.000 \\
0.050 & 0.000 & 0.994 & 1.000 \\
\hline
 & \multicolumn{3}{|c|}{$\phi = 0.95$} \\
\hline
0.001 & 0.000 & 1.000 & 1.000 \\
0.010 & 0.000 & 0.991 & 1.000 \\
0.050 & 0.000 & 0.958 & 1.000 \\
\hline
\end{tabular}
\caption*{Proportion of whitened residuals determined by Ljung-Box test at varying significance levels $\alpha$ (rows) from fitting data simulated from $M_{100}$ with $\beta = 3$, $\sigma^2 = 15$, and increasing $\phi$ (embedded tables) to regression models with independent (OLS), AR(1), and AR(2) error structures.}
\end{table}

\section{Fitting dynamic regression models \label{sec:fmri:dr}}

We now turn our attention to the dynamic regression models described in Section \ref{sec:dlm:arwn}. Specifically, we examine the dynamic intercept model ($M_{101}$), dynamic slope model ($M_{011}$), and a model with both a dynamic intercept and a dynamic slope ($M_{111}$). $M_{101}$ can be thought of as a regression model with AR(1)+WN errors, which is commonly used to analyze fMRI time series. Of particular interest to us is the possibility of modeling fMRI time series using $M_{011}$ or $M_{111}$ . While models similar to $M_{101}$ are thought to adequately capture autocorrelation in fMRI data, we explore the possibility that a model such as $M_{011}$ can improve on existing methods by incorporating a dynamic slope that can adapt to learning or changes in focus on the part of the subject.

\subsection{Identifiability of dynamic regression models \label{sec:fmri:id}}

Before applying the dynamic regression models to actual fMRI data, we examine whether the models themselves are identifiable. To do this, we simulated 1000 time series of length $T = 250$ from each of $M_{101}$, $M_{011}$, and $M_{111}$ using the expected BOLD response pictured in the middle panel of Figure \ref{fig:fmri:design} as the covariate $\mbox{conv}_t$. We let the true $\beta = (750, 15)'$ for all models and repeated the simulations for various values of $\phi$, $\sigma^2_s$, $\sigma^2_m$, $\rho$, and $\sigma^2_b$ (these last two parameters are only relevant for $M_{111}$). Specifically, for $M_{101}$ and $M_{011}$, we simulate for all combinations of $\phi \in \{0.1, 0.5, 0.9\}$, $\sigma^2_s \in \{1, 5, 10, 15, 20\}$, and $\sigma^2_m = 10$. For $M_{111}$, we simulate for all combinations of $\phi \in \{0.3, 0.6, 0.9\}$, $\sigma^2_s \in \{1, 5, 10, 15, 20\}$, $\sigma^2_b \in \{1, 5, 10, 15, 20\}$, $\rho \in \{0.3, 0.6, 0.9\}$, and $\sigma^2_m = 10$.

For each simulated time series, we calculate MLEs for the unknown fixed parameters using the {\tt dlmMLE} function the R package {\tt dlm} \citep{petris:camp:2009:dynamic}. This function uses a call to {\tt optim} to minimize the negative log likelihood, expressed as
\begin{equation}
-\log p(y_{1:T}|\theta) \propto \frac{1}{2}\sum_{t=1}^T \log |Q_t| + \frac{1}{2}\sum_{t=1}^T (y_t-f_t)'Q_t^{-1}(y_t-f_t), \label{sec:fmri:dlmll}
\end{equation}
where $f_t$ and $Q_t$ depend implicitly on $\theta$ and are calculated according to the Kalman filter given by equation \eqref{eqn:dlm:kal} with initial values $m_0 = C_0 = 0$. These initial values constrain the initial state, $x_0$, to be 0. As such, the state is interpreted to be the change in the slope or intercept over time.

Figure \ref{fig:fmri:id:M011SNR} displays histograms of the MLEs for fits of $M_{011}$ to the data simulated from $M_{011}$ with $\phi = 0.1$ and increasing signal-to-noise ratio $\sigma^2_s / \sigma^2_m$. $\beta$ appears to be identified for all values of the signal-to-noise ratio, as evidenced by two-dimensional histograms that show an ellipse with a clear mode near the true value. $\phi$, $\sigma^2_s$, and $\sigma^2_m$, however, don't appear to be identified as well for lower values of the signal-to-noise ratio and particularly for $\sigma^2_s / \sigma^2_m = 0.1$. Figure \ref{fig:fmri:id:M011PRR} shows similar plots with $\sigma^2_s / \sigma^2_m = 0.1$ and increasing $\phi$. While identification of $\phi$, $\sigma^2_s$, and $\sigma^2_m$ is poor for $\phi = 0.1$, a drastic improvement is shown by increasing $\phi$ to 0.5 and 0.9. $\beta$, again, is identified well for all fixed parameter values shown in the figure.

\begin{figure}
\ssp
\centering
\caption{Identifying dynamic slope model by increasing signal-to-noise ratio} \label{fig:fmri:id:M011SNR}
\includegraphics[width=0.75\textwidth]{1000-250-M011-conv-750-15-1-5-SNR-10-10}
\caption*{Histograms in 1D (for $\phi$, second column) and 2D (for $\beta$ and $(\sigma^2_s,\sigma^2_m)$, 1st and 3rd columns) of MLEs of fits of $M_{011}$ to data simulated from $M_{011}$ with $\beta = (750,15)'$, $\phi = 0.1$, $\sigma^2_m = 10$, and increasing $\sigma^2_s$ (rows).}
\end{figure}

\begin{figure}
\ssp
\centering
\caption{Identifying dynamic slope model by increasing autocorrelation} \label{fig:fmri:id:M011PRR}
\includegraphics[width=1.0\textwidth]{1000-250-M011-conv-750-15-PRR-5-1-10-10}
\caption*{Histograms in 1D (for $\phi$, second column) and 2D (for $\beta$ and $(\sigma^2_s,\sigma^2_m)$, 1st and 3rd columns) of MLEs of fits of $M_{011}$ to data simulated from $M_{011}$ with $\beta = (750,15)'$, $\sigma^2_s = 1$, $\sigma^2_m = 10$, and increasing $\phi$ (rows).}
\end{figure}

Figures \ref{fig:fmri:id:M011SNR} and \ref{fig:fmri:id:M011PRR} provide evidence that $M_{011}$ is identifiable provided the signal-to-noise ratio and autocorrelation coefficient is not too low. Similar plots shown in Figures \ref{fig:fmri:id:M101SNR} and \ref{fig:fmri:id:M101PRR} reveal that identifiability of $M_{101}$ is more challenging. We see in Figure \ref{fig:fmri:id:M101SNR} that no signal-to-noise ratio between 0.1 and 2 is adequate for identifying $M_{101}$ with $\phi = 0.1$. However, in Figure \ref{fig:fmri:id:M101PRR}, we see that for $\sigma^2_s / \sigma^2_m = 0.1$, increasing $\phi$ to 0.9 is enough to identify the dynamic intercept model. In all cases, however, $\beta$ appears adequately identified.

\begin{figure}
\ssp
\centering
\caption{Identifying dynamic intercept model by increasing signal-to-noise ratio} \label{fig:fmri:id:M101SNR}
\includegraphics[width=0.75\textwidth]{1000-250-M101-conv-750-15-1-5-SNR-10-10}
\caption*{Histograms in 1D (for $\phi$, second column) and 2D (for $\beta$ and $(\sigma^2_s,\sigma^2_m)$, 1st and 3rd columns) of MLEs of fits of $M_{101}$ to data simulated from $M_{101}$ with $\beta = (750,15)'$, $\phi = 0.1$, $\sigma^2_m = 10$, and increasing $\sigma^2_s$ (rows).}
\end{figure}

\begin{figure}
\ssp
\centering
\caption{Identifying dynamic intercept model by increasing autocorrelation} \label{fig:fmri:id:M101PRR}
\includegraphics[width=1.0\textwidth]{1000-250-M101-conv-750-15-PRR-5-1-10-10}
\caption*{Histograms in 1D (for $\phi$, second column) and 2D (for $\beta$ and $(\sigma^2_s,\sigma^2_m)$, 1st and 3rd columns) of MLEs of fits of $M_{101}$ to data simulated from $M_{101}$ with $\beta = (750,15)'$, $\sigma^2_s = 1$, $\sigma^2_m = 10$, and increasing $\phi$ (rows).}
\end{figure}

Lastly, identification of $M_{111}$ appears to be the most challenging. Figure \ref{fig:fmri:id:M111lowb} shows that by increasing the ratio of the white noise variance of the dynamic slope to the white noise variance of the dynamic intercept -- $\sigma^2_s / \sigma^2_b$ -- to 15, $\sigma^2_m$ can be identified but $\phi$ and $\sigma^2_b$ are clearly not. On the other hand, if $\sigma^2_b$ is increased to 20 as in Figure \ref{fig:fmri:id:M111highb}, identification of $\phi$ and $\sigma^2_b$ is improved, but a larger ratio $\sigma^2_s / \sigma^2_b$ is needed to identify $\sigma^2_m$. In all cases, identification of $\beta$ appears to be worse relative to $M_{011}$ and $M_{101}$.

\begin{figure}
\ssp
\centering
\caption{Identifying model with both dynamic slope and intercept with small intercept variance} \label{fig:fmri:id:M111lowb}
\includegraphics[width=1.0\textwidth]{1000-250-M111-conv-750-15-9-6-SNR-1-10}
\caption*{Histograms in 1D (for $\sigma^2_m$, last column) and 2D (for $\beta$, $(\phi,\rho)$ and $(\sigma^2_s,\sigma^2_m)$, first 3 columns) of MLEs of fits of $M_{111}$ to data simulated from $M_{111}$ with $\beta = (750,15)'$, $\phi = 0.9$, $\rho = 0.6$, $\sigma^2_b = 1$, $\sigma^2_m = 10$, and increasing $\sigma^2_s$ (rows).}
\end{figure}

\begin{figure}
\ssp
\centering
\caption{Identifying model with both dynamic slope and intercept with large intercept variance} \label{fig:fmri:id:M111highb}
\includegraphics[width=1.0\textwidth]{1000-250-M111-conv-750-15-9-6-SNR-20-10}
\caption*{Histograms in 1D (for $\sigma^2_m$, last column) and 2D (for $\beta$, $(\phi,\rho)$ and $(\sigma^2_s,\sigma^2_m)$, first 3 columns) of MLEs of fits of $M_{111}$ to data simulated from $M_{111}$ with $\beta = (750,15)'$, $\phi = 0.9$, $\rho = 0.6$, $\sigma^2_b = 20$, $\sigma^2_m = 10$, and increasing $\sigma^2_s$ (rows).}
\end{figure}

Figures \ref{fig:fmri:id:M011SNR} through \ref{fig:fmri:id:M111highb} suggest that out of the three models, $M_{011}$ is the easiest to identify and $M_{111}$ is the most difficult. This is further supported by the decrease in the proportion of simulations for which the {\tt optim} successively converges to the MLEs (see the convergence rates displayed under the plots for $\beta$). Models with identifiability issues are somtimes characterized by flat likelihoods that can make it difficult to find local maxima using iterative routines. For this reason, we discard $M_{111}$ at this point and examine only $M_{011}$, $M_{101}$, and $M_{001}$ in the remainder of this chapter.

\subsection{Fitting real fMRI data \label{sec:fmri:mle}}

\begin{figure}
\ssp
\centering
\caption{Histograms of MLEs for regression coefficients} \label{fig:fmri:mle:beta}
\includegraphics[width=0.6\textwidth]{craig_mle-beta}
\caption*{Two-dimensional histograms of MLEs of $\beta$ for real fMRI data from six brain regions (rows) fitted to dynamic regression models (columns). Blue X's denote the marginal averages of the MLEs from each brain region, and for each of two clusters in SV-left and SV-right.}
\end{figure}

\begin{figure}
\ssp
\centering
\caption{Histograms of MLEs for autocorrelation coefficient} \label{fig:fmri:mle:phi}
\includegraphics[width=0.4\textwidth]{craig_mle-phi}
\caption*{Histograms if MLEs of $\phi$ for real fMRI data from six brain regions (rows) fitted to dynamic regression models (columns). Blue vertical lines denote the average MLE of $\phi$ from each brain region, and for each of two clusters in SV-left and SV-right.}
\end{figure}

\begin{figure}
\ssp
\centering
\caption{Histograms of MLEs for state and observation variances} \label{fig:fmri:mle:sigma}
\includegraphics[width=0.6\textwidth]{craig_mle-sigma}
\caption*{Two-dimensional histograms of MLEs of $(\sigma^2_s,\sigma^2_m)$ for real fMRI data from six brain regions (rows) fitted to dynamic regression models (columns). Blue X's denote the marginal averages of the MLEs from each brain region, and for each of two clusters in SV-left and SV-right.}
\end{figure}

\begin{table}
\ssp
\centering
\caption{Average MLEs in single cluster brain regions} \label{tab:fmri:mle:means}
\begin{tabular}{|l|rrrr|}
\hline
Parameter & FP & IPS-left & IPS-right & PV \\
\hline
 & \multicolumn{4}{|c|}{$M_{011}$} \\
\hline
$\beta_0$ & 759.155 & 951.101 & 831.359 & 808.257 \\
$\beta_1$ & 1.395 & 15.009 & 24.894 & 16.492 \\
$\phi$ & 0.736 & 0.853 & 0.871 & 0.832 \\
$\sigma^2_s$ & 8.746 & 27.268 & 53.171 & 70.646 \\
$\sigma^2_m$ & 10.031 & 22.522 & 41.340 & 21.826 \\
\hline
 & \multicolumn{4}{|c|}{$M_{101}$} \\
\hline
$\beta_0$ & 759.875 & 950.038 & 830.901 & 807.434 \\
$\beta_1$ & -1.032 & 18.879 & 26.838 & 21.247 \\
$\phi$ & 0.746 & 0.637 & 0.654 & 0.596 \\
$\sigma^2_s$ & 3.323 & 16.970 & 29.565 & 23.498 \\
$\sigma^2_m$ & 6.105 & 0.534 & 1.382 & 1.727 \\
\hline
 & \multicolumn{4}{|c|}{$M_{001}$} \\
\hline
$\beta_0$ & 759.204 & 950.773 & 831.191 & 807.846 \\
$\beta_1$ & -1.448 & 16.087 & 24.688 & 19.039 \\
$\sigma^2_m$ & 12.480 & 29.579 & 54.511 & 37.629 \\
\hline
\end{tabular}
\caption*{Average MLEs calculated marginally for each fixed parameter (rows) for real fMRI data from four different brain regions (columns) fit to dynamic regression models (embedded tables).}
\end{table}

\begin{table}
\ssp
\centering
\caption{Average MLEs in bi-cluster brain regions} \label{tab:fmri:mle:clusters}
\begin{tabular}{|l|rr|rr|}
\hline
Parameter & \multicolumn{2}{|c|}{SV-left} & \multicolumn{2}{|c|}{SV-right} \\
\hline
 & Cluster H & Cluster L & Cluster H & Cluster L \\
\hline
 & \multicolumn{4}{|c|}{$M_{011}$} \\
\hline
$\beta_0$ & 874.999 & 363.601 & 697.816 & 308.080 \\
$\beta_1$ & 23.133 & 12.203 & 21.767 & 9.415 \\
$\phi$ & 0.566 & 0.520 & 0.489 & 0.630 \\
$\sigma^2_s$ & 11.475 & 4.378 & 13.162 & 4.889 \\
$\sigma^2_m$ & 0.502 & 0.393 & 2.423 & 3.906 \\
\hline
 & \multicolumn{4}{|c|}{$M_{101}$} \\
\hline
$\beta_0$ & 875.319 & 362.054 & 698.025 & 307.655 \\
$\beta_1$ & 20.009 & 11.708 & 13.883 & 9.361 \\
$\phi$ & 0.821 & 0.761 & 0.979 & 0.864 \\
$\sigma^2_s$ & 11.941 & 7.986 & 1.727 & 7.427 \\
$\sigma^2_m$ & 14.116 & 4.953 & 16.345 & 8.719 \\
\hline
 & \multicolumn{4}{|c|}{$M_{001}$} \\
\hline
$\beta_0$ & 875.084 & 361.957 & 697.769 & 307.538 \\
$\beta_1$ & 22.414 & 12.135 & 21.723 & 10.168 \\
$\sigma^2_m$ & 17.409 & 6.332 & 19.450 & 11.108 \\
\hline
\end{tabular}
\caption*{Average MLEs calculated marginally for each fixed parameter (rows) for real fMRI data from each cluster of secondary visual left and secondary visual right (columns) fit to dynamic regression models (embedded tables).}
\end{table}

\begin{table}
\ssp
\centering
\caption{Proportion of voxels with high activation} \label{tab:fmri:prop}
\begin{tabular}{|c|cc|}
\hline
Model & SV-left & SV-right \\
\hline
$M_{011}$ & 0.672 & 0.648 \\
$M_{101}$ & 0.677 & 0.648 \\
$M_{001}$ & 0.672 & 0.648 \\
\hline
\end{tabular}
\caption*{Proportion of voxels in each of secondary visual left and right (columns) classified into high activation cluster for each model fit (rows).}
\end{table}

\section{Comparing dynamic regression models using PL \label{sec:fmri:pl}}

\subsection{PL on simulated fMRI data \label{sec:fmri:sim}}

\begin{figure}
\ssp
\centering
\caption{Simulated fMRI data from dynamic slope model} \label{fig:fmri:sim}
\includegraphics[width=1.0\textwidth]{fmri-ar-sim-750-15-99-2-5-M101}
\caption*{Simulated fMRI time series $y_t$ (top) from $M_{011}$ with $\beta = (750,15)'$, $\phi = 0.99$, $\sigma^2_s = 2$, and $\sigma^2_m = 5$. Convolution of the hrf and neural activation pattern $\mbox{conv}_t$ and simulated change in dynamic slope $x_t$ are displayed in the middle and bottom panels, respectively.}
\end{figure}

\begin{figure}
\ssp
\centering
\caption{Credible intervals from PL compared with MCMC for simulated fMRI data} \label{fig:fmri:quant}
\includegraphics[width=1.0\textwidth]{fmri_pl_quant-M101-M101-560-1-1-FALSE-FALSE-FALSE}
\caption*{Sequential 95\% credible intervals for the filtered distributions of the dynamic slope (top left) and fixed parameters (other panels) of $M_{011}$ using PL with increasing number of particles (colors) compared with MCMC (black X's) run on simulated data from $M_{011}$ with $\beta = (750,15)'$, $\phi = 0.95$, $\sigma^2_s = 10$, and $\sigma^2_m = 10$ (displayed above top middle panel). True values of fixed parameters used for simulation and true simulated dynamic slopes are indicated by gray lines. MCMC estimates are displayed only for the filtered distributions of $\beta_1 + x_T$ and the fixed parameters conditional on all the data ($T = 250$). The same prior distributions on the initial state and fixed parameters were used for running both PL and MCMC, with $p(x_0) = \delta_{0}(x_0)$, $b_0$ and $\phi_0$ set to the true $\beta$ and $\phi$, respectively, and remaining hyperparameters displayed above the top left and right panels.}
\end{figure}

\subsection{Distinguishing dynamic regression models \label{sec:fmri:dist}}

\begin{figure}
\ssp
\centering
\caption{Distinguishing the dynamic slope model from the dynamic intercept and simple linear regression models} \label{fig:fmri:phi:M011}
\includegraphics[width=1.0\textwidth]{fmri_pl_loglik-M101-1-500-phi-496-594-1}
\caption*{Log marginal likelihoods of data coming from different dynamic regression models (colored lines) when simulated from $M_{011}$ with $\sigma^2_m = 10$ and increasing $\phi$ (x-axis) and $\sigma^2_s$. Log marginal likelihoods from three independent PL runs with each model, for each simulation, are displayed by colored points.}
\end{figure}

\begin{figure}
\ssp
\centering
\caption{Distinguishing the dynamic intercept model from the dynamic slope and simple linear regression models} \label{fig:fmri:phi:M101}
\includegraphics[width=1.0\textwidth]{fmri_pl_loglik-M011-1-500-phi-496-594-1}
\caption*{Log marginal likelihoods of data coming from different dynamic regression models (colored lines) when simulated from $M_{101}$ with $\sigma^2_m = 10$ and increasing $\phi$ (x-axis) and $\sigma^2_s$. Log marginal likelihoods from three independent PL runs with each model, for each simulation, are displayed by colored points.}
\end{figure}

\begin{figure}
\ssp
\centering
\caption{Distinguishing the dynamic slope model from the dynamic intercept and simple linear regression models with increasing prior variance} \label{fig:fmri:kappa:M011}
\includegraphics[width=1.0\textwidth]{fmri_pl_loglik-phiSNR-M101-3-500-496-594-1-5-10-15}
\caption*{Points $(\phi,\sigma^2_s / \sigma^2_m)$ at which $M_{011}$ is distinguished from $M_{101}$ and $M_{001}$ as being most likely to generate data simulated from $M_{011}$ with $\beta = (750,15)'$ and increasing prior variance $\kappa$ (plot panels). $M_{011}$ is determined to be distinguished from the other models if log marginal likelihood estimates from each of three PL runs for $M_{011}$ are greater than estimates from all three PL runs with each of the other models. Points are displayed for each of three separate simulations at each set of fixed parameter values. If log likelihood estimates for $M_{011}$ never exceed those for the other models for all three runs, no point is plotted.}
\end{figure}

\begin{figure}
\ssp
\centering
\caption{Distinguishing the dynamic intercept model from the dynamic slope and simple linear regression models with increasing prior variance} \label{fig:fmri:kappa:M101}
\includegraphics[width=1.0\textwidth]{fmri_pl_loglik-phiSNR-M011-3-500-496-594-1-5-10-15}
\caption*{Points $(\phi,\sigma^2_s / \sigma^2_m)$ at which $M_{101}$ is distinguished from $M_{011}$ and $M_{001}$ as being most likely to generate data simulated from $M_{101}$ with $\beta = (750,15)'$ and increasing prior variance $\kappa$ (plot panels). $M_{101}$ is determined to be distinguished from the other models if log marginal likelihood estimates from each of three PL runs for $M_{101}$ are greater than estimates from all three PL runs with each of the other models. Points are displayed for each of three separate simulations at each set of fixed parameter values. If log likelihood estimates for $M_{011}$ never exceed those for the other models for all three runs, no point is plotted.}
\end{figure}

\begin{figure}
\ssp
\centering
\caption{Log marginal likelihoods of data simulated from dynamic slope model with increasing particles in PL} \label{fig:fmri:loglik:M011}
\includegraphics[width=1.0\textwidth]{fmri_pl_loglik-M101-498-1}
\caption*{Kernel density approximation to the distribution of 20 log marginal likelihood estimates for $M_{011}$ (black lines) and $M_{101}$ (red lines) along with true log marginal likelihood of $M_{001}$ (blue vertical lines) using PL with increasing number of particles (plot panels) on simulated data from $M_{011}$ with $\beta = (750,15)'$, $\phi = 0.3$, $\sigma^2_s = 1$, and $\sigma^2_m = 10$.}
\end{figure}

\begin{figure}
\ssp
\centering
\caption{Log marginal likelihoods of data simulated from dynamic intercept model with increasing particles in PL} \label{fig:fmri:loglik:M101}
\includegraphics[width=1.0\textwidth]{fmri_pl_loglik-M011-500-1}
\caption*{Kernel density approximation to the distribution of 20 log marginal likelihood estimates for $M_{011}$ (black lines) and $M_{101}$ (red lines) along with true log marginal likelihood of $M_{001}$ (blue vertical lines) using PL with increasing number of particles (plot panels) on simulated data from $M_{101}$ with $\beta = (750,15)'$, $\phi = 0.5$, $\sigma^2_s = 1$, and $\sigma^2_m = 10$.}
\end{figure}

\begin{figure}
\ssp
\centering
\caption{Ternary diagrams of posterior model probabilities for simulated fMRI data from dynamic slope model} \label{fig:fmri:comp:M011}
\includegraphics[width=1.0\textwidth]{fmri_pl_comp-M101-498-1}
\caption*{Posterior model probabilities among dynamic regression models (corners of triangles) for twenty runs of the PL (black dots) for increasing number of particles (plot panels) on data simulated from $M_{011}$ with $\beta = (750,15)'$, $\phi = 0.3$, $\sigma^2_s = 1$, and $\sigma^2_m = 10$. Each point represents a set of posterior probabilities (one for each model), and the proximity of the point to a particular corner of the triangle represents the posterior probability of the model in that corner relative to the other models.}
\end{figure}

\begin{figure}
\ssp
\centering
\caption{Ternary diagrams of posterior model probabilities for simulated fMRI data from dynamic intercept model} \label{fig:fmri:comp:M101}
\includegraphics[width=1.0\textwidth]{fmri_pl_comp-M011-500-1}
\caption*{Posterior model probabilities among dynamic regression models (corners of triangles) for twenty runs of the PL (black dots) for increasing number of particles (plot panels) on data simulated from $M_{101}$ with $\beta = (750,15)'$, $\phi = 0.5$, $\sigma^2_s = 1$, and $\sigma^2_m = 10$. Each point represents a set of posterior probabilities (one for each model), and the proximity of the point to a particular corner of the triangle represents the posterior probability of the model in that corner relative to the other models.}
\end{figure}

\subsection{Real fMRI data \label{sec:fmri:real}}

\begin{figure}
\ssp
\centering
\caption{Log marginal likelihoods of real fMRI data} \label{fig:fmri:loglik:real}
\includegraphics[width=1.0\textwidth]{craig_pl-loglik-1-5000-1-FALSE-FALSE-FALSE}
\caption*{Kernel density approximation to the distribution of log marginal likelihood estimates of data from 125 voxels from each of 6 different brain regions (plot panels) for $M_{011}$ (black lines) and $M_{101}$ (red lines) along with true log marginal likelihood of $M_{001}$ (blue vertical lines) using PL with 5000 particles.}
\end{figure}

\begin{table}
\ssp
\centering
\caption{Proportion of voxels favoring different regression models} \label{}
\begin{tabular}{|c|ccc|}
\hline
Region & $M_{101}$ & $M_{011}$ & $M_{001}$ \\
\hline
FP & 0.976 & 0.000 & 0.024 \\
IPS-left & 0.992 & 0.008 & 0.000 \\
IPS-right & 0.880 & 0.096 & 0.024 \\
PV & 1.000 & 0.000 & 0.000 \\
SV-left & 0.800 & 0.144 & 0.056 \\
SV-right & 0.952 & 0.032 & 0.016 \\
\hline
\end{tabular}
\caption*{Proportion of voxels in each brain region (rows) with highest posterior model probability coming from each of $M_{101}$, $M_{011}$, and $M_{001}$ (columns). For $M_{101}$ and $M_{011}$, posterior model probabilities were calculated using the PL with 5000 particles. For $M_{001}$, the true posterior model probability was calculated analytically.}
\end{table}

\begin{figure}
\ssp
\centering
\caption{Posterior probabilities of dynamic regression models for real fMRI data} \label{fig:fmri:comp:real}
\includegraphics[width=1.0\textwidth]{craig_pl-comp-1-5000-1-FALSE-FALSE-FALSE}
\caption*{Posterior model probabilities among dynamic regression models (corners of triangles) for 125 voxels (black dots) in each of 6 brain regions (plot panels) using the PL with 5000 particles. Each point represents a set of posterior probabilities (one for each model), and the proximity of the point to a particular corner of the triangle represents the posterior probability of the model in that corner relative to the other models.}
\end{figure}

\begin{figure}
\ssp
\centering
\caption{Filtered dynamic slopes and posterior model probabilities for data from SV-left} \label{fig:fmri:slopes:real}
\includegraphics[width=1.0\textwidth]{craig_state-pl-SV-left-M101-3-FALSE-5000}
\caption*{Sequential 95\% credible intervals for dynamic slopes (lines) and 95\% credible intervals for $\phi|y_{1:T}$ (displayed above plot panels) along with cluster classification (color of lines) and posterior model probabilities (colored bar along top of plots) using the PL with 5000 particles on a 5 by 5 slice of voxels in the y-z plane of secondary visual left. The proportion of the solid bar colored for a particular model represents the posterior probability of that model relative to the others.}
\end{figure}

%Priors:
%
%\begin{align*}
%\beta \sim \mbox{N}(b_0,B_0) &\quad \sigma^2_m \sim \mbox{IG}(a_{m_0},b_{m_0}) \\
%\phi \sim \mbox{N}(\phi_0,\Phi_0) &\quad \sigma^2_s \sim \mbox{IG}(a_{s_0},b_{s_0})
%\end{align*}
%
%Let \[B_0 = \kappa^2 \left(\begin{array}{cc} 1000 & 0 \\ 0 & 225 \end{array}\right) \quad \Phi_0 = \kappa^2\times0.25.\] $b_0$ and $\phi_0$ are set to true values used for simulating data, or MLEs if true values are unknown, i.e. for real data. $a_{m_0}$, $b_{m_0}$, $a_{s_0}$, and $b_{s_0}$ are set such that mean of each inverse-gamma prior is equal to the true values or MLEs for estimating $\sigma^2_m$ and $\sigma^2_s$, and such that the variance of the inverse-gamma priors are equal to $\kappa^2\times500$.
